{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "'''\n",
    "@File    : data_processing.py\n",
    "@IDE     : PyCharm\n",
    "@Author  : Yaokun Li\n",
    "@Date    : 2022/10/18 20:30\n",
    "@Description :\n",
    "'''\n",
    "\n",
    "import gensim\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import trigrams\n",
    "\n",
    "\n",
    "def getLanguageDataSet(data, language):\n",
    "    return data.filter(lambda x: x['language'] == language)\n",
    "\n",
    "\n",
    "def getJapaneseDataSet(data):\n",
    "    return getLanguageDataSet(data, \"japanese\")\n",
    "\n",
    "\n",
    "def getEnglishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"english\")\n",
    "\n",
    "\n",
    "def getFinnishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"finnish\")\n",
    "\n",
    "\n",
    "MAX_VOCAB_SIZE = 5000\n",
    "UNK, PAD = '<UNK>', '<PAD>'\n",
    "\n",
    "\n",
    "def build_vocab(sent_list, max_size, min_freq, tokenizer):\n",
    "    vocab_dic = {}\n",
    "    for sent in sent_list:\n",
    "        for word in tokenizer(sent):\n",
    "            vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[\n",
    "                 :max_size]\n",
    "    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n",
    "    return vocab_dic\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "\n",
    "class QADataSet():\n",
    "    def __init__(self, tokenizer, dataset):\n",
    "        self.vocabulary = None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.question = []\n",
    "        self.answer_text = []\n",
    "        self.answer_start = []\n",
    "        self.document = []\n",
    "        self.tokenized_question = []\n",
    "        self.tokenized_answer_text = []\n",
    "        self.tokenized_document = []\n",
    "        self.answer_label = []\n",
    "\n",
    "        for element in dataset:\n",
    "            self.question.append(element[\"question_text\"].lower())\n",
    "            self.answer_text.append(element[\"annotations\"][\"answer_text\"][0])\n",
    "            self.answer_start.append(element[\"annotations\"][\"answer_start\"])\n",
    "            self.document.append(element[\"document_plaintext\"].lower())\n",
    "            if (element[\"annotations\"][\"answer_start\"] == [-1]):\n",
    "                self.answer_label.append(torch.tensor([0], dtype=torch.int64).cuda())\n",
    "            else:\n",
    "                self.answer_label.append(torch.tensor([1], dtype=torch.int64).cuda())\n",
    "\n",
    "\n",
    "        for s in self.answer_text:\n",
    "            self.tokenized_answer_text.append(self.__tokenize(s))\n",
    "\n",
    "        for s in self.question:\n",
    "            self.tokenized_question.append(self.__tokenize(s))\n",
    "\n",
    "        for s in self.document:\n",
    "            self.tokenized_document.append(self.__tokenize(s))\n",
    "\n",
    "        self.get_vocab()\n",
    "        self.document_num = []\n",
    "        self.question_num = []\n",
    "        for sent in self.tokenized_document:\n",
    "            self.document_num.append([self.vocabulary.get(word, MAX_VOCAB_SIZE) for word in sent])\n",
    "        for sent in self.tokenized_question:\n",
    "            self.question_num.append([self.vocabulary.get(word, MAX_VOCAB_SIZE) for word in sent])\n",
    "\n",
    "    def get_vocab(self):\n",
    "        self.vocabulary = build_vocab(self.question + self.document, MAX_VOCAB_SIZE, 2, self.tokenizer)\n",
    "\n",
    "        return self.vocabulary\n",
    "\n",
    "    def __tokenize(self, l, with_stop_word=True):\n",
    "        return self.tokenizer(l)\n",
    "\n",
    "    def get_overlaps_words_num(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.question_num, self.document_num):\n",
    "            count = 0\n",
    "            for word in question:\n",
    "                if word in document:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_document_length(self):\n",
    "        return [len(document) for document in self.document_num]\n",
    "\n",
    "    def get_question_length(self):\n",
    "        return [len(question) for question in self.question_num]\n",
    "\n",
    "    def get_overlaps_2_gram(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.tokenized_question, self.tokenized_document):\n",
    "            count = 0\n",
    "            doc_bigrams = list(bigrams(document))\n",
    "            for word in bigrams(question):\n",
    "                if word in doc_bigrams:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_overlaps_3_gram(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.tokenized_question, self.tokenized_document):\n",
    "            count = 0\n",
    "            doc_bigrams = list(trigrams(document))\n",
    "            for word in trigrams(question):\n",
    "                if word in doc_bigrams:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_label(self):\n",
    "        return torch.cat(self.answer_label, dim=0)\n",
    "\n",
    "    def get_question_bow(self, vocab_size):\n",
    "        data = []\n",
    "        for ques in self.question_num:\n",
    "            bow = [0]*vocab_size\n",
    "            for word in ques:\n",
    "                bow[word] += 1\n",
    "            data.append(bow)\n",
    "        return data\n",
    "\n",
    "    def get_doc_bow(self, vocab_size):\n",
    "        data = []\n",
    "        for ques in self.document_num:\n",
    "            bow = [0] * vocab_size\n",
    "            for word in ques:\n",
    "                bow[word] += 1\n",
    "            data.append(bow)\n",
    "        return data\n",
    "\n",
    "    def get_features(self):\n",
    "        feature1 = self.get_overlaps_words_num()\n",
    "        feature2 = self.get_overlaps_2_gram()\n",
    "        feature5 = self.get_overlaps_3_gram()\n",
    "        # feature3 = self.get_document_length()\n",
    "        # feature4 = self.get_question_length()\n",
    "        feature_ques_bow = torch.Tensor(self.get_question_bow(MAX_VOCAB_SIZE + 1)).cuda()\n",
    "        feature_doc_bow = torch.Tensor(self.get_doc_bow(MAX_VOCAB_SIZE + 1)).cuda()\n",
    "        X = torch.Tensor([ feature2, feature5]).t().cuda()\n",
    "        return torch.cat([feature_ques_bow,feature_doc_bow, X], dim = 1)\n",
    "\n",
    "    def get_answer_text_vec(self):\n",
    "        w2vModel = gensim.models.KeyedVectors.load_word2vec_format(\"week1/vector.txt\", binary=False)\n",
    "        data = []\n",
    "        en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in self.tokenized_answer_text],\n",
    "                                  dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()\n",
    "        else:\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True)\n",
    "\n",
    "        data.append(en_tensor_)\n",
    "        return torch.cat(data, dim=0)\n",
    "\n",
    "    def get_document_vec(self):\n",
    "        w2vModel = gensim.models.KeyedVectors.load_word2vec_format(\"week1/vector.txt\", binary=False)\n",
    "        data = []\n",
    "        en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in self.tokenized_document],\n",
    "                                  dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()\n",
    "        else:\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True)\n",
    "\n",
    "        data.append(en_tensor_)\n",
    "        return torch.cat(data, dim=0)\n",
    "\n",
    "    def get_question_vec(self):\n",
    "        w2vModel = gensim.models.KeyedVectors.load_word2vec_format(\"week1/vector.txt\", binary=False)\n",
    "        data = []\n",
    "        en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in self.tokenized_question],\n",
    "                                  dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()\n",
    "        else:\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True)\n",
    "\n",
    "        data.append(en_tensor_)\n",
    "        return torch.cat(data, dim=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class AnswerableClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels = 2, num_hidden = 100):\n",
    "        super(AnswerableClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(num_hidden, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return self.final(self.nonlinear(self.dropout(self.linear(bow_vec))))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import tokenizer\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6\n",
      "Reusing dataset parquet (/home/lyk/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7895af4465c54c0d9d0ddb068db6de95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 64\n",
    "lr = 0.0005"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "torch_tokenizer = get_tokenizer('basic_english', language=\"en\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/lyk/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e961ad4e6a80dccd.arrow\n"
     ]
    }
   ],
   "source": [
    "train_english_qa_dataset = QADataSet(torch_tokenizer, tokenizer.getEnglishDataSet(train_set))\n",
    "train_features = train_english_qa_dataset.get_features()\n",
    "train_label = train_english_qa_dataset.get_label()\n",
    "train_features_model_dataset = Data.TensorDataset(train_features, train_label)\n",
    "train_features_model_loader = Data.DataLoader(dataset=train_features_model_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/lyk/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5432f3a6e01e68fc.arrow\n"
     ]
    }
   ],
   "source": [
    "val_english_qa_dataset = QADataSet(torch_tokenizer,\n",
    "                                   tokenizer.getEnglishDataSet(validation_set))\n",
    "val_features = val_english_qa_dataset.get_features()\n",
    "val_label = val_english_qa_dataset.get_label()\n",
    "val_features_model_dataset = Data.TensorDataset(val_features, val_label)\n",
    "val_features_model_loader = Data.DataLoader(dataset=val_features_model_dataset,\n",
    "                                            batch_size= batch_size,\n",
    "                                            shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def train_features_model( model, train_loader, criterion, optimizer, model_file_name, epochs):\n",
    "    max_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_num = 0\n",
    "\n",
    "        for features, label in train_loader:\n",
    "            predict_label = model(features)\n",
    "            loss = criterion(predict_label, label)\n",
    "\n",
    "            pred = predict_label.max(-1, keepdim=True)[1]\n",
    "            acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            if (acc > max_acc):\n",
    "                max_acc = acc\n",
    "                torch.save(model.state_dict(), model_file_name)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_num += 1\n",
    "            print(\"epoch:\", epoch + 1, \"batch_num:\", batch_num, \"loss:\", round(loss.item(), 4), \"acc:\", acc)\n",
    "    return max_acc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 1 loss: 44.2427 acc: 0.515625\n",
      "epoch: 1 batch_num: 2 loss: 44.0469 acc: 0.484375\n",
      "epoch: 1 batch_num: 3 loss: 43.2315 acc: 0.671875\n",
      "epoch: 1 batch_num: 4 loss: 43.2498 acc: 0.609375\n",
      "epoch: 1 batch_num: 5 loss: 42.0908 acc: 0.59375\n",
      "epoch: 1 batch_num: 6 loss: 44.7493 acc: 0.578125\n",
      "epoch: 1 batch_num: 7 loss: 45.2881 acc: 0.546875\n",
      "epoch: 1 batch_num: 8 loss: 42.9856 acc: 0.53125\n",
      "epoch: 1 batch_num: 9 loss: 45.4687 acc: 0.484375\n",
      "epoch: 1 batch_num: 10 loss: 45.7899 acc: 0.546875\n",
      "epoch: 1 batch_num: 11 loss: 42.0714 acc: 0.640625\n",
      "epoch: 1 batch_num: 12 loss: 43.8922 acc: 0.59375\n",
      "epoch: 1 batch_num: 13 loss: 43.0143 acc: 0.703125\n",
      "epoch: 1 batch_num: 14 loss: 44.1418 acc: 0.65625\n",
      "epoch: 1 batch_num: 15 loss: 42.6057 acc: 0.765625\n",
      "epoch: 1 batch_num: 16 loss: 42.3941 acc: 0.640625\n",
      "epoch: 1 batch_num: 17 loss: 42.6035 acc: 0.609375\n",
      "epoch: 1 batch_num: 18 loss: 44.1874 acc: 0.546875\n",
      "epoch: 1 batch_num: 19 loss: 43.3476 acc: 0.578125\n",
      "epoch: 1 batch_num: 20 loss: 40.5957 acc: 0.734375\n",
      "epoch: 1 batch_num: 21 loss: 40.5295 acc: 0.65625\n",
      "epoch: 1 batch_num: 22 loss: 42.4628 acc: 0.671875\n",
      "epoch: 1 batch_num: 23 loss: 41.2021 acc: 0.625\n",
      "epoch: 1 batch_num: 24 loss: 42.2831 acc: 0.65625\n",
      "epoch: 1 batch_num: 25 loss: 41.4292 acc: 0.625\n",
      "epoch: 1 batch_num: 26 loss: 40.2185 acc: 0.734375\n",
      "epoch: 1 batch_num: 27 loss: 38.9466 acc: 0.734375\n",
      "epoch: 1 batch_num: 28 loss: 39.9662 acc: 0.765625\n",
      "epoch: 1 batch_num: 29 loss: 40.6585 acc: 0.6875\n",
      "epoch: 1 batch_num: 30 loss: 40.9685 acc: 0.65625\n",
      "epoch: 1 batch_num: 31 loss: 37.4346 acc: 0.765625\n",
      "epoch: 1 batch_num: 32 loss: 41.0584 acc: 0.625\n",
      "epoch: 1 batch_num: 33 loss: 37.6069 acc: 0.765625\n",
      "epoch: 1 batch_num: 34 loss: 41.8966 acc: 0.5625\n",
      "epoch: 1 batch_num: 35 loss: 39.2142 acc: 0.75\n",
      "epoch: 1 batch_num: 36 loss: 46.5023 acc: 0.46875\n",
      "epoch: 1 batch_num: 37 loss: 39.6243 acc: 0.734375\n",
      "epoch: 1 batch_num: 38 loss: 38.6414 acc: 0.75\n",
      "epoch: 1 batch_num: 39 loss: 39.6769 acc: 0.75\n",
      "epoch: 1 batch_num: 40 loss: 40.978 acc: 0.640625\n",
      "epoch: 1 batch_num: 41 loss: 40.6698 acc: 0.640625\n",
      "epoch: 1 batch_num: 42 loss: 38.6848 acc: 0.703125\n",
      "epoch: 1 batch_num: 43 loss: 39.971 acc: 0.71875\n",
      "epoch: 1 batch_num: 44 loss: 40.4654 acc: 0.75\n",
      "epoch: 1 batch_num: 45 loss: 35.4923 acc: 0.75\n",
      "epoch: 1 batch_num: 46 loss: 38.2972 acc: 0.734375\n",
      "epoch: 1 batch_num: 47 loss: 38.4477 acc: 0.640625\n",
      "epoch: 1 batch_num: 48 loss: 41.8604 acc: 0.6875\n",
      "epoch: 1 batch_num: 49 loss: 35.469 acc: 0.78125\n",
      "epoch: 1 batch_num: 50 loss: 37.4483 acc: 0.796875\n",
      "epoch: 1 batch_num: 51 loss: 38.9408 acc: 0.75\n",
      "epoch: 1 batch_num: 52 loss: 39.7556 acc: 0.71875\n",
      "epoch: 1 batch_num: 53 loss: 33.7206 acc: 0.78125\n",
      "epoch: 1 batch_num: 54 loss: 34.0359 acc: 0.8125\n",
      "epoch: 1 batch_num: 55 loss: 39.7598 acc: 0.6875\n",
      "epoch: 1 batch_num: 56 loss: 39.4673 acc: 0.609375\n",
      "epoch: 1 batch_num: 57 loss: 35.6411 acc: 0.75\n",
      "epoch: 1 batch_num: 58 loss: 37.1318 acc: 0.765625\n",
      "epoch: 1 batch_num: 59 loss: 34.8412 acc: 0.734375\n",
      "epoch: 1 batch_num: 60 loss: 40.6385 acc: 0.65625\n",
      "epoch: 1 batch_num: 61 loss: 33.732 acc: 0.8125\n",
      "epoch: 1 batch_num: 62 loss: 37.894 acc: 0.65625\n",
      "epoch: 1 batch_num: 63 loss: 36.2849 acc: 0.703125\n",
      "epoch: 1 batch_num: 64 loss: 39.8196 acc: 0.6875\n",
      "epoch: 1 batch_num: 65 loss: 38.6982 acc: 0.71875\n",
      "epoch: 1 batch_num: 66 loss: 35.1097 acc: 0.75\n",
      "epoch: 1 batch_num: 67 loss: 33.5003 acc: 0.75\n",
      "epoch: 1 batch_num: 68 loss: 29.8887 acc: 0.84375\n",
      "epoch: 1 batch_num: 69 loss: 33.6282 acc: 0.84375\n",
      "epoch: 1 batch_num: 70 loss: 44.0275 acc: 0.578125\n",
      "epoch: 1 batch_num: 71 loss: 38.4696 acc: 0.78125\n",
      "epoch: 1 batch_num: 72 loss: 34.2863 acc: 0.828125\n",
      "epoch: 1 batch_num: 73 loss: 32.0076 acc: 0.78125\n",
      "epoch: 1 batch_num: 74 loss: 30.1763 acc: 0.796875\n",
      "epoch: 1 batch_num: 75 loss: 33.4089 acc: 0.84375\n",
      "epoch: 1 batch_num: 76 loss: 36.0465 acc: 0.6875\n",
      "epoch: 1 batch_num: 77 loss: 33.8087 acc: 0.75\n",
      "epoch: 1 batch_num: 78 loss: 51.3037 acc: 0.734375\n",
      "epoch: 1 batch_num: 79 loss: 36.8478 acc: 0.78125\n",
      "epoch: 1 batch_num: 80 loss: 36.6224 acc: 0.703125\n",
      "epoch: 1 batch_num: 81 loss: 38.0379 acc: 0.6875\n",
      "epoch: 1 batch_num: 82 loss: 35.6412 acc: 0.71875\n",
      "epoch: 1 batch_num: 83 loss: 45.5871 acc: 0.734375\n",
      "epoch: 1 batch_num: 84 loss: 34.0393 acc: 0.71875\n",
      "epoch: 1 batch_num: 85 loss: 32.6522 acc: 0.78125\n",
      "epoch: 1 batch_num: 86 loss: 34.509 acc: 0.734375\n",
      "epoch: 1 batch_num: 87 loss: 35.0437 acc: 0.78125\n",
      "epoch: 1 batch_num: 88 loss: 30.3416 acc: 0.828125\n",
      "epoch: 1 batch_num: 89 loss: 36.2876 acc: 0.78125\n",
      "epoch: 1 batch_num: 90 loss: 33.2585 acc: 0.8125\n",
      "epoch: 1 batch_num: 91 loss: 36.7098 acc: 0.75\n",
      "epoch: 1 batch_num: 92 loss: 36.1272 acc: 0.703125\n",
      "epoch: 1 batch_num: 93 loss: 28.425 acc: 0.84375\n",
      "epoch: 1 batch_num: 94 loss: 35.504 acc: 0.703125\n",
      "epoch: 1 batch_num: 95 loss: 34.4284 acc: 0.828125\n",
      "epoch: 1 batch_num: 96 loss: 37.282 acc: 0.703125\n",
      "epoch: 1 batch_num: 97 loss: 39.2676 acc: 0.640625\n",
      "epoch: 1 batch_num: 98 loss: 36.3947 acc: 0.671875\n",
      "epoch: 1 batch_num: 99 loss: 38.6035 acc: 0.734375\n",
      "epoch: 1 batch_num: 100 loss: 31.8938 acc: 0.75\n",
      "epoch: 1 batch_num: 101 loss: 33.2836 acc: 0.734375\n",
      "epoch: 1 batch_num: 102 loss: 35.9217 acc: 0.78125\n",
      "epoch: 1 batch_num: 103 loss: 30.9064 acc: 0.734375\n",
      "epoch: 1 batch_num: 104 loss: 31.5485 acc: 0.828125\n",
      "epoch: 1 batch_num: 105 loss: 31.7102 acc: 0.78125\n",
      "epoch: 1 batch_num: 106 loss: 29.5617 acc: 0.796875\n",
      "epoch: 1 batch_num: 107 loss: 42.5774 acc: 0.671875\n",
      "epoch: 1 batch_num: 108 loss: 47.7611 acc: 0.703125\n",
      "epoch: 1 batch_num: 109 loss: 30.3041 acc: 0.78125\n",
      "epoch: 1 batch_num: 110 loss: 34.4494 acc: 0.765625\n",
      "epoch: 1 batch_num: 111 loss: 30.8925 acc: 0.828125\n",
      "epoch: 1 batch_num: 112 loss: 34.5647 acc: 0.734375\n",
      "epoch: 1 batch_num: 113 loss: 38.1272 acc: 0.703125\n",
      "epoch: 1 batch_num: 114 loss: 41.3771 acc: 0.671875\n",
      "epoch: 1 batch_num: 115 loss: 32.3358 acc: 0.75\n",
      "epoch: 1 batch_num: 116 loss: 15.6393 acc: 0.7586206896551724\n",
      "epoch: 2 batch_num: 1 loss: 26.8812 acc: 0.8125\n",
      "epoch: 2 batch_num: 2 loss: 35.3198 acc: 0.75\n",
      "epoch: 2 batch_num: 3 loss: 33.8208 acc: 0.703125\n",
      "epoch: 2 batch_num: 4 loss: 28.9612 acc: 0.828125\n",
      "epoch: 2 batch_num: 5 loss: 31.9238 acc: 0.765625\n",
      "epoch: 2 batch_num: 6 loss: 31.4466 acc: 0.75\n",
      "epoch: 2 batch_num: 7 loss: 24.7976 acc: 0.875\n",
      "epoch: 2 batch_num: 8 loss: 33.8883 acc: 0.78125\n",
      "epoch: 2 batch_num: 9 loss: 36.1126 acc: 0.78125\n",
      "epoch: 2 batch_num: 10 loss: 26.9343 acc: 0.859375\n",
      "epoch: 2 batch_num: 11 loss: 27.7671 acc: 0.828125\n",
      "epoch: 2 batch_num: 12 loss: 24.0336 acc: 0.859375\n",
      "epoch: 2 batch_num: 13 loss: 28.2129 acc: 0.78125\n",
      "epoch: 2 batch_num: 14 loss: 28.8172 acc: 0.796875\n",
      "epoch: 2 batch_num: 15 loss: 31.4183 acc: 0.796875\n",
      "epoch: 2 batch_num: 16 loss: 30.9574 acc: 0.71875\n",
      "epoch: 2 batch_num: 17 loss: 27.7385 acc: 0.8125\n",
      "epoch: 2 batch_num: 18 loss: 30.4371 acc: 0.78125\n",
      "epoch: 2 batch_num: 19 loss: 24.6453 acc: 0.890625\n",
      "epoch: 2 batch_num: 20 loss: 24.8452 acc: 0.875\n",
      "epoch: 2 batch_num: 21 loss: 28.1531 acc: 0.796875\n",
      "epoch: 2 batch_num: 22 loss: 22.7728 acc: 0.890625\n",
      "epoch: 2 batch_num: 23 loss: 33.983 acc: 0.765625\n",
      "epoch: 2 batch_num: 24 loss: 33.6054 acc: 0.765625\n",
      "epoch: 2 batch_num: 25 loss: 32.2889 acc: 0.734375\n",
      "epoch: 2 batch_num: 26 loss: 28.1595 acc: 0.828125\n",
      "epoch: 2 batch_num: 27 loss: 28.0468 acc: 0.765625\n",
      "epoch: 2 batch_num: 28 loss: 30.1642 acc: 0.8125\n",
      "epoch: 2 batch_num: 29 loss: 24.0912 acc: 0.90625\n",
      "epoch: 2 batch_num: 30 loss: 30.0681 acc: 0.765625\n",
      "epoch: 2 batch_num: 31 loss: 31.2003 acc: 0.8125\n",
      "epoch: 2 batch_num: 32 loss: 25.2594 acc: 0.828125\n",
      "epoch: 2 batch_num: 33 loss: 25.0099 acc: 0.84375\n",
      "epoch: 2 batch_num: 34 loss: 26.7721 acc: 0.84375\n",
      "epoch: 2 batch_num: 35 loss: 26.0277 acc: 0.90625\n",
      "epoch: 2 batch_num: 36 loss: 22.3124 acc: 0.890625\n",
      "epoch: 2 batch_num: 37 loss: 31.535 acc: 0.78125\n",
      "epoch: 2 batch_num: 38 loss: 25.2574 acc: 0.84375\n",
      "epoch: 2 batch_num: 39 loss: 31.7133 acc: 0.828125\n",
      "epoch: 2 batch_num: 40 loss: 26.0155 acc: 0.84375\n",
      "epoch: 2 batch_num: 41 loss: 26.3358 acc: 0.859375\n",
      "epoch: 2 batch_num: 42 loss: 29.0127 acc: 0.765625\n",
      "epoch: 2 batch_num: 43 loss: 24.3462 acc: 0.890625\n",
      "epoch: 2 batch_num: 44 loss: 29.3276 acc: 0.75\n",
      "epoch: 2 batch_num: 45 loss: 23.9529 acc: 0.875\n",
      "epoch: 2 batch_num: 46 loss: 28.8191 acc: 0.828125\n",
      "epoch: 2 batch_num: 47 loss: 34.2452 acc: 0.796875\n",
      "epoch: 2 batch_num: 48 loss: 26.9864 acc: 0.828125\n",
      "epoch: 2 batch_num: 49 loss: 25.9126 acc: 0.796875\n",
      "epoch: 2 batch_num: 50 loss: 33.1735 acc: 0.71875\n",
      "epoch: 2 batch_num: 51 loss: 28.8837 acc: 0.78125\n",
      "epoch: 2 batch_num: 52 loss: 27.4896 acc: 0.859375\n",
      "epoch: 2 batch_num: 53 loss: 30.9925 acc: 0.828125\n",
      "epoch: 2 batch_num: 54 loss: 21.3838 acc: 0.875\n",
      "epoch: 2 batch_num: 55 loss: 29.2029 acc: 0.78125\n",
      "epoch: 2 batch_num: 56 loss: 28.6107 acc: 0.796875\n",
      "epoch: 2 batch_num: 57 loss: 33.0255 acc: 0.75\n",
      "epoch: 2 batch_num: 58 loss: 25.6618 acc: 0.859375\n",
      "epoch: 2 batch_num: 59 loss: 27.8225 acc: 0.84375\n",
      "epoch: 2 batch_num: 60 loss: 26.8667 acc: 0.84375\n",
      "epoch: 2 batch_num: 61 loss: 30.8167 acc: 0.796875\n",
      "epoch: 2 batch_num: 62 loss: 29.5427 acc: 0.734375\n",
      "epoch: 2 batch_num: 63 loss: 26.0162 acc: 0.8125\n",
      "epoch: 2 batch_num: 64 loss: 34.6225 acc: 0.859375\n",
      "epoch: 2 batch_num: 65 loss: 28.4901 acc: 0.8125\n",
      "epoch: 2 batch_num: 66 loss: 31.8316 acc: 0.71875\n",
      "epoch: 2 batch_num: 67 loss: 32.7432 acc: 0.734375\n",
      "epoch: 2 batch_num: 68 loss: 30.8068 acc: 0.78125\n",
      "epoch: 2 batch_num: 69 loss: 27.4953 acc: 0.8125\n",
      "epoch: 2 batch_num: 70 loss: 27.0995 acc: 0.796875\n",
      "epoch: 2 batch_num: 71 loss: 39.2152 acc: 0.703125\n",
      "epoch: 2 batch_num: 72 loss: 29.2552 acc: 0.859375\n",
      "epoch: 2 batch_num: 73 loss: 30.4729 acc: 0.765625\n",
      "epoch: 2 batch_num: 74 loss: 27.9522 acc: 0.8125\n",
      "epoch: 2 batch_num: 75 loss: 26.4333 acc: 0.8125\n",
      "epoch: 2 batch_num: 76 loss: 22.5863 acc: 0.828125\n",
      "epoch: 2 batch_num: 77 loss: 23.6332 acc: 0.796875\n",
      "epoch: 2 batch_num: 78 loss: 25.9374 acc: 0.84375\n",
      "epoch: 2 batch_num: 79 loss: 28.3968 acc: 0.8125\n",
      "epoch: 2 batch_num: 80 loss: 36.0748 acc: 0.765625\n",
      "epoch: 2 batch_num: 81 loss: 34.0898 acc: 0.8125\n",
      "epoch: 2 batch_num: 82 loss: 32.2496 acc: 0.78125\n",
      "epoch: 2 batch_num: 83 loss: 30.3835 acc: 0.8125\n",
      "epoch: 2 batch_num: 84 loss: 30.6042 acc: 0.78125\n",
      "epoch: 2 batch_num: 85 loss: 26.5579 acc: 0.8125\n",
      "epoch: 2 batch_num: 86 loss: 30.76 acc: 0.84375\n",
      "epoch: 2 batch_num: 87 loss: 27.67 acc: 0.796875\n",
      "epoch: 2 batch_num: 88 loss: 28.1373 acc: 0.84375\n",
      "epoch: 2 batch_num: 89 loss: 31.9508 acc: 0.78125\n",
      "epoch: 2 batch_num: 90 loss: 33.2093 acc: 0.734375\n",
      "epoch: 2 batch_num: 91 loss: 30.2532 acc: 0.828125\n",
      "epoch: 2 batch_num: 92 loss: 29.1088 acc: 0.8125\n",
      "epoch: 2 batch_num: 93 loss: 34.3606 acc: 0.71875\n",
      "epoch: 2 batch_num: 94 loss: 27.1041 acc: 0.875\n",
      "epoch: 2 batch_num: 95 loss: 24.5276 acc: 0.828125\n",
      "epoch: 2 batch_num: 96 loss: 32.8492 acc: 0.84375\n",
      "epoch: 2 batch_num: 97 loss: 28.2885 acc: 0.765625\n",
      "epoch: 2 batch_num: 98 loss: 29.948 acc: 0.765625\n",
      "epoch: 2 batch_num: 99 loss: 27.0811 acc: 0.78125\n",
      "epoch: 2 batch_num: 100 loss: 29.0714 acc: 0.734375\n",
      "epoch: 2 batch_num: 101 loss: 24.898 acc: 0.828125\n",
      "epoch: 2 batch_num: 102 loss: 22.7171 acc: 0.875\n",
      "epoch: 2 batch_num: 103 loss: 28.2528 acc: 0.828125\n",
      "epoch: 2 batch_num: 104 loss: 27.5162 acc: 0.828125\n",
      "epoch: 2 batch_num: 105 loss: 27.6617 acc: 0.78125\n",
      "epoch: 2 batch_num: 106 loss: 29.3018 acc: 0.859375\n",
      "epoch: 2 batch_num: 107 loss: 38.5573 acc: 0.71875\n",
      "epoch: 2 batch_num: 108 loss: 21.9986 acc: 0.859375\n",
      "epoch: 2 batch_num: 109 loss: 26.102 acc: 0.828125\n",
      "epoch: 2 batch_num: 110 loss: 24.5667 acc: 0.84375\n",
      "epoch: 2 batch_num: 111 loss: 24.5469 acc: 0.859375\n",
      "epoch: 2 batch_num: 112 loss: 23.0957 acc: 0.828125\n",
      "epoch: 2 batch_num: 113 loss: 32.1349 acc: 0.78125\n",
      "epoch: 2 batch_num: 114 loss: 28.7312 acc: 0.765625\n",
      "epoch: 2 batch_num: 115 loss: 22.4921 acc: 0.875\n",
      "epoch: 2 batch_num: 116 loss: 9.4502 acc: 0.896551724137931\n",
      "epoch: 3 batch_num: 1 loss: 17.6021 acc: 0.90625\n",
      "epoch: 3 batch_num: 2 loss: 20.6702 acc: 0.875\n",
      "epoch: 3 batch_num: 3 loss: 18.9128 acc: 0.875\n",
      "epoch: 3 batch_num: 4 loss: 27.1913 acc: 0.765625\n",
      "epoch: 3 batch_num: 5 loss: 23.8408 acc: 0.8125\n",
      "epoch: 3 batch_num: 6 loss: 23.7104 acc: 0.828125\n",
      "epoch: 3 batch_num: 7 loss: 22.8637 acc: 0.859375\n",
      "epoch: 3 batch_num: 8 loss: 27.028 acc: 0.78125\n",
      "epoch: 3 batch_num: 9 loss: 22.3696 acc: 0.796875\n",
      "epoch: 3 batch_num: 10 loss: 17.067 acc: 0.90625\n",
      "epoch: 3 batch_num: 11 loss: 30.3299 acc: 0.75\n",
      "epoch: 3 batch_num: 12 loss: 19.9179 acc: 0.9375\n",
      "epoch: 3 batch_num: 13 loss: 20.0605 acc: 0.921875\n",
      "epoch: 3 batch_num: 14 loss: 23.0523 acc: 0.859375\n",
      "epoch: 3 batch_num: 15 loss: 26.7498 acc: 0.859375\n",
      "epoch: 3 batch_num: 16 loss: 23.6208 acc: 0.828125\n",
      "epoch: 3 batch_num: 17 loss: 18.0451 acc: 0.921875\n",
      "epoch: 3 batch_num: 18 loss: 19.7296 acc: 0.90625\n",
      "epoch: 3 batch_num: 19 loss: 19.2317 acc: 0.875\n",
      "epoch: 3 batch_num: 20 loss: 23.1288 acc: 0.859375\n",
      "epoch: 3 batch_num: 21 loss: 20.709 acc: 0.875\n",
      "epoch: 3 batch_num: 22 loss: 23.0508 acc: 0.890625\n",
      "epoch: 3 batch_num: 23 loss: 19.7982 acc: 0.90625\n",
      "epoch: 3 batch_num: 24 loss: 22.2981 acc: 0.828125\n",
      "epoch: 3 batch_num: 25 loss: 14.061 acc: 0.9375\n",
      "epoch: 3 batch_num: 26 loss: 28.1489 acc: 0.828125\n",
      "epoch: 3 batch_num: 27 loss: 19.2092 acc: 0.875\n",
      "epoch: 3 batch_num: 28 loss: 20.7521 acc: 0.890625\n",
      "epoch: 3 batch_num: 29 loss: 24.3458 acc: 0.78125\n",
      "epoch: 3 batch_num: 30 loss: 29.2791 acc: 0.8125\n",
      "epoch: 3 batch_num: 31 loss: 25.5505 acc: 0.796875\n",
      "epoch: 3 batch_num: 32 loss: 23.0066 acc: 0.890625\n",
      "epoch: 3 batch_num: 33 loss: 22.6471 acc: 0.890625\n",
      "epoch: 3 batch_num: 34 loss: 23.8315 acc: 0.890625\n",
      "epoch: 3 batch_num: 35 loss: 24.3287 acc: 0.859375\n",
      "epoch: 3 batch_num: 36 loss: 18.7873 acc: 0.90625\n",
      "epoch: 3 batch_num: 37 loss: 25.4481 acc: 0.875\n",
      "epoch: 3 batch_num: 38 loss: 28.698 acc: 0.78125\n",
      "epoch: 3 batch_num: 39 loss: 26.8302 acc: 0.828125\n",
      "epoch: 3 batch_num: 40 loss: 21.5363 acc: 0.828125\n",
      "epoch: 3 batch_num: 41 loss: 23.8756 acc: 0.84375\n",
      "epoch: 3 batch_num: 42 loss: 21.7681 acc: 0.84375\n",
      "epoch: 3 batch_num: 43 loss: 24.7746 acc: 0.828125\n",
      "epoch: 3 batch_num: 44 loss: 28.714 acc: 0.78125\n",
      "epoch: 3 batch_num: 45 loss: 23.0524 acc: 0.859375\n",
      "epoch: 3 batch_num: 46 loss: 26.2 acc: 0.78125\n",
      "epoch: 3 batch_num: 47 loss: 18.1276 acc: 0.890625\n",
      "epoch: 3 batch_num: 48 loss: 24.0974 acc: 0.84375\n",
      "epoch: 3 batch_num: 49 loss: 19.7931 acc: 0.9375\n",
      "epoch: 3 batch_num: 50 loss: 23.6393 acc: 0.828125\n",
      "epoch: 3 batch_num: 51 loss: 25.3655 acc: 0.859375\n",
      "epoch: 3 batch_num: 52 loss: 22.4046 acc: 0.859375\n",
      "epoch: 3 batch_num: 53 loss: 30.0476 acc: 0.796875\n",
      "epoch: 3 batch_num: 54 loss: 22.1088 acc: 0.859375\n",
      "epoch: 3 batch_num: 55 loss: 22.1081 acc: 0.859375\n",
      "epoch: 3 batch_num: 56 loss: 24.3713 acc: 0.828125\n",
      "epoch: 3 batch_num: 57 loss: 22.5186 acc: 0.828125\n",
      "epoch: 3 batch_num: 58 loss: 22.2429 acc: 0.875\n",
      "epoch: 3 batch_num: 59 loss: 31.4627 acc: 0.84375\n",
      "epoch: 3 batch_num: 60 loss: 17.3625 acc: 0.9375\n",
      "epoch: 3 batch_num: 61 loss: 21.6035 acc: 0.859375\n",
      "epoch: 3 batch_num: 62 loss: 17.4262 acc: 0.890625\n",
      "epoch: 3 batch_num: 63 loss: 19.344 acc: 0.859375\n",
      "epoch: 3 batch_num: 64 loss: 27.569 acc: 0.84375\n",
      "epoch: 3 batch_num: 65 loss: 17.8631 acc: 0.90625\n",
      "epoch: 3 batch_num: 66 loss: 25.4299 acc: 0.828125\n",
      "epoch: 3 batch_num: 67 loss: 26.3004 acc: 0.84375\n",
      "epoch: 3 batch_num: 68 loss: 17.1086 acc: 0.90625\n",
      "epoch: 3 batch_num: 69 loss: 22.1301 acc: 0.8125\n",
      "epoch: 3 batch_num: 70 loss: 19.1088 acc: 0.890625\n",
      "epoch: 3 batch_num: 71 loss: 20.1424 acc: 0.859375\n",
      "epoch: 3 batch_num: 72 loss: 22.4461 acc: 0.859375\n",
      "epoch: 3 batch_num: 73 loss: 24.8784 acc: 0.828125\n",
      "epoch: 3 batch_num: 74 loss: 23.4734 acc: 0.875\n",
      "epoch: 3 batch_num: 75 loss: 23.4337 acc: 0.84375\n",
      "epoch: 3 batch_num: 76 loss: 21.015 acc: 0.875\n",
      "epoch: 3 batch_num: 77 loss: 17.9235 acc: 0.890625\n",
      "epoch: 3 batch_num: 78 loss: 19.1309 acc: 0.875\n",
      "epoch: 3 batch_num: 79 loss: 23.2101 acc: 0.875\n",
      "epoch: 3 batch_num: 80 loss: 20.8328 acc: 0.84375\n",
      "epoch: 3 batch_num: 81 loss: 21.1372 acc: 0.859375\n",
      "epoch: 3 batch_num: 82 loss: 28.0419 acc: 0.8125\n",
      "epoch: 3 batch_num: 83 loss: 23.1003 acc: 0.859375\n",
      "epoch: 3 batch_num: 84 loss: 27.3088 acc: 0.8125\n",
      "epoch: 3 batch_num: 85 loss: 27.4559 acc: 0.828125\n",
      "epoch: 3 batch_num: 86 loss: 20.9552 acc: 0.84375\n",
      "epoch: 3 batch_num: 87 loss: 19.8824 acc: 0.9375\n",
      "epoch: 3 batch_num: 88 loss: 27.3637 acc: 0.78125\n",
      "epoch: 3 batch_num: 89 loss: 26.5102 acc: 0.84375\n",
      "epoch: 3 batch_num: 90 loss: 23.233 acc: 0.828125\n",
      "epoch: 3 batch_num: 91 loss: 21.4937 acc: 0.84375\n",
      "epoch: 3 batch_num: 92 loss: 28.7017 acc: 0.828125\n",
      "epoch: 3 batch_num: 93 loss: 25.7021 acc: 0.78125\n",
      "epoch: 3 batch_num: 94 loss: 18.3043 acc: 0.890625\n",
      "epoch: 3 batch_num: 95 loss: 21.9762 acc: 0.90625\n",
      "epoch: 3 batch_num: 96 loss: 21.5288 acc: 0.859375\n",
      "epoch: 3 batch_num: 97 loss: 18.2188 acc: 0.90625\n",
      "epoch: 3 batch_num: 98 loss: 27.5583 acc: 0.78125\n",
      "epoch: 3 batch_num: 99 loss: 20.5864 acc: 0.921875\n",
      "epoch: 3 batch_num: 100 loss: 21.8568 acc: 0.859375\n",
      "epoch: 3 batch_num: 101 loss: 19.1925 acc: 0.9375\n",
      "epoch: 3 batch_num: 102 loss: 25.4273 acc: 0.8125\n",
      "epoch: 3 batch_num: 103 loss: 28.1145 acc: 0.8125\n",
      "epoch: 3 batch_num: 104 loss: 19.9107 acc: 0.859375\n",
      "epoch: 3 batch_num: 105 loss: 28.4552 acc: 0.78125\n",
      "epoch: 3 batch_num: 106 loss: 27.5814 acc: 0.828125\n",
      "epoch: 3 batch_num: 107 loss: 20.3764 acc: 0.875\n",
      "epoch: 3 batch_num: 108 loss: 21.6545 acc: 0.828125\n",
      "epoch: 3 batch_num: 109 loss: 24.8311 acc: 0.84375\n",
      "epoch: 3 batch_num: 110 loss: 24.4962 acc: 0.796875\n",
      "epoch: 3 batch_num: 111 loss: 16.703 acc: 0.921875\n",
      "epoch: 3 batch_num: 112 loss: 18.5977 acc: 0.9375\n",
      "epoch: 3 batch_num: 113 loss: 25.6535 acc: 0.890625\n",
      "epoch: 3 batch_num: 114 loss: 16.2686 acc: 0.921875\n",
      "epoch: 3 batch_num: 115 loss: 20.7863 acc: 0.84375\n",
      "epoch: 3 batch_num: 116 loss: 9.667 acc: 0.8275862068965517\n",
      "epoch: 4 batch_num: 1 loss: 17.9958 acc: 0.890625\n",
      "epoch: 4 batch_num: 2 loss: 20.728 acc: 0.90625\n",
      "epoch: 4 batch_num: 3 loss: 20.1203 acc: 0.9375\n",
      "epoch: 4 batch_num: 4 loss: 17.4701 acc: 0.921875\n",
      "epoch: 4 batch_num: 5 loss: 14.6198 acc: 0.9375\n",
      "epoch: 4 batch_num: 6 loss: 16.5935 acc: 0.921875\n",
      "epoch: 4 batch_num: 7 loss: 20.1319 acc: 0.890625\n",
      "epoch: 4 batch_num: 8 loss: 15.3715 acc: 0.9375\n",
      "epoch: 4 batch_num: 9 loss: 22.7222 acc: 0.875\n",
      "epoch: 4 batch_num: 10 loss: 19.5237 acc: 0.875\n",
      "epoch: 4 batch_num: 11 loss: 15.2035 acc: 0.921875\n",
      "epoch: 4 batch_num: 12 loss: 24.9933 acc: 0.875\n",
      "epoch: 4 batch_num: 13 loss: 21.4692 acc: 0.90625\n",
      "epoch: 4 batch_num: 14 loss: 19.5992 acc: 0.859375\n",
      "epoch: 4 batch_num: 15 loss: 16.3966 acc: 0.890625\n",
      "epoch: 4 batch_num: 16 loss: 17.5183 acc: 0.890625\n",
      "epoch: 4 batch_num: 17 loss: 18.4426 acc: 0.90625\n",
      "epoch: 4 batch_num: 18 loss: 13.0472 acc: 0.953125\n",
      "epoch: 4 batch_num: 19 loss: 17.2929 acc: 0.859375\n",
      "epoch: 4 batch_num: 20 loss: 13.9944 acc: 0.9375\n",
      "epoch: 4 batch_num: 21 loss: 18.7955 acc: 0.921875\n",
      "epoch: 4 batch_num: 22 loss: 13.2092 acc: 0.9375\n",
      "epoch: 4 batch_num: 23 loss: 16.1637 acc: 0.90625\n",
      "epoch: 4 batch_num: 24 loss: 18.072 acc: 0.859375\n",
      "epoch: 4 batch_num: 25 loss: 12.0251 acc: 0.96875\n",
      "epoch: 4 batch_num: 26 loss: 17.219 acc: 0.9375\n",
      "epoch: 4 batch_num: 27 loss: 15.036 acc: 0.90625\n",
      "epoch: 4 batch_num: 28 loss: 16.39 acc: 0.90625\n",
      "epoch: 4 batch_num: 29 loss: 21.1965 acc: 0.84375\n",
      "epoch: 4 batch_num: 30 loss: 11.5269 acc: 0.9375\n",
      "epoch: 4 batch_num: 31 loss: 19.2913 acc: 0.828125\n",
      "epoch: 4 batch_num: 32 loss: 16.5533 acc: 0.890625\n",
      "epoch: 4 batch_num: 33 loss: 19.3704 acc: 0.875\n",
      "epoch: 4 batch_num: 34 loss: 13.3469 acc: 0.953125\n",
      "epoch: 4 batch_num: 35 loss: 16.8281 acc: 0.9375\n",
      "epoch: 4 batch_num: 36 loss: 15.4489 acc: 0.9375\n",
      "epoch: 4 batch_num: 37 loss: 12.3714 acc: 0.96875\n",
      "epoch: 4 batch_num: 38 loss: 14.804 acc: 0.9375\n",
      "epoch: 4 batch_num: 39 loss: 16.7346 acc: 0.921875\n",
      "epoch: 4 batch_num: 40 loss: 18.6047 acc: 0.859375\n",
      "epoch: 4 batch_num: 41 loss: 14.9152 acc: 0.9375\n",
      "epoch: 4 batch_num: 42 loss: 17.6372 acc: 0.90625\n",
      "epoch: 4 batch_num: 43 loss: 14.2132 acc: 0.953125\n",
      "epoch: 4 batch_num: 44 loss: 20.2675 acc: 0.875\n",
      "epoch: 4 batch_num: 45 loss: 15.6377 acc: 0.875\n",
      "epoch: 4 batch_num: 46 loss: 16.8997 acc: 0.90625\n",
      "epoch: 4 batch_num: 47 loss: 11.478 acc: 0.953125\n",
      "epoch: 4 batch_num: 48 loss: 18.9552 acc: 0.921875\n",
      "epoch: 4 batch_num: 49 loss: 20.0233 acc: 0.828125\n",
      "epoch: 4 batch_num: 50 loss: 20.0549 acc: 0.84375\n",
      "epoch: 4 batch_num: 51 loss: 22.2644 acc: 0.890625\n",
      "epoch: 4 batch_num: 52 loss: 24.9177 acc: 0.8125\n",
      "epoch: 4 batch_num: 53 loss: 20.4022 acc: 0.890625\n",
      "epoch: 4 batch_num: 54 loss: 13.0677 acc: 0.9375\n",
      "epoch: 4 batch_num: 55 loss: 19.2972 acc: 0.90625\n",
      "epoch: 4 batch_num: 56 loss: 18.9923 acc: 0.890625\n",
      "epoch: 4 batch_num: 57 loss: 14.0515 acc: 0.921875\n",
      "epoch: 4 batch_num: 58 loss: 15.1194 acc: 0.9375\n",
      "epoch: 4 batch_num: 59 loss: 19.6545 acc: 0.84375\n",
      "epoch: 4 batch_num: 60 loss: 23.9065 acc: 0.78125\n",
      "epoch: 4 batch_num: 61 loss: 16.9003 acc: 0.9375\n",
      "epoch: 4 batch_num: 62 loss: 16.3296 acc: 0.890625\n",
      "epoch: 4 batch_num: 63 loss: 19.9612 acc: 0.890625\n",
      "epoch: 4 batch_num: 64 loss: 20.7046 acc: 0.890625\n",
      "epoch: 4 batch_num: 65 loss: 20.6487 acc: 0.84375\n",
      "epoch: 4 batch_num: 66 loss: 16.2938 acc: 0.9375\n",
      "epoch: 4 batch_num: 67 loss: 18.0605 acc: 0.84375\n",
      "epoch: 4 batch_num: 68 loss: 16.337 acc: 0.890625\n",
      "epoch: 4 batch_num: 69 loss: 22.3482 acc: 0.828125\n",
      "epoch: 4 batch_num: 70 loss: 15.3071 acc: 0.90625\n",
      "epoch: 4 batch_num: 71 loss: 24.6015 acc: 0.890625\n",
      "epoch: 4 batch_num: 72 loss: 17.7085 acc: 0.890625\n",
      "epoch: 4 batch_num: 73 loss: 17.7297 acc: 0.890625\n",
      "epoch: 4 batch_num: 74 loss: 14.1747 acc: 0.90625\n",
      "epoch: 4 batch_num: 75 loss: 19.7614 acc: 0.84375\n",
      "epoch: 4 batch_num: 76 loss: 23.693 acc: 0.796875\n",
      "epoch: 4 batch_num: 77 loss: 16.2491 acc: 0.890625\n",
      "epoch: 4 batch_num: 78 loss: 24.072 acc: 0.78125\n",
      "epoch: 4 batch_num: 79 loss: 13.6412 acc: 0.890625\n",
      "epoch: 4 batch_num: 80 loss: 26.1707 acc: 0.859375\n",
      "epoch: 4 batch_num: 81 loss: 23.9347 acc: 0.859375\n",
      "epoch: 4 batch_num: 82 loss: 14.205 acc: 0.890625\n",
      "epoch: 4 batch_num: 83 loss: 15.4923 acc: 0.875\n",
      "epoch: 4 batch_num: 84 loss: 12.4944 acc: 0.890625\n",
      "epoch: 4 batch_num: 85 loss: 18.777 acc: 0.90625\n",
      "epoch: 4 batch_num: 86 loss: 18.2581 acc: 0.875\n",
      "epoch: 4 batch_num: 87 loss: 19.6385 acc: 0.859375\n",
      "epoch: 4 batch_num: 88 loss: 15.8019 acc: 0.90625\n",
      "epoch: 4 batch_num: 89 loss: 20.2261 acc: 0.8125\n",
      "epoch: 4 batch_num: 90 loss: 15.7325 acc: 0.9375\n",
      "epoch: 4 batch_num: 91 loss: 20.7475 acc: 0.90625\n",
      "epoch: 4 batch_num: 92 loss: 22.6216 acc: 0.84375\n",
      "epoch: 4 batch_num: 93 loss: 16.0894 acc: 0.890625\n",
      "epoch: 4 batch_num: 94 loss: 24.4732 acc: 0.859375\n",
      "epoch: 4 batch_num: 95 loss: 20.3132 acc: 0.90625\n",
      "epoch: 4 batch_num: 96 loss: 20.3656 acc: 0.859375\n",
      "epoch: 4 batch_num: 97 loss: 19.5968 acc: 0.84375\n",
      "epoch: 4 batch_num: 98 loss: 25.0469 acc: 0.828125\n",
      "epoch: 4 batch_num: 99 loss: 14.6128 acc: 0.921875\n",
      "epoch: 4 batch_num: 100 loss: 20.3717 acc: 0.828125\n",
      "epoch: 4 batch_num: 101 loss: 16.3876 acc: 0.921875\n",
      "epoch: 4 batch_num: 102 loss: 14.4102 acc: 0.921875\n",
      "epoch: 4 batch_num: 103 loss: 13.3409 acc: 0.9375\n",
      "epoch: 4 batch_num: 104 loss: 16.7225 acc: 0.875\n",
      "epoch: 4 batch_num: 105 loss: 19.8112 acc: 0.859375\n",
      "epoch: 4 batch_num: 106 loss: 21.2122 acc: 0.875\n",
      "epoch: 4 batch_num: 107 loss: 12.3221 acc: 0.921875\n",
      "epoch: 4 batch_num: 108 loss: 16.4552 acc: 0.90625\n",
      "epoch: 4 batch_num: 109 loss: 18.4244 acc: 0.890625\n",
      "epoch: 4 batch_num: 110 loss: 20.7267 acc: 0.875\n",
      "epoch: 4 batch_num: 111 loss: 22.5814 acc: 0.78125\n",
      "epoch: 4 batch_num: 112 loss: 21.2696 acc: 0.859375\n",
      "epoch: 4 batch_num: 113 loss: 25.0459 acc: 0.859375\n",
      "epoch: 4 batch_num: 114 loss: 21.9944 acc: 0.828125\n",
      "epoch: 4 batch_num: 115 loss: 24.0753 acc: 0.875\n",
      "epoch: 4 batch_num: 116 loss: 13.6024 acc: 0.7241379310344828\n",
      "epoch: 5 batch_num: 1 loss: 9.848 acc: 0.96875\n",
      "epoch: 5 batch_num: 2 loss: 14.105 acc: 0.921875\n",
      "epoch: 5 batch_num: 3 loss: 10.1332 acc: 0.984375\n",
      "epoch: 5 batch_num: 4 loss: 15.1202 acc: 0.890625\n",
      "epoch: 5 batch_num: 5 loss: 11.2571 acc: 0.96875\n",
      "epoch: 5 batch_num: 6 loss: 13.2199 acc: 0.921875\n",
      "epoch: 5 batch_num: 7 loss: 18.9807 acc: 0.921875\n",
      "epoch: 5 batch_num: 8 loss: 9.0605 acc: 0.96875\n",
      "epoch: 5 batch_num: 9 loss: 10.7668 acc: 0.96875\n",
      "epoch: 5 batch_num: 10 loss: 10.63 acc: 0.96875\n",
      "epoch: 5 batch_num: 11 loss: 12.8431 acc: 0.9375\n",
      "epoch: 5 batch_num: 12 loss: 16.8794 acc: 0.9375\n",
      "epoch: 5 batch_num: 13 loss: 17.4308 acc: 0.90625\n",
      "epoch: 5 batch_num: 14 loss: 14.6263 acc: 0.875\n",
      "epoch: 5 batch_num: 15 loss: 15.3633 acc: 0.921875\n",
      "epoch: 5 batch_num: 16 loss: 15.7459 acc: 0.90625\n",
      "epoch: 5 batch_num: 17 loss: 18.1756 acc: 0.859375\n",
      "epoch: 5 batch_num: 18 loss: 14.7465 acc: 0.875\n",
      "epoch: 5 batch_num: 19 loss: 14.9729 acc: 0.921875\n",
      "epoch: 5 batch_num: 20 loss: 16.2529 acc: 0.890625\n",
      "epoch: 5 batch_num: 21 loss: 9.3732 acc: 0.96875\n",
      "epoch: 5 batch_num: 22 loss: 12.7836 acc: 0.96875\n",
      "epoch: 5 batch_num: 23 loss: 11.3804 acc: 0.953125\n",
      "epoch: 5 batch_num: 24 loss: 10.9586 acc: 0.921875\n",
      "epoch: 5 batch_num: 25 loss: 13.6118 acc: 0.96875\n",
      "epoch: 5 batch_num: 26 loss: 10.5379 acc: 0.921875\n",
      "epoch: 5 batch_num: 27 loss: 11.6094 acc: 0.953125\n",
      "epoch: 5 batch_num: 28 loss: 13.6055 acc: 0.9375\n",
      "epoch: 5 batch_num: 29 loss: 10.5607 acc: 0.96875\n",
      "epoch: 5 batch_num: 30 loss: 12.4207 acc: 0.96875\n",
      "epoch: 5 batch_num: 31 loss: 16.3209 acc: 0.84375\n",
      "epoch: 5 batch_num: 32 loss: 13.6494 acc: 0.953125\n",
      "epoch: 5 batch_num: 33 loss: 14.6045 acc: 0.890625\n",
      "epoch: 5 batch_num: 34 loss: 19.855 acc: 0.875\n",
      "epoch: 5 batch_num: 35 loss: 17.1501 acc: 0.890625\n",
      "epoch: 5 batch_num: 36 loss: 19.4089 acc: 0.875\n",
      "epoch: 5 batch_num: 37 loss: 13.4158 acc: 0.921875\n",
      "epoch: 5 batch_num: 38 loss: 11.5165 acc: 0.953125\n",
      "epoch: 5 batch_num: 39 loss: 14.073 acc: 0.921875\n",
      "epoch: 5 batch_num: 40 loss: 12.0276 acc: 0.90625\n",
      "epoch: 5 batch_num: 41 loss: 16.3485 acc: 0.890625\n",
      "epoch: 5 batch_num: 42 loss: 16.0478 acc: 0.9375\n",
      "epoch: 5 batch_num: 43 loss: 14.0158 acc: 0.921875\n",
      "epoch: 5 batch_num: 44 loss: 21.278 acc: 0.859375\n",
      "epoch: 5 batch_num: 45 loss: 11.4699 acc: 0.984375\n",
      "epoch: 5 batch_num: 46 loss: 17.8651 acc: 0.9375\n",
      "epoch: 5 batch_num: 47 loss: 9.3538 acc: 0.96875\n",
      "epoch: 5 batch_num: 48 loss: 12.8412 acc: 0.96875\n",
      "epoch: 5 batch_num: 49 loss: 10.8567 acc: 0.953125\n",
      "epoch: 5 batch_num: 50 loss: 20.0502 acc: 0.859375\n",
      "epoch: 5 batch_num: 51 loss: 10.2507 acc: 0.984375\n",
      "epoch: 5 batch_num: 52 loss: 11.0923 acc: 0.90625\n",
      "epoch: 5 batch_num: 53 loss: 14.475 acc: 0.921875\n",
      "epoch: 5 batch_num: 54 loss: 11.8138 acc: 0.953125\n",
      "epoch: 5 batch_num: 55 loss: 14.3158 acc: 0.890625\n",
      "epoch: 5 batch_num: 56 loss: 12.2343 acc: 0.921875\n",
      "epoch: 5 batch_num: 57 loss: 19.6228 acc: 0.890625\n",
      "epoch: 5 batch_num: 58 loss: 13.5758 acc: 0.921875\n",
      "epoch: 5 batch_num: 59 loss: 17.3659 acc: 0.9375\n",
      "epoch: 5 batch_num: 60 loss: 13.3613 acc: 0.921875\n",
      "epoch: 5 batch_num: 61 loss: 13.0655 acc: 0.921875\n",
      "epoch: 5 batch_num: 62 loss: 21.3259 acc: 0.875\n",
      "epoch: 5 batch_num: 63 loss: 14.1577 acc: 0.890625\n",
      "epoch: 5 batch_num: 64 loss: 16.5059 acc: 0.875\n",
      "epoch: 5 batch_num: 65 loss: 18.5987 acc: 0.90625\n",
      "epoch: 5 batch_num: 66 loss: 11.5844 acc: 0.9375\n",
      "epoch: 5 batch_num: 67 loss: 12.1184 acc: 0.90625\n",
      "epoch: 5 batch_num: 68 loss: 22.0979 acc: 0.84375\n",
      "epoch: 5 batch_num: 69 loss: 10.5452 acc: 0.953125\n",
      "epoch: 5 batch_num: 70 loss: 9.5798 acc: 0.953125\n",
      "epoch: 5 batch_num: 71 loss: 13.8091 acc: 0.90625\n",
      "epoch: 5 batch_num: 72 loss: 9.7333 acc: 0.953125\n",
      "epoch: 5 batch_num: 73 loss: 14.2709 acc: 0.9375\n",
      "epoch: 5 batch_num: 74 loss: 14.7302 acc: 0.953125\n",
      "epoch: 5 batch_num: 75 loss: 18.706 acc: 0.875\n",
      "epoch: 5 batch_num: 76 loss: 21.9968 acc: 0.875\n",
      "epoch: 5 batch_num: 77 loss: 14.5169 acc: 0.890625\n",
      "epoch: 5 batch_num: 78 loss: 13.4961 acc: 0.9375\n",
      "epoch: 5 batch_num: 79 loss: 14.1646 acc: 0.890625\n",
      "epoch: 5 batch_num: 80 loss: 10.413 acc: 0.96875\n",
      "epoch: 5 batch_num: 81 loss: 12.4481 acc: 0.953125\n",
      "epoch: 5 batch_num: 82 loss: 18.4635 acc: 0.890625\n",
      "epoch: 5 batch_num: 83 loss: 15.4044 acc: 0.921875\n",
      "epoch: 5 batch_num: 84 loss: 14.8638 acc: 0.890625\n",
      "epoch: 5 batch_num: 85 loss: 19.7436 acc: 0.859375\n",
      "epoch: 5 batch_num: 86 loss: 17.4699 acc: 0.859375\n",
      "epoch: 5 batch_num: 87 loss: 17.0102 acc: 0.890625\n",
      "epoch: 5 batch_num: 88 loss: 10.6809 acc: 0.9375\n",
      "epoch: 5 batch_num: 89 loss: 13.9292 acc: 0.90625\n",
      "epoch: 5 batch_num: 90 loss: 10.6363 acc: 0.96875\n",
      "epoch: 5 batch_num: 91 loss: 12.9226 acc: 0.90625\n",
      "epoch: 5 batch_num: 92 loss: 12.8083 acc: 0.9375\n",
      "epoch: 5 batch_num: 93 loss: 17.244 acc: 0.90625\n",
      "epoch: 5 batch_num: 94 loss: 17.6988 acc: 0.90625\n",
      "epoch: 5 batch_num: 95 loss: 13.3773 acc: 0.921875\n",
      "epoch: 5 batch_num: 96 loss: 13.8721 acc: 0.921875\n",
      "epoch: 5 batch_num: 97 loss: 15.0203 acc: 0.9375\n",
      "epoch: 5 batch_num: 98 loss: 15.7272 acc: 0.921875\n",
      "epoch: 5 batch_num: 99 loss: 12.8453 acc: 0.9375\n",
      "epoch: 5 batch_num: 100 loss: 9.1148 acc: 0.96875\n",
      "epoch: 5 batch_num: 101 loss: 15.6642 acc: 0.90625\n",
      "epoch: 5 batch_num: 102 loss: 14.2953 acc: 0.921875\n",
      "epoch: 5 batch_num: 103 loss: 17.3335 acc: 0.875\n",
      "epoch: 5 batch_num: 104 loss: 7.8167 acc: 0.96875\n",
      "epoch: 5 batch_num: 105 loss: 14.9752 acc: 0.9375\n",
      "epoch: 5 batch_num: 106 loss: 20.547 acc: 0.84375\n",
      "epoch: 5 batch_num: 107 loss: 13.059 acc: 0.953125\n",
      "epoch: 5 batch_num: 108 loss: 7.8365 acc: 0.984375\n",
      "epoch: 5 batch_num: 109 loss: 15.2668 acc: 0.9375\n",
      "epoch: 5 batch_num: 110 loss: 12.1242 acc: 0.96875\n",
      "epoch: 5 batch_num: 111 loss: 10.2319 acc: 0.953125\n",
      "epoch: 5 batch_num: 112 loss: 10.9208 acc: 0.96875\n",
      "epoch: 5 batch_num: 113 loss: 12.918 acc: 0.96875\n",
      "epoch: 5 batch_num: 114 loss: 16.5483 acc: 0.9375\n",
      "epoch: 5 batch_num: 115 loss: 14.862 acc: 0.890625\n",
      "epoch: 5 batch_num: 116 loss: 7.4807 acc: 0.896551724137931\n",
      "epoch: 6 batch_num: 1 loss: 10.6158 acc: 0.953125\n",
      "epoch: 6 batch_num: 2 loss: 10.9853 acc: 0.921875\n",
      "epoch: 6 batch_num: 3 loss: 10.6637 acc: 0.96875\n",
      "epoch: 6 batch_num: 4 loss: 8.3604 acc: 0.96875\n",
      "epoch: 6 batch_num: 5 loss: 8.5588 acc: 0.953125\n",
      "epoch: 6 batch_num: 6 loss: 11.4993 acc: 0.921875\n",
      "epoch: 6 batch_num: 7 loss: 11.5763 acc: 0.9375\n",
      "epoch: 6 batch_num: 8 loss: 14.6445 acc: 0.9375\n",
      "epoch: 6 batch_num: 9 loss: 14.6527 acc: 0.921875\n",
      "epoch: 6 batch_num: 10 loss: 8.7083 acc: 0.953125\n",
      "epoch: 6 batch_num: 11 loss: 7.5324 acc: 1.0\n",
      "epoch: 6 batch_num: 12 loss: 14.7013 acc: 0.921875\n",
      "epoch: 6 batch_num: 13 loss: 10.0605 acc: 0.984375\n",
      "epoch: 6 batch_num: 14 loss: 9.5566 acc: 0.953125\n",
      "epoch: 6 batch_num: 15 loss: 12.7431 acc: 0.9375\n",
      "epoch: 6 batch_num: 16 loss: 8.7081 acc: 0.953125\n",
      "epoch: 6 batch_num: 17 loss: 12.4982 acc: 0.953125\n",
      "epoch: 6 batch_num: 18 loss: 12.3885 acc: 0.9375\n",
      "epoch: 6 batch_num: 19 loss: 9.9831 acc: 0.90625\n",
      "epoch: 6 batch_num: 20 loss: 10.2883 acc: 0.953125\n",
      "epoch: 6 batch_num: 21 loss: 12.27 acc: 0.953125\n",
      "epoch: 6 batch_num: 22 loss: 10.2902 acc: 0.9375\n",
      "epoch: 6 batch_num: 23 loss: 9.7017 acc: 0.953125\n",
      "epoch: 6 batch_num: 24 loss: 11.5466 acc: 0.9375\n",
      "epoch: 6 batch_num: 25 loss: 13.2022 acc: 0.90625\n",
      "epoch: 6 batch_num: 26 loss: 8.7041 acc: 0.96875\n",
      "epoch: 6 batch_num: 27 loss: 10.0581 acc: 0.96875\n",
      "epoch: 6 batch_num: 28 loss: 11.1765 acc: 0.96875\n",
      "epoch: 6 batch_num: 29 loss: 10.8909 acc: 0.96875\n",
      "epoch: 6 batch_num: 30 loss: 7.6122 acc: 0.96875\n",
      "epoch: 6 batch_num: 31 loss: 8.5684 acc: 0.953125\n",
      "epoch: 6 batch_num: 32 loss: 8.4376 acc: 0.984375\n",
      "epoch: 6 batch_num: 33 loss: 6.8973 acc: 0.96875\n",
      "epoch: 6 batch_num: 34 loss: 10.6465 acc: 0.9375\n",
      "epoch: 6 batch_num: 35 loss: 8.7814 acc: 0.984375\n",
      "epoch: 6 batch_num: 36 loss: 10.8275 acc: 0.953125\n",
      "epoch: 6 batch_num: 37 loss: 11.8043 acc: 0.96875\n",
      "epoch: 6 batch_num: 38 loss: 7.3476 acc: 0.984375\n",
      "epoch: 6 batch_num: 39 loss: 10.0474 acc: 0.953125\n",
      "epoch: 6 batch_num: 40 loss: 8.3109 acc: 0.953125\n",
      "epoch: 6 batch_num: 41 loss: 10.554 acc: 0.953125\n",
      "epoch: 6 batch_num: 42 loss: 9.8265 acc: 0.984375\n",
      "epoch: 6 batch_num: 43 loss: 11.324 acc: 0.9375\n",
      "epoch: 6 batch_num: 44 loss: 8.0711 acc: 0.953125\n",
      "epoch: 6 batch_num: 45 loss: 12.691 acc: 0.921875\n",
      "epoch: 6 batch_num: 46 loss: 8.6946 acc: 0.984375\n",
      "epoch: 6 batch_num: 47 loss: 12.037 acc: 0.96875\n",
      "epoch: 6 batch_num: 48 loss: 12.5598 acc: 0.890625\n",
      "epoch: 6 batch_num: 49 loss: 9.3546 acc: 0.96875\n",
      "epoch: 6 batch_num: 50 loss: 8.5392 acc: 0.984375\n",
      "epoch: 6 batch_num: 51 loss: 11.9455 acc: 0.953125\n",
      "epoch: 6 batch_num: 52 loss: 9.7644 acc: 0.953125\n",
      "epoch: 6 batch_num: 53 loss: 9.2682 acc: 0.96875\n",
      "epoch: 6 batch_num: 54 loss: 14.5108 acc: 0.890625\n",
      "epoch: 6 batch_num: 55 loss: 14.5992 acc: 0.9375\n",
      "epoch: 6 batch_num: 56 loss: 7.8922 acc: 0.984375\n",
      "epoch: 6 batch_num: 57 loss: 10.9814 acc: 0.953125\n",
      "epoch: 6 batch_num: 58 loss: 9.3555 acc: 0.9375\n",
      "epoch: 6 batch_num: 59 loss: 4.8075 acc: 1.0\n",
      "epoch: 6 batch_num: 60 loss: 9.8384 acc: 0.9375\n",
      "epoch: 6 batch_num: 61 loss: 8.904 acc: 0.953125\n",
      "epoch: 6 batch_num: 62 loss: 9.8668 acc: 0.953125\n",
      "epoch: 6 batch_num: 63 loss: 14.2415 acc: 0.9375\n",
      "epoch: 6 batch_num: 64 loss: 7.4515 acc: 0.96875\n",
      "epoch: 6 batch_num: 65 loss: 13.3752 acc: 0.9375\n",
      "epoch: 6 batch_num: 66 loss: 10.0983 acc: 0.921875\n",
      "epoch: 6 batch_num: 67 loss: 7.0222 acc: 0.96875\n",
      "epoch: 6 batch_num: 68 loss: 13.3293 acc: 0.9375\n",
      "epoch: 6 batch_num: 69 loss: 5.8767 acc: 0.984375\n",
      "epoch: 6 batch_num: 70 loss: 7.6275 acc: 0.96875\n",
      "epoch: 6 batch_num: 71 loss: 13.1309 acc: 0.90625\n",
      "epoch: 6 batch_num: 72 loss: 10.9577 acc: 0.953125\n",
      "epoch: 6 batch_num: 73 loss: 8.9194 acc: 0.953125\n",
      "epoch: 6 batch_num: 74 loss: 19.1178 acc: 0.890625\n",
      "epoch: 6 batch_num: 75 loss: 17.1705 acc: 0.875\n",
      "epoch: 6 batch_num: 76 loss: 15.4611 acc: 0.90625\n",
      "epoch: 6 batch_num: 77 loss: 12.3641 acc: 0.921875\n",
      "epoch: 6 batch_num: 78 loss: 9.3573 acc: 0.953125\n",
      "epoch: 6 batch_num: 79 loss: 8.019 acc: 0.953125\n",
      "epoch: 6 batch_num: 80 loss: 9.511 acc: 0.953125\n",
      "epoch: 6 batch_num: 81 loss: 11.7111 acc: 0.96875\n",
      "epoch: 6 batch_num: 82 loss: 11.4516 acc: 0.9375\n",
      "epoch: 6 batch_num: 83 loss: 15.4894 acc: 0.890625\n",
      "epoch: 6 batch_num: 84 loss: 10.0605 acc: 0.9375\n",
      "epoch: 6 batch_num: 85 loss: 8.8711 acc: 0.9375\n",
      "epoch: 6 batch_num: 86 loss: 10.2618 acc: 0.953125\n",
      "epoch: 6 batch_num: 87 loss: 11.4488 acc: 0.953125\n",
      "epoch: 6 batch_num: 88 loss: 16.4734 acc: 0.90625\n",
      "epoch: 6 batch_num: 89 loss: 9.2697 acc: 0.9375\n",
      "epoch: 6 batch_num: 90 loss: 8.246 acc: 0.953125\n",
      "epoch: 6 batch_num: 91 loss: 16.1757 acc: 0.90625\n",
      "epoch: 6 batch_num: 92 loss: 16.6128 acc: 0.9375\n",
      "epoch: 6 batch_num: 93 loss: 10.2424 acc: 0.9375\n",
      "epoch: 6 batch_num: 94 loss: 12.9913 acc: 0.90625\n",
      "epoch: 6 batch_num: 95 loss: 10.0001 acc: 0.953125\n",
      "epoch: 6 batch_num: 96 loss: 13.0634 acc: 0.921875\n",
      "epoch: 6 batch_num: 97 loss: 10.609 acc: 0.953125\n",
      "epoch: 6 batch_num: 98 loss: 10.8442 acc: 0.9375\n",
      "epoch: 6 batch_num: 99 loss: 15.8285 acc: 0.921875\n",
      "epoch: 6 batch_num: 100 loss: 13.1899 acc: 0.921875\n",
      "epoch: 6 batch_num: 101 loss: 7.7624 acc: 0.953125\n",
      "epoch: 6 batch_num: 102 loss: 11.133 acc: 0.9375\n",
      "epoch: 6 batch_num: 103 loss: 8.2657 acc: 0.984375\n",
      "epoch: 6 batch_num: 104 loss: 8.6391 acc: 0.96875\n",
      "epoch: 6 batch_num: 105 loss: 17.2271 acc: 0.890625\n",
      "epoch: 6 batch_num: 106 loss: 10.0749 acc: 0.9375\n",
      "epoch: 6 batch_num: 107 loss: 8.8486 acc: 0.96875\n",
      "epoch: 6 batch_num: 108 loss: 11.061 acc: 0.953125\n",
      "epoch: 6 batch_num: 109 loss: 13.5273 acc: 0.9375\n",
      "epoch: 6 batch_num: 110 loss: 12.5492 acc: 0.9375\n",
      "epoch: 6 batch_num: 111 loss: 12.6801 acc: 0.90625\n",
      "epoch: 6 batch_num: 112 loss: 8.8335 acc: 0.96875\n",
      "epoch: 6 batch_num: 113 loss: 11.503 acc: 0.953125\n",
      "epoch: 6 batch_num: 114 loss: 11.8859 acc: 0.953125\n",
      "epoch: 6 batch_num: 115 loss: 7.9343 acc: 0.984375\n",
      "epoch: 6 batch_num: 116 loss: 6.7496 acc: 0.896551724137931\n",
      "epoch: 7 batch_num: 1 loss: 6.7194 acc: 0.9375\n",
      "epoch: 7 batch_num: 2 loss: 7.6475 acc: 0.953125\n",
      "epoch: 7 batch_num: 3 loss: 6.7638 acc: 0.96875\n",
      "epoch: 7 batch_num: 4 loss: 5.3832 acc: 0.984375\n",
      "epoch: 7 batch_num: 5 loss: 7.3261 acc: 0.96875\n",
      "epoch: 7 batch_num: 6 loss: 6.4307 acc: 0.984375\n",
      "epoch: 7 batch_num: 7 loss: 7.9242 acc: 0.96875\n",
      "epoch: 7 batch_num: 8 loss: 8.5492 acc: 0.96875\n",
      "epoch: 7 batch_num: 9 loss: 10.4582 acc: 0.953125\n",
      "epoch: 7 batch_num: 10 loss: 10.4929 acc: 0.9375\n",
      "epoch: 7 batch_num: 11 loss: 8.8864 acc: 0.9375\n",
      "epoch: 7 batch_num: 12 loss: 6.1878 acc: 1.0\n",
      "epoch: 7 batch_num: 13 loss: 9.7665 acc: 0.921875\n",
      "epoch: 7 batch_num: 14 loss: 5.5072 acc: 1.0\n",
      "epoch: 7 batch_num: 15 loss: 7.8991 acc: 0.96875\n",
      "epoch: 7 batch_num: 16 loss: 7.0151 acc: 0.984375\n",
      "epoch: 7 batch_num: 17 loss: 6.5997 acc: 1.0\n",
      "epoch: 7 batch_num: 18 loss: 6.708 acc: 0.984375\n",
      "epoch: 7 batch_num: 19 loss: 9.8731 acc: 0.953125\n",
      "epoch: 7 batch_num: 20 loss: 11.6042 acc: 0.9375\n",
      "epoch: 7 batch_num: 21 loss: 11.8425 acc: 0.9375\n",
      "epoch: 7 batch_num: 22 loss: 5.4996 acc: 0.984375\n",
      "epoch: 7 batch_num: 23 loss: 5.1541 acc: 0.984375\n",
      "epoch: 7 batch_num: 24 loss: 10.1952 acc: 0.953125\n",
      "epoch: 7 batch_num: 25 loss: 6.8549 acc: 1.0\n",
      "epoch: 7 batch_num: 26 loss: 6.213 acc: 0.984375\n",
      "epoch: 7 batch_num: 27 loss: 8.0093 acc: 0.96875\n",
      "epoch: 7 batch_num: 28 loss: 4.9133 acc: 1.0\n",
      "epoch: 7 batch_num: 29 loss: 5.714 acc: 1.0\n",
      "epoch: 7 batch_num: 30 loss: 10.0291 acc: 0.953125\n",
      "epoch: 7 batch_num: 31 loss: 6.2678 acc: 0.984375\n",
      "epoch: 7 batch_num: 32 loss: 6.2515 acc: 0.984375\n",
      "epoch: 7 batch_num: 33 loss: 7.8139 acc: 0.953125\n",
      "epoch: 7 batch_num: 34 loss: 7.7395 acc: 0.984375\n",
      "epoch: 7 batch_num: 35 loss: 7.1538 acc: 0.96875\n",
      "epoch: 7 batch_num: 36 loss: 11.7495 acc: 0.921875\n",
      "epoch: 7 batch_num: 37 loss: 6.8362 acc: 1.0\n",
      "epoch: 7 batch_num: 38 loss: 9.8831 acc: 0.96875\n",
      "epoch: 7 batch_num: 39 loss: 7.284 acc: 0.96875\n",
      "epoch: 7 batch_num: 40 loss: 8.2432 acc: 0.96875\n",
      "epoch: 7 batch_num: 41 loss: 8.6295 acc: 0.984375\n",
      "epoch: 7 batch_num: 42 loss: 8.5223 acc: 0.953125\n",
      "epoch: 7 batch_num: 43 loss: 6.2562 acc: 0.984375\n",
      "epoch: 7 batch_num: 44 loss: 8.6114 acc: 0.96875\n",
      "epoch: 7 batch_num: 45 loss: 10.0582 acc: 0.96875\n",
      "epoch: 7 batch_num: 46 loss: 5.9344 acc: 0.984375\n",
      "epoch: 7 batch_num: 47 loss: 9.781 acc: 0.921875\n",
      "epoch: 7 batch_num: 48 loss: 6.0991 acc: 0.984375\n",
      "epoch: 7 batch_num: 49 loss: 7.9332 acc: 0.984375\n",
      "epoch: 7 batch_num: 50 loss: 9.4434 acc: 0.953125\n",
      "epoch: 7 batch_num: 51 loss: 5.6822 acc: 0.984375\n",
      "epoch: 7 batch_num: 52 loss: 7.9025 acc: 0.96875\n",
      "epoch: 7 batch_num: 53 loss: 9.7356 acc: 0.90625\n",
      "epoch: 7 batch_num: 54 loss: 6.5507 acc: 0.984375\n",
      "epoch: 7 batch_num: 55 loss: 6.5358 acc: 0.984375\n",
      "epoch: 7 batch_num: 56 loss: 6.3761 acc: 0.984375\n",
      "epoch: 7 batch_num: 57 loss: 7.375 acc: 0.96875\n",
      "epoch: 7 batch_num: 58 loss: 4.0662 acc: 1.0\n",
      "epoch: 7 batch_num: 59 loss: 10.8498 acc: 0.953125\n",
      "epoch: 7 batch_num: 60 loss: 6.9496 acc: 0.953125\n",
      "epoch: 7 batch_num: 61 loss: 10.5745 acc: 0.921875\n",
      "epoch: 7 batch_num: 62 loss: 11.4331 acc: 0.9375\n",
      "epoch: 7 batch_num: 63 loss: 7.293 acc: 0.96875\n",
      "epoch: 7 batch_num: 64 loss: 4.8548 acc: 1.0\n",
      "epoch: 7 batch_num: 65 loss: 6.9394 acc: 0.984375\n",
      "epoch: 7 batch_num: 66 loss: 6.3357 acc: 0.984375\n",
      "epoch: 7 batch_num: 67 loss: 11.7733 acc: 0.9375\n",
      "epoch: 7 batch_num: 68 loss: 7.8741 acc: 0.96875\n",
      "epoch: 7 batch_num: 69 loss: 7.1214 acc: 0.96875\n",
      "epoch: 7 batch_num: 70 loss: 9.968 acc: 0.9375\n",
      "epoch: 7 batch_num: 71 loss: 7.997 acc: 0.984375\n",
      "epoch: 7 batch_num: 72 loss: 13.4012 acc: 0.921875\n",
      "epoch: 7 batch_num: 73 loss: 4.4828 acc: 0.984375\n",
      "epoch: 7 batch_num: 74 loss: 6.4384 acc: 0.984375\n",
      "epoch: 7 batch_num: 75 loss: 8.9482 acc: 0.96875\n",
      "epoch: 7 batch_num: 76 loss: 7.3038 acc: 0.96875\n",
      "epoch: 7 batch_num: 77 loss: 9.9361 acc: 0.96875\n",
      "epoch: 7 batch_num: 78 loss: 10.1165 acc: 0.9375\n",
      "epoch: 7 batch_num: 79 loss: 9.8309 acc: 0.96875\n",
      "epoch: 7 batch_num: 80 loss: 10.2341 acc: 0.953125\n",
      "epoch: 7 batch_num: 81 loss: 10.8789 acc: 0.953125\n",
      "epoch: 7 batch_num: 82 loss: 10.2325 acc: 0.953125\n",
      "epoch: 7 batch_num: 83 loss: 10.8639 acc: 0.953125\n",
      "epoch: 7 batch_num: 84 loss: 4.6184 acc: 0.984375\n",
      "epoch: 7 batch_num: 85 loss: 12.7916 acc: 0.9375\n",
      "epoch: 7 batch_num: 86 loss: 6.534 acc: 0.96875\n",
      "epoch: 7 batch_num: 87 loss: 8.8579 acc: 0.921875\n",
      "epoch: 7 batch_num: 88 loss: 8.4044 acc: 0.9375\n",
      "epoch: 7 batch_num: 89 loss: 9.6038 acc: 0.953125\n",
      "epoch: 7 batch_num: 90 loss: 9.7985 acc: 0.953125\n",
      "epoch: 7 batch_num: 91 loss: 7.1866 acc: 1.0\n",
      "epoch: 7 batch_num: 92 loss: 10.4947 acc: 0.953125\n",
      "epoch: 7 batch_num: 93 loss: 9.1681 acc: 0.96875\n",
      "epoch: 7 batch_num: 94 loss: 6.2512 acc: 0.96875\n",
      "epoch: 7 batch_num: 95 loss: 10.6031 acc: 0.921875\n",
      "epoch: 7 batch_num: 96 loss: 6.4628 acc: 0.984375\n",
      "epoch: 7 batch_num: 97 loss: 7.3651 acc: 0.96875\n",
      "epoch: 7 batch_num: 98 loss: 9.1959 acc: 0.9375\n",
      "epoch: 7 batch_num: 99 loss: 9.5136 acc: 0.9375\n",
      "epoch: 7 batch_num: 100 loss: 12.362 acc: 0.9375\n",
      "epoch: 7 batch_num: 101 loss: 12.9715 acc: 0.90625\n",
      "epoch: 7 batch_num: 102 loss: 6.7634 acc: 0.984375\n",
      "epoch: 7 batch_num: 103 loss: 4.6603 acc: 1.0\n",
      "epoch: 7 batch_num: 104 loss: 6.0154 acc: 0.984375\n",
      "epoch: 7 batch_num: 105 loss: 8.8695 acc: 0.96875\n",
      "epoch: 7 batch_num: 106 loss: 7.8137 acc: 0.96875\n",
      "epoch: 7 batch_num: 107 loss: 11.2026 acc: 0.953125\n",
      "epoch: 7 batch_num: 108 loss: 8.768 acc: 0.953125\n",
      "epoch: 7 batch_num: 109 loss: 8.9464 acc: 0.96875\n",
      "epoch: 7 batch_num: 110 loss: 5.2178 acc: 0.984375\n",
      "epoch: 7 batch_num: 111 loss: 8.0521 acc: 0.953125\n",
      "epoch: 7 batch_num: 112 loss: 15.1582 acc: 0.953125\n",
      "epoch: 7 batch_num: 113 loss: 8.1347 acc: 0.96875\n",
      "epoch: 7 batch_num: 114 loss: 8.2082 acc: 0.953125\n",
      "epoch: 7 batch_num: 115 loss: 9.4555 acc: 0.96875\n",
      "epoch: 7 batch_num: 116 loss: 3.1613 acc: 0.9655172413793104\n",
      "epoch: 8 batch_num: 1 loss: 5.227 acc: 1.0\n",
      "epoch: 8 batch_num: 2 loss: 7.0208 acc: 0.96875\n",
      "epoch: 8 batch_num: 3 loss: 6.5753 acc: 0.96875\n",
      "epoch: 8 batch_num: 4 loss: 3.6916 acc: 1.0\n",
      "epoch: 8 batch_num: 5 loss: 4.3188 acc: 1.0\n",
      "epoch: 8 batch_num: 6 loss: 4.8279 acc: 1.0\n",
      "epoch: 8 batch_num: 7 loss: 5.5611 acc: 0.984375\n",
      "epoch: 8 batch_num: 8 loss: 6.7736 acc: 0.984375\n",
      "epoch: 8 batch_num: 9 loss: 5.2392 acc: 0.984375\n",
      "epoch: 8 batch_num: 10 loss: 4.3856 acc: 1.0\n",
      "epoch: 8 batch_num: 11 loss: 3.6822 acc: 1.0\n",
      "epoch: 8 batch_num: 12 loss: 9.1854 acc: 0.953125\n",
      "epoch: 8 batch_num: 13 loss: 5.9424 acc: 0.984375\n",
      "epoch: 8 batch_num: 14 loss: 4.0473 acc: 1.0\n",
      "epoch: 8 batch_num: 15 loss: 3.219 acc: 1.0\n",
      "epoch: 8 batch_num: 16 loss: 8.0933 acc: 0.96875\n",
      "epoch: 8 batch_num: 17 loss: 3.9723 acc: 1.0\n",
      "epoch: 8 batch_num: 18 loss: 5.1604 acc: 0.96875\n",
      "epoch: 8 batch_num: 19 loss: 4.1899 acc: 1.0\n",
      "epoch: 8 batch_num: 20 loss: 8.7295 acc: 0.984375\n",
      "epoch: 8 batch_num: 21 loss: 4.1634 acc: 0.984375\n",
      "epoch: 8 batch_num: 22 loss: 4.6213 acc: 1.0\n",
      "epoch: 8 batch_num: 23 loss: 7.5775 acc: 0.96875\n",
      "epoch: 8 batch_num: 24 loss: 9.6314 acc: 0.953125\n",
      "epoch: 8 batch_num: 25 loss: 7.7928 acc: 0.953125\n",
      "epoch: 8 batch_num: 26 loss: 6.9668 acc: 0.96875\n",
      "epoch: 8 batch_num: 27 loss: 3.7496 acc: 1.0\n",
      "epoch: 8 batch_num: 28 loss: 6.8388 acc: 0.984375\n",
      "epoch: 8 batch_num: 29 loss: 3.967 acc: 1.0\n",
      "epoch: 8 batch_num: 30 loss: 4.4614 acc: 1.0\n",
      "epoch: 8 batch_num: 31 loss: 8.4319 acc: 0.953125\n",
      "epoch: 8 batch_num: 32 loss: 9.3339 acc: 0.953125\n",
      "epoch: 8 batch_num: 33 loss: 5.8499 acc: 1.0\n",
      "epoch: 8 batch_num: 34 loss: 5.7167 acc: 0.984375\n",
      "epoch: 8 batch_num: 35 loss: 7.0304 acc: 0.953125\n",
      "epoch: 8 batch_num: 36 loss: 4.2593 acc: 1.0\n",
      "epoch: 8 batch_num: 37 loss: 5.5902 acc: 0.984375\n",
      "epoch: 8 batch_num: 38 loss: 8.1546 acc: 0.96875\n",
      "epoch: 8 batch_num: 39 loss: 6.471 acc: 0.984375\n",
      "epoch: 8 batch_num: 40 loss: 5.1633 acc: 1.0\n",
      "epoch: 8 batch_num: 41 loss: 3.8854 acc: 1.0\n",
      "epoch: 8 batch_num: 42 loss: 3.2704 acc: 1.0\n",
      "epoch: 8 batch_num: 43 loss: 10.1415 acc: 0.953125\n",
      "epoch: 8 batch_num: 44 loss: 3.0059 acc: 1.0\n",
      "epoch: 8 batch_num: 45 loss: 6.7243 acc: 0.984375\n",
      "epoch: 8 batch_num: 46 loss: 9.6376 acc: 0.984375\n",
      "epoch: 8 batch_num: 47 loss: 5.0621 acc: 0.96875\n",
      "epoch: 8 batch_num: 48 loss: 9.3062 acc: 0.9375\n",
      "epoch: 8 batch_num: 49 loss: 3.0889 acc: 1.0\n",
      "epoch: 8 batch_num: 50 loss: 4.4394 acc: 1.0\n",
      "epoch: 8 batch_num: 51 loss: 5.5544 acc: 0.96875\n",
      "epoch: 8 batch_num: 52 loss: 4.1553 acc: 1.0\n",
      "epoch: 8 batch_num: 53 loss: 3.3674 acc: 1.0\n",
      "epoch: 8 batch_num: 54 loss: 5.4662 acc: 0.96875\n",
      "epoch: 8 batch_num: 55 loss: 5.6758 acc: 0.96875\n",
      "epoch: 8 batch_num: 56 loss: 3.67 acc: 1.0\n",
      "epoch: 8 batch_num: 57 loss: 5.6258 acc: 0.984375\n",
      "epoch: 8 batch_num: 58 loss: 4.1748 acc: 0.984375\n",
      "epoch: 8 batch_num: 59 loss: 4.2973 acc: 1.0\n",
      "epoch: 8 batch_num: 60 loss: 5.7195 acc: 0.984375\n",
      "epoch: 8 batch_num: 61 loss: 4.2381 acc: 1.0\n",
      "epoch: 8 batch_num: 62 loss: 8.0474 acc: 0.96875\n",
      "epoch: 8 batch_num: 63 loss: 4.9887 acc: 0.984375\n",
      "epoch: 8 batch_num: 64 loss: 5.7265 acc: 0.96875\n",
      "epoch: 8 batch_num: 65 loss: 9.1133 acc: 0.953125\n",
      "epoch: 8 batch_num: 66 loss: 4.3447 acc: 1.0\n",
      "epoch: 8 batch_num: 67 loss: 7.0272 acc: 0.984375\n",
      "epoch: 8 batch_num: 68 loss: 9.2302 acc: 0.96875\n",
      "epoch: 8 batch_num: 69 loss: 4.9118 acc: 1.0\n",
      "epoch: 8 batch_num: 70 loss: 10.1067 acc: 0.984375\n",
      "epoch: 8 batch_num: 71 loss: 3.3576 acc: 1.0\n",
      "epoch: 8 batch_num: 72 loss: 4.8866 acc: 0.984375\n",
      "epoch: 8 batch_num: 73 loss: 3.8309 acc: 1.0\n",
      "epoch: 8 batch_num: 74 loss: 4.112 acc: 1.0\n",
      "epoch: 8 batch_num: 75 loss: 5.4356 acc: 0.953125\n",
      "epoch: 8 batch_num: 76 loss: 13.0058 acc: 0.96875\n",
      "epoch: 8 batch_num: 77 loss: 5.1014 acc: 0.984375\n",
      "epoch: 8 batch_num: 78 loss: 6.1675 acc: 0.96875\n",
      "epoch: 8 batch_num: 79 loss: 5.0614 acc: 1.0\n",
      "epoch: 8 batch_num: 80 loss: 7.6268 acc: 0.9375\n",
      "epoch: 8 batch_num: 81 loss: 6.6898 acc: 0.953125\n",
      "epoch: 8 batch_num: 82 loss: 4.6576 acc: 1.0\n",
      "epoch: 8 batch_num: 83 loss: 4.377 acc: 0.984375\n",
      "epoch: 8 batch_num: 84 loss: 6.6567 acc: 0.9375\n",
      "epoch: 8 batch_num: 85 loss: 8.7799 acc: 0.96875\n",
      "epoch: 8 batch_num: 86 loss: 4.297 acc: 0.984375\n",
      "epoch: 8 batch_num: 87 loss: 9.0797 acc: 0.96875\n",
      "epoch: 8 batch_num: 88 loss: 3.434 acc: 0.984375\n",
      "epoch: 8 batch_num: 89 loss: 5.2429 acc: 0.984375\n",
      "epoch: 8 batch_num: 90 loss: 6.2555 acc: 0.96875\n",
      "epoch: 8 batch_num: 91 loss: 4.8241 acc: 0.984375\n",
      "epoch: 8 batch_num: 92 loss: 9.4856 acc: 0.9375\n",
      "epoch: 8 batch_num: 93 loss: 6.4088 acc: 0.953125\n",
      "epoch: 8 batch_num: 94 loss: 8.4413 acc: 0.984375\n",
      "epoch: 8 batch_num: 95 loss: 7.199 acc: 0.953125\n",
      "epoch: 8 batch_num: 96 loss: 6.4594 acc: 0.984375\n",
      "epoch: 8 batch_num: 97 loss: 9.1987 acc: 0.96875\n",
      "epoch: 8 batch_num: 98 loss: 7.0951 acc: 0.96875\n",
      "epoch: 8 batch_num: 99 loss: 7.3394 acc: 0.96875\n",
      "epoch: 8 batch_num: 100 loss: 9.6364 acc: 0.953125\n",
      "epoch: 8 batch_num: 101 loss: 6.875 acc: 0.953125\n",
      "epoch: 8 batch_num: 102 loss: 7.7473 acc: 0.953125\n",
      "epoch: 8 batch_num: 103 loss: 9.7901 acc: 0.9375\n",
      "epoch: 8 batch_num: 104 loss: 5.8218 acc: 0.984375\n",
      "epoch: 8 batch_num: 105 loss: 6.4825 acc: 0.953125\n",
      "epoch: 8 batch_num: 106 loss: 3.8684 acc: 1.0\n",
      "epoch: 8 batch_num: 107 loss: 6.8559 acc: 0.96875\n",
      "epoch: 8 batch_num: 108 loss: 5.938 acc: 0.984375\n",
      "epoch: 8 batch_num: 109 loss: 7.5583 acc: 0.96875\n",
      "epoch: 8 batch_num: 110 loss: 5.2164 acc: 0.984375\n",
      "epoch: 8 batch_num: 111 loss: 11.4877 acc: 0.921875\n",
      "epoch: 8 batch_num: 112 loss: 5.4534 acc: 0.984375\n",
      "epoch: 8 batch_num: 113 loss: 6.5116 acc: 0.96875\n",
      "epoch: 8 batch_num: 114 loss: 4.5779 acc: 0.96875\n",
      "epoch: 8 batch_num: 115 loss: 3.8839 acc: 1.0\n",
      "epoch: 8 batch_num: 116 loss: 9.4152 acc: 0.8620689655172413\n",
      "epoch: 9 batch_num: 1 loss: 4.934 acc: 0.984375\n",
      "epoch: 9 batch_num: 2 loss: 5.0194 acc: 0.984375\n",
      "epoch: 9 batch_num: 3 loss: 4.1199 acc: 0.984375\n",
      "epoch: 9 batch_num: 4 loss: 4.6455 acc: 0.984375\n",
      "epoch: 9 batch_num: 5 loss: 4.3943 acc: 0.984375\n",
      "epoch: 9 batch_num: 6 loss: 4.4671 acc: 1.0\n",
      "epoch: 9 batch_num: 7 loss: 8.552 acc: 0.953125\n",
      "epoch: 9 batch_num: 8 loss: 3.8955 acc: 1.0\n",
      "epoch: 9 batch_num: 9 loss: 5.0667 acc: 0.984375\n",
      "epoch: 9 batch_num: 10 loss: 4.3433 acc: 0.984375\n",
      "epoch: 9 batch_num: 11 loss: 4.2463 acc: 0.984375\n",
      "epoch: 9 batch_num: 12 loss: 3.0793 acc: 0.984375\n",
      "epoch: 9 batch_num: 13 loss: 2.984 acc: 1.0\n",
      "epoch: 9 batch_num: 14 loss: 4.9107 acc: 0.984375\n",
      "epoch: 9 batch_num: 15 loss: 4.1617 acc: 0.984375\n",
      "epoch: 9 batch_num: 16 loss: 6.4461 acc: 0.984375\n",
      "epoch: 9 batch_num: 17 loss: 3.4124 acc: 1.0\n",
      "epoch: 9 batch_num: 18 loss: 5.9074 acc: 0.96875\n",
      "epoch: 9 batch_num: 19 loss: 4.1136 acc: 0.984375\n",
      "epoch: 9 batch_num: 20 loss: 3.0621 acc: 0.984375\n",
      "epoch: 9 batch_num: 21 loss: 5.8705 acc: 0.96875\n",
      "epoch: 9 batch_num: 22 loss: 2.2226 acc: 0.984375\n",
      "epoch: 9 batch_num: 23 loss: 6.2851 acc: 0.984375\n",
      "epoch: 9 batch_num: 24 loss: 3.8296 acc: 0.984375\n",
      "epoch: 9 batch_num: 25 loss: 3.304 acc: 1.0\n",
      "epoch: 9 batch_num: 26 loss: 4.1841 acc: 1.0\n",
      "epoch: 9 batch_num: 27 loss: 4.9336 acc: 0.96875\n",
      "epoch: 9 batch_num: 28 loss: 4.7744 acc: 1.0\n",
      "epoch: 9 batch_num: 29 loss: 2.4233 acc: 1.0\n",
      "epoch: 9 batch_num: 30 loss: 3.9401 acc: 1.0\n",
      "epoch: 9 batch_num: 31 loss: 3.7749 acc: 1.0\n",
      "epoch: 9 batch_num: 32 loss: 3.3391 acc: 1.0\n",
      "epoch: 9 batch_num: 33 loss: 5.3684 acc: 0.984375\n",
      "epoch: 9 batch_num: 34 loss: 5.0155 acc: 1.0\n",
      "epoch: 9 batch_num: 35 loss: 7.522 acc: 0.96875\n",
      "epoch: 9 batch_num: 36 loss: 3.5626 acc: 1.0\n",
      "epoch: 9 batch_num: 37 loss: 5.6104 acc: 0.984375\n",
      "epoch: 9 batch_num: 38 loss: 4.1174 acc: 0.96875\n",
      "epoch: 9 batch_num: 39 loss: 5.268 acc: 0.984375\n",
      "epoch: 9 batch_num: 40 loss: 4.6593 acc: 0.984375\n",
      "epoch: 9 batch_num: 41 loss: 3.4023 acc: 1.0\n",
      "epoch: 9 batch_num: 42 loss: 5.3544 acc: 0.984375\n",
      "epoch: 9 batch_num: 43 loss: 5.116 acc: 0.984375\n",
      "epoch: 9 batch_num: 44 loss: 4.5705 acc: 0.984375\n",
      "epoch: 9 batch_num: 45 loss: 11.4497 acc: 0.9375\n",
      "epoch: 9 batch_num: 46 loss: 5.9051 acc: 0.96875\n",
      "epoch: 9 batch_num: 47 loss: 4.4255 acc: 0.984375\n",
      "epoch: 9 batch_num: 48 loss: 4.5359 acc: 1.0\n",
      "epoch: 9 batch_num: 49 loss: 2.5693 acc: 1.0\n",
      "epoch: 9 batch_num: 50 loss: 3.5568 acc: 0.984375\n",
      "epoch: 9 batch_num: 51 loss: 2.8952 acc: 1.0\n",
      "epoch: 9 batch_num: 52 loss: 6.1162 acc: 0.984375\n",
      "epoch: 9 batch_num: 53 loss: 4.1321 acc: 1.0\n",
      "epoch: 9 batch_num: 54 loss: 6.655 acc: 0.984375\n",
      "epoch: 9 batch_num: 55 loss: 4.9179 acc: 1.0\n",
      "epoch: 9 batch_num: 56 loss: 6.447 acc: 0.953125\n",
      "epoch: 9 batch_num: 57 loss: 3.5177 acc: 1.0\n",
      "epoch: 9 batch_num: 58 loss: 3.4747 acc: 1.0\n",
      "epoch: 9 batch_num: 59 loss: 3.4444 acc: 1.0\n",
      "epoch: 9 batch_num: 60 loss: 7.8856 acc: 0.984375\n",
      "epoch: 9 batch_num: 61 loss: 3.6696 acc: 0.984375\n",
      "epoch: 9 batch_num: 62 loss: 7.6009 acc: 0.96875\n",
      "epoch: 9 batch_num: 63 loss: 6.626 acc: 0.96875\n",
      "epoch: 9 batch_num: 64 loss: 4.8964 acc: 0.984375\n",
      "epoch: 9 batch_num: 65 loss: 3.4317 acc: 1.0\n",
      "epoch: 9 batch_num: 66 loss: 3.134 acc: 1.0\n",
      "epoch: 9 batch_num: 67 loss: 6.4926 acc: 0.984375\n",
      "epoch: 9 batch_num: 68 loss: 3.422 acc: 1.0\n",
      "epoch: 9 batch_num: 69 loss: 3.4956 acc: 0.984375\n",
      "epoch: 9 batch_num: 70 loss: 3.6625 acc: 1.0\n",
      "epoch: 9 batch_num: 71 loss: 2.7902 acc: 1.0\n",
      "epoch: 9 batch_num: 72 loss: 3.0398 acc: 0.984375\n",
      "epoch: 9 batch_num: 73 loss: 6.8075 acc: 0.984375\n",
      "epoch: 9 batch_num: 74 loss: 3.8796 acc: 1.0\n",
      "epoch: 9 batch_num: 75 loss: 6.7684 acc: 0.96875\n",
      "epoch: 9 batch_num: 76 loss: 8.8665 acc: 0.953125\n",
      "epoch: 9 batch_num: 77 loss: 3.2816 acc: 1.0\n",
      "epoch: 9 batch_num: 78 loss: 3.2696 acc: 0.984375\n",
      "epoch: 9 batch_num: 79 loss: 4.0256 acc: 1.0\n",
      "epoch: 9 batch_num: 80 loss: 4.0307 acc: 1.0\n",
      "epoch: 9 batch_num: 81 loss: 5.6431 acc: 0.96875\n",
      "epoch: 9 batch_num: 82 loss: 4.9752 acc: 0.984375\n",
      "epoch: 9 batch_num: 83 loss: 4.6214 acc: 0.984375\n",
      "epoch: 9 batch_num: 84 loss: 3.6759 acc: 1.0\n",
      "epoch: 9 batch_num: 85 loss: 4.577 acc: 0.984375\n",
      "epoch: 9 batch_num: 86 loss: 3.7244 acc: 1.0\n",
      "epoch: 9 batch_num: 87 loss: 2.7704 acc: 1.0\n",
      "epoch: 9 batch_num: 88 loss: 4.7487 acc: 0.984375\n",
      "epoch: 9 batch_num: 89 loss: 7.0557 acc: 0.96875\n",
      "epoch: 9 batch_num: 90 loss: 6.0323 acc: 0.984375\n",
      "epoch: 9 batch_num: 91 loss: 3.7378 acc: 1.0\n",
      "epoch: 9 batch_num: 92 loss: 4.9548 acc: 1.0\n",
      "epoch: 9 batch_num: 93 loss: 8.0829 acc: 0.953125\n",
      "epoch: 9 batch_num: 94 loss: 3.2875 acc: 1.0\n",
      "epoch: 9 batch_num: 95 loss: 3.8254 acc: 1.0\n",
      "epoch: 9 batch_num: 96 loss: 3.5196 acc: 0.984375\n",
      "epoch: 9 batch_num: 97 loss: 3.2602 acc: 0.984375\n",
      "epoch: 9 batch_num: 98 loss: 5.6066 acc: 0.953125\n",
      "epoch: 9 batch_num: 99 loss: 4.2312 acc: 1.0\n",
      "epoch: 9 batch_num: 100 loss: 4.6956 acc: 1.0\n",
      "epoch: 9 batch_num: 101 loss: 4.8513 acc: 0.984375\n",
      "epoch: 9 batch_num: 102 loss: 5.441 acc: 0.984375\n",
      "epoch: 9 batch_num: 103 loss: 4.8833 acc: 0.984375\n",
      "epoch: 9 batch_num: 104 loss: 3.1064 acc: 1.0\n",
      "epoch: 9 batch_num: 105 loss: 8.66 acc: 0.953125\n",
      "epoch: 9 batch_num: 106 loss: 3.4255 acc: 1.0\n",
      "epoch: 9 batch_num: 107 loss: 6.8767 acc: 0.96875\n",
      "epoch: 9 batch_num: 108 loss: 5.5603 acc: 0.96875\n",
      "epoch: 9 batch_num: 109 loss: 2.142 acc: 1.0\n",
      "epoch: 9 batch_num: 110 loss: 3.6911 acc: 1.0\n",
      "epoch: 9 batch_num: 111 loss: 5.8708 acc: 0.984375\n",
      "epoch: 9 batch_num: 112 loss: 7.2901 acc: 0.96875\n",
      "epoch: 9 batch_num: 113 loss: 2.2864 acc: 1.0\n",
      "epoch: 9 batch_num: 114 loss: 4.6006 acc: 0.96875\n",
      "epoch: 9 batch_num: 115 loss: 5.3431 acc: 0.953125\n",
      "epoch: 9 batch_num: 116 loss: 1.6601 acc: 1.0\n",
      "epoch: 10 batch_num: 1 loss: 2.2652 acc: 1.0\n",
      "epoch: 10 batch_num: 2 loss: 3.3514 acc: 0.984375\n",
      "epoch: 10 batch_num: 3 loss: 3.4077 acc: 0.984375\n",
      "epoch: 10 batch_num: 4 loss: 2.6028 acc: 1.0\n",
      "epoch: 10 batch_num: 5 loss: 4.0175 acc: 1.0\n",
      "epoch: 10 batch_num: 6 loss: 3.5048 acc: 1.0\n",
      "epoch: 10 batch_num: 7 loss: 2.8731 acc: 1.0\n",
      "epoch: 10 batch_num: 8 loss: 2.8373 acc: 1.0\n",
      "epoch: 10 batch_num: 9 loss: 5.4821 acc: 0.984375\n",
      "epoch: 10 batch_num: 10 loss: 2.5327 acc: 1.0\n",
      "epoch: 10 batch_num: 11 loss: 1.8628 acc: 1.0\n",
      "epoch: 10 batch_num: 12 loss: 2.2975 acc: 0.984375\n",
      "epoch: 10 batch_num: 13 loss: 2.8295 acc: 1.0\n",
      "epoch: 10 batch_num: 14 loss: 3.4121 acc: 0.984375\n",
      "epoch: 10 batch_num: 15 loss: 3.6805 acc: 0.984375\n",
      "epoch: 10 batch_num: 16 loss: 3.9494 acc: 0.984375\n",
      "epoch: 10 batch_num: 17 loss: 1.7633 acc: 1.0\n",
      "epoch: 10 batch_num: 18 loss: 4.9444 acc: 0.984375\n",
      "epoch: 10 batch_num: 19 loss: 3.5665 acc: 0.984375\n",
      "epoch: 10 batch_num: 20 loss: 2.6311 acc: 1.0\n",
      "epoch: 10 batch_num: 21 loss: 5.3393 acc: 0.984375\n",
      "epoch: 10 batch_num: 22 loss: 5.219 acc: 0.984375\n",
      "epoch: 10 batch_num: 23 loss: 3.1313 acc: 1.0\n",
      "epoch: 10 batch_num: 24 loss: 8.099 acc: 0.984375\n",
      "epoch: 10 batch_num: 25 loss: 3.1837 acc: 0.984375\n",
      "epoch: 10 batch_num: 26 loss: 3.0011 acc: 1.0\n",
      "epoch: 10 batch_num: 27 loss: 2.5835 acc: 1.0\n",
      "epoch: 10 batch_num: 28 loss: 4.9703 acc: 0.984375\n",
      "epoch: 10 batch_num: 29 loss: 4.0369 acc: 0.984375\n",
      "epoch: 10 batch_num: 30 loss: 2.8071 acc: 1.0\n",
      "epoch: 10 batch_num: 31 loss: 2.4388 acc: 1.0\n",
      "epoch: 10 batch_num: 32 loss: 2.7049 acc: 1.0\n",
      "epoch: 10 batch_num: 33 loss: 4.4327 acc: 0.984375\n",
      "epoch: 10 batch_num: 34 loss: 6.6655 acc: 0.96875\n",
      "epoch: 10 batch_num: 35 loss: 4.5383 acc: 1.0\n",
      "epoch: 10 batch_num: 36 loss: 4.8916 acc: 0.96875\n",
      "epoch: 10 batch_num: 37 loss: 2.4894 acc: 0.984375\n",
      "epoch: 10 batch_num: 38 loss: 4.8979 acc: 0.984375\n",
      "epoch: 10 batch_num: 39 loss: 2.8877 acc: 1.0\n",
      "epoch: 10 batch_num: 40 loss: 3.0954 acc: 0.984375\n",
      "epoch: 10 batch_num: 41 loss: 1.7671 acc: 1.0\n",
      "epoch: 10 batch_num: 42 loss: 2.6302 acc: 1.0\n",
      "epoch: 10 batch_num: 43 loss: 5.2858 acc: 0.984375\n",
      "epoch: 10 batch_num: 44 loss: 4.95 acc: 0.984375\n",
      "epoch: 10 batch_num: 45 loss: 5.7186 acc: 0.984375\n",
      "epoch: 10 batch_num: 46 loss: 2.3142 acc: 1.0\n",
      "epoch: 10 batch_num: 47 loss: 4.487 acc: 0.984375\n",
      "epoch: 10 batch_num: 48 loss: 5.9397 acc: 0.984375\n",
      "epoch: 10 batch_num: 49 loss: 2.5815 acc: 1.0\n",
      "epoch: 10 batch_num: 50 loss: 2.7908 acc: 1.0\n",
      "epoch: 10 batch_num: 51 loss: 4.5638 acc: 0.96875\n",
      "epoch: 10 batch_num: 52 loss: 3.0429 acc: 0.984375\n",
      "epoch: 10 batch_num: 53 loss: 3.2089 acc: 0.984375\n",
      "epoch: 10 batch_num: 54 loss: 1.8182 acc: 1.0\n",
      "epoch: 10 batch_num: 55 loss: 6.0513 acc: 0.984375\n",
      "epoch: 10 batch_num: 56 loss: 1.7877 acc: 1.0\n",
      "epoch: 10 batch_num: 57 loss: 2.5381 acc: 1.0\n",
      "epoch: 10 batch_num: 58 loss: 4.6211 acc: 0.984375\n",
      "epoch: 10 batch_num: 59 loss: 2.2753 acc: 1.0\n",
      "epoch: 10 batch_num: 60 loss: 2.9108 acc: 1.0\n",
      "epoch: 10 batch_num: 61 loss: 4.2978 acc: 0.96875\n",
      "epoch: 10 batch_num: 62 loss: 1.8764 acc: 1.0\n",
      "epoch: 10 batch_num: 63 loss: 3.5892 acc: 0.984375\n",
      "epoch: 10 batch_num: 64 loss: 3.8917 acc: 0.984375\n",
      "epoch: 10 batch_num: 65 loss: 3.1856 acc: 1.0\n",
      "epoch: 10 batch_num: 66 loss: 1.6655 acc: 1.0\n",
      "epoch: 10 batch_num: 67 loss: 3.3556 acc: 1.0\n",
      "epoch: 10 batch_num: 68 loss: 3.5502 acc: 1.0\n",
      "epoch: 10 batch_num: 69 loss: 4.3234 acc: 0.984375\n",
      "epoch: 10 batch_num: 70 loss: 3.0164 acc: 0.96875\n",
      "epoch: 10 batch_num: 71 loss: 3.5001 acc: 0.984375\n",
      "epoch: 10 batch_num: 72 loss: 2.2482 acc: 1.0\n",
      "epoch: 10 batch_num: 73 loss: 3.4944 acc: 0.984375\n",
      "epoch: 10 batch_num: 74 loss: 4.3387 acc: 0.984375\n",
      "epoch: 10 batch_num: 75 loss: 2.1157 acc: 1.0\n",
      "epoch: 10 batch_num: 76 loss: 3.207 acc: 0.984375\n",
      "epoch: 10 batch_num: 77 loss: 2.9957 acc: 1.0\n",
      "epoch: 10 batch_num: 78 loss: 6.5238 acc: 0.953125\n",
      "epoch: 10 batch_num: 79 loss: 3.1472 acc: 1.0\n",
      "epoch: 10 batch_num: 80 loss: 3.6423 acc: 0.984375\n",
      "epoch: 10 batch_num: 81 loss: 3.2479 acc: 1.0\n",
      "epoch: 10 batch_num: 82 loss: 4.2246 acc: 0.984375\n",
      "epoch: 10 batch_num: 83 loss: 2.8879 acc: 1.0\n",
      "epoch: 10 batch_num: 84 loss: 3.0943 acc: 0.984375\n",
      "epoch: 10 batch_num: 85 loss: 3.8199 acc: 0.984375\n",
      "epoch: 10 batch_num: 86 loss: 3.4741 acc: 0.984375\n",
      "epoch: 10 batch_num: 87 loss: 1.7148 acc: 1.0\n",
      "epoch: 10 batch_num: 88 loss: 2.1822 acc: 1.0\n",
      "epoch: 10 batch_num: 89 loss: 6.0259 acc: 0.96875\n",
      "epoch: 10 batch_num: 90 loss: 3.8941 acc: 0.984375\n",
      "epoch: 10 batch_num: 91 loss: 3.5938 acc: 1.0\n",
      "epoch: 10 batch_num: 92 loss: 3.2747 acc: 0.984375\n",
      "epoch: 10 batch_num: 93 loss: 4.476 acc: 0.96875\n",
      "epoch: 10 batch_num: 94 loss: 4.2177 acc: 0.984375\n",
      "epoch: 10 batch_num: 95 loss: 2.7281 acc: 1.0\n",
      "epoch: 10 batch_num: 96 loss: 4.7626 acc: 1.0\n",
      "epoch: 10 batch_num: 97 loss: 2.1783 acc: 1.0\n",
      "epoch: 10 batch_num: 98 loss: 3.6811 acc: 0.984375\n",
      "epoch: 10 batch_num: 99 loss: 2.6333 acc: 1.0\n",
      "epoch: 10 batch_num: 100 loss: 1.8852 acc: 1.0\n",
      "epoch: 10 batch_num: 101 loss: 4.218 acc: 0.96875\n",
      "epoch: 10 batch_num: 102 loss: 3.9284 acc: 0.984375\n",
      "epoch: 10 batch_num: 103 loss: 2.8519 acc: 1.0\n",
      "epoch: 10 batch_num: 104 loss: 3.1744 acc: 1.0\n",
      "epoch: 10 batch_num: 105 loss: 4.4543 acc: 0.984375\n",
      "epoch: 10 batch_num: 106 loss: 3.5497 acc: 0.984375\n",
      "epoch: 10 batch_num: 107 loss: 4.71 acc: 0.984375\n",
      "epoch: 10 batch_num: 108 loss: 1.8984 acc: 1.0\n",
      "epoch: 10 batch_num: 109 loss: 3.7035 acc: 1.0\n",
      "epoch: 10 batch_num: 110 loss: 8.6059 acc: 0.96875\n",
      "epoch: 10 batch_num: 111 loss: 2.4821 acc: 1.0\n",
      "epoch: 10 batch_num: 112 loss: 5.8075 acc: 0.984375\n",
      "epoch: 10 batch_num: 113 loss: 3.0958 acc: 1.0\n",
      "epoch: 10 batch_num: 114 loss: 2.7364 acc: 1.0\n",
      "epoch: 10 batch_num: 115 loss: 2.0579 acc: 1.0\n",
      "epoch: 10 batch_num: 116 loss: 2.3911 acc: 0.9655172413793104\n",
      "epoch: 11 batch_num: 1 loss: 3.3911 acc: 1.0\n",
      "epoch: 11 batch_num: 2 loss: 2.3585 acc: 1.0\n",
      "epoch: 11 batch_num: 3 loss: 2.4981 acc: 1.0\n",
      "epoch: 11 batch_num: 4 loss: 2.2435 acc: 1.0\n",
      "epoch: 11 batch_num: 5 loss: 1.2945 acc: 1.0\n",
      "epoch: 11 batch_num: 6 loss: 5.2677 acc: 0.96875\n",
      "epoch: 11 batch_num: 7 loss: 4.2447 acc: 0.96875\n",
      "epoch: 11 batch_num: 8 loss: 4.0384 acc: 0.984375\n",
      "epoch: 11 batch_num: 9 loss: 3.0454 acc: 0.984375\n",
      "epoch: 11 batch_num: 10 loss: 3.1741 acc: 0.984375\n",
      "epoch: 11 batch_num: 11 loss: 2.1012 acc: 1.0\n",
      "epoch: 11 batch_num: 12 loss: 2.3014 acc: 1.0\n",
      "epoch: 11 batch_num: 13 loss: 2.3013 acc: 1.0\n",
      "epoch: 11 batch_num: 14 loss: 2.6548 acc: 1.0\n",
      "epoch: 11 batch_num: 15 loss: 3.0508 acc: 1.0\n",
      "epoch: 11 batch_num: 16 loss: 1.9736 acc: 1.0\n",
      "epoch: 11 batch_num: 17 loss: 2.5851 acc: 1.0\n",
      "epoch: 11 batch_num: 18 loss: 1.996 acc: 1.0\n",
      "epoch: 11 batch_num: 19 loss: 3.3844 acc: 1.0\n",
      "epoch: 11 batch_num: 20 loss: 2.2539 acc: 1.0\n",
      "epoch: 11 batch_num: 21 loss: 3.3779 acc: 0.984375\n",
      "epoch: 11 batch_num: 22 loss: 2.7705 acc: 1.0\n",
      "epoch: 11 batch_num: 23 loss: 4.0297 acc: 0.984375\n",
      "epoch: 11 batch_num: 24 loss: 3.2805 acc: 1.0\n",
      "epoch: 11 batch_num: 25 loss: 3.2103 acc: 1.0\n",
      "epoch: 11 batch_num: 26 loss: 2.1018 acc: 1.0\n",
      "epoch: 11 batch_num: 27 loss: 2.6636 acc: 0.984375\n",
      "epoch: 11 batch_num: 28 loss: 2.3778 acc: 0.984375\n",
      "epoch: 11 batch_num: 29 loss: 5.7708 acc: 0.984375\n",
      "epoch: 11 batch_num: 30 loss: 1.5074 acc: 1.0\n",
      "epoch: 11 batch_num: 31 loss: 1.8651 acc: 1.0\n",
      "epoch: 11 batch_num: 32 loss: 1.6001 acc: 1.0\n",
      "epoch: 11 batch_num: 33 loss: 3.3195 acc: 0.984375\n",
      "epoch: 11 batch_num: 34 loss: 3.9393 acc: 1.0\n",
      "epoch: 11 batch_num: 35 loss: 1.5115 acc: 1.0\n",
      "epoch: 11 batch_num: 36 loss: 2.8813 acc: 1.0\n",
      "epoch: 11 batch_num: 37 loss: 2.5869 acc: 1.0\n",
      "epoch: 11 batch_num: 38 loss: 1.7455 acc: 1.0\n",
      "epoch: 11 batch_num: 39 loss: 4.1844 acc: 0.96875\n",
      "epoch: 11 batch_num: 40 loss: 2.3579 acc: 1.0\n",
      "epoch: 11 batch_num: 41 loss: 1.9231 acc: 1.0\n",
      "epoch: 11 batch_num: 42 loss: 6.0621 acc: 0.984375\n",
      "epoch: 11 batch_num: 43 loss: 2.508 acc: 1.0\n",
      "epoch: 11 batch_num: 44 loss: 2.6145 acc: 1.0\n",
      "epoch: 11 batch_num: 45 loss: 2.9203 acc: 0.984375\n",
      "epoch: 11 batch_num: 46 loss: 1.3849 acc: 1.0\n",
      "epoch: 11 batch_num: 47 loss: 3.5588 acc: 0.984375\n",
      "epoch: 11 batch_num: 48 loss: 3.3105 acc: 0.984375\n",
      "epoch: 11 batch_num: 49 loss: 4.0548 acc: 0.984375\n",
      "epoch: 11 batch_num: 50 loss: 2.0912 acc: 1.0\n",
      "epoch: 11 batch_num: 51 loss: 2.3502 acc: 1.0\n",
      "epoch: 11 batch_num: 52 loss: 2.8827 acc: 1.0\n",
      "epoch: 11 batch_num: 53 loss: 2.3676 acc: 1.0\n",
      "epoch: 11 batch_num: 54 loss: 2.1322 acc: 1.0\n",
      "epoch: 11 batch_num: 55 loss: 2.7415 acc: 1.0\n",
      "epoch: 11 batch_num: 56 loss: 5.403 acc: 0.96875\n",
      "epoch: 11 batch_num: 57 loss: 2.2228 acc: 1.0\n",
      "epoch: 11 batch_num: 58 loss: 1.6532 acc: 1.0\n",
      "epoch: 11 batch_num: 59 loss: 2.5899 acc: 1.0\n",
      "epoch: 11 batch_num: 60 loss: 1.6948 acc: 1.0\n",
      "epoch: 11 batch_num: 61 loss: 2.0491 acc: 1.0\n",
      "epoch: 11 batch_num: 62 loss: 2.4013 acc: 1.0\n",
      "epoch: 11 batch_num: 63 loss: 2.1478 acc: 1.0\n",
      "epoch: 11 batch_num: 64 loss: 1.9319 acc: 0.984375\n",
      "epoch: 11 batch_num: 65 loss: 2.344 acc: 1.0\n",
      "epoch: 11 batch_num: 66 loss: 1.7889 acc: 1.0\n",
      "epoch: 11 batch_num: 67 loss: 1.6308 acc: 1.0\n",
      "epoch: 11 batch_num: 68 loss: 4.5107 acc: 0.96875\n",
      "epoch: 11 batch_num: 69 loss: 3.7107 acc: 1.0\n",
      "epoch: 11 batch_num: 70 loss: 2.3903 acc: 1.0\n",
      "epoch: 11 batch_num: 71 loss: 3.5723 acc: 1.0\n",
      "epoch: 11 batch_num: 72 loss: 2.2065 acc: 1.0\n",
      "epoch: 11 batch_num: 73 loss: 1.6271 acc: 1.0\n",
      "epoch: 11 batch_num: 74 loss: 2.6262 acc: 1.0\n",
      "epoch: 11 batch_num: 75 loss: 1.6475 acc: 1.0\n",
      "epoch: 11 batch_num: 76 loss: 2.0193 acc: 1.0\n",
      "epoch: 11 batch_num: 77 loss: 3.3776 acc: 0.984375\n",
      "epoch: 11 batch_num: 78 loss: 2.1748 acc: 1.0\n",
      "epoch: 11 batch_num: 79 loss: 2.3004 acc: 1.0\n",
      "epoch: 11 batch_num: 80 loss: 5.3837 acc: 0.984375\n",
      "epoch: 11 batch_num: 81 loss: 5.9306 acc: 0.984375\n",
      "epoch: 11 batch_num: 82 loss: 1.5349 acc: 1.0\n",
      "epoch: 11 batch_num: 83 loss: 5.4619 acc: 0.984375\n",
      "epoch: 11 batch_num: 84 loss: 2.6641 acc: 1.0\n",
      "epoch: 11 batch_num: 85 loss: 2.5059 acc: 1.0\n",
      "epoch: 11 batch_num: 86 loss: 3.6343 acc: 0.984375\n",
      "epoch: 11 batch_num: 87 loss: 2.916 acc: 0.984375\n",
      "epoch: 11 batch_num: 88 loss: 4.4164 acc: 0.984375\n",
      "epoch: 11 batch_num: 89 loss: 3.2073 acc: 0.984375\n",
      "epoch: 11 batch_num: 90 loss: 2.1351 acc: 1.0\n",
      "epoch: 11 batch_num: 91 loss: 2.2692 acc: 0.984375\n",
      "epoch: 11 batch_num: 92 loss: 2.1306 acc: 1.0\n",
      "epoch: 11 batch_num: 93 loss: 6.394 acc: 0.984375\n",
      "epoch: 11 batch_num: 94 loss: 2.5213 acc: 0.984375\n",
      "epoch: 11 batch_num: 95 loss: 2.4911 acc: 1.0\n",
      "epoch: 11 batch_num: 96 loss: 1.9466 acc: 1.0\n",
      "epoch: 11 batch_num: 97 loss: 3.2288 acc: 1.0\n",
      "epoch: 11 batch_num: 98 loss: 2.5916 acc: 1.0\n",
      "epoch: 11 batch_num: 99 loss: 2.3471 acc: 1.0\n",
      "epoch: 11 batch_num: 100 loss: 2.6471 acc: 1.0\n",
      "epoch: 11 batch_num: 101 loss: 4.8941 acc: 0.96875\n",
      "epoch: 11 batch_num: 102 loss: 2.4511 acc: 1.0\n",
      "epoch: 11 batch_num: 103 loss: 2.4121 acc: 1.0\n",
      "epoch: 11 batch_num: 104 loss: 3.6037 acc: 1.0\n",
      "epoch: 11 batch_num: 105 loss: 4.0615 acc: 0.984375\n",
      "epoch: 11 batch_num: 106 loss: 2.5239 acc: 1.0\n",
      "epoch: 11 batch_num: 107 loss: 2.0425 acc: 1.0\n",
      "epoch: 11 batch_num: 108 loss: 4.1248 acc: 0.984375\n",
      "epoch: 11 batch_num: 109 loss: 1.1489 acc: 1.0\n",
      "epoch: 11 batch_num: 110 loss: 3.3712 acc: 0.984375\n",
      "epoch: 11 batch_num: 111 loss: 3.5091 acc: 0.984375\n",
      "epoch: 11 batch_num: 112 loss: 7.395 acc: 0.953125\n",
      "epoch: 11 batch_num: 113 loss: 2.021 acc: 1.0\n",
      "epoch: 11 batch_num: 114 loss: 6.0525 acc: 0.984375\n",
      "epoch: 11 batch_num: 115 loss: 2.2655 acc: 1.0\n",
      "epoch: 11 batch_num: 116 loss: 0.7263 acc: 1.0\n",
      "epoch: 12 batch_num: 1 loss: 3.5578 acc: 0.984375\n",
      "epoch: 12 batch_num: 2 loss: 1.5164 acc: 1.0\n",
      "epoch: 12 batch_num: 3 loss: 1.6416 acc: 1.0\n",
      "epoch: 12 batch_num: 4 loss: 3.0022 acc: 1.0\n",
      "epoch: 12 batch_num: 5 loss: 2.6527 acc: 0.984375\n",
      "epoch: 12 batch_num: 6 loss: 1.1471 acc: 1.0\n",
      "epoch: 12 batch_num: 7 loss: 1.1702 acc: 1.0\n",
      "epoch: 12 batch_num: 8 loss: 1.3571 acc: 1.0\n",
      "epoch: 12 batch_num: 9 loss: 3.3869 acc: 0.984375\n",
      "epoch: 12 batch_num: 10 loss: 2.1094 acc: 1.0\n",
      "epoch: 12 batch_num: 11 loss: 2.402 acc: 0.984375\n",
      "epoch: 12 batch_num: 12 loss: 1.537 acc: 1.0\n",
      "epoch: 12 batch_num: 13 loss: 3.2093 acc: 0.984375\n",
      "epoch: 12 batch_num: 14 loss: 4.0256 acc: 0.984375\n",
      "epoch: 12 batch_num: 15 loss: 2.5943 acc: 1.0\n",
      "epoch: 12 batch_num: 16 loss: 2.3845 acc: 1.0\n",
      "epoch: 12 batch_num: 17 loss: 1.5263 acc: 1.0\n",
      "epoch: 12 batch_num: 18 loss: 1.8611 acc: 1.0\n",
      "epoch: 12 batch_num: 19 loss: 1.5422 acc: 1.0\n",
      "epoch: 12 batch_num: 20 loss: 2.1804 acc: 0.984375\n",
      "epoch: 12 batch_num: 21 loss: 1.0283 acc: 1.0\n",
      "epoch: 12 batch_num: 22 loss: 5.5703 acc: 0.984375\n",
      "epoch: 12 batch_num: 23 loss: 4.1583 acc: 0.984375\n",
      "epoch: 12 batch_num: 24 loss: 2.9377 acc: 0.984375\n",
      "epoch: 12 batch_num: 25 loss: 2.3206 acc: 1.0\n",
      "epoch: 12 batch_num: 26 loss: 0.9588 acc: 1.0\n",
      "epoch: 12 batch_num: 27 loss: 1.7774 acc: 1.0\n",
      "epoch: 12 batch_num: 28 loss: 2.0681 acc: 1.0\n",
      "epoch: 12 batch_num: 29 loss: 1.623 acc: 1.0\n",
      "epoch: 12 batch_num: 30 loss: 5.3986 acc: 0.984375\n",
      "epoch: 12 batch_num: 31 loss: 1.5648 acc: 1.0\n",
      "epoch: 12 batch_num: 32 loss: 1.5684 acc: 1.0\n",
      "epoch: 12 batch_num: 33 loss: 1.9245 acc: 0.984375\n",
      "epoch: 12 batch_num: 34 loss: 1.1009 acc: 1.0\n",
      "epoch: 12 batch_num: 35 loss: 2.278 acc: 1.0\n",
      "epoch: 12 batch_num: 36 loss: 0.9422 acc: 1.0\n",
      "epoch: 12 batch_num: 37 loss: 2.6632 acc: 0.984375\n",
      "epoch: 12 batch_num: 38 loss: 2.1212 acc: 1.0\n",
      "epoch: 12 batch_num: 39 loss: 1.2658 acc: 1.0\n",
      "epoch: 12 batch_num: 40 loss: 2.1208 acc: 1.0\n",
      "epoch: 12 batch_num: 41 loss: 2.5467 acc: 0.984375\n",
      "epoch: 12 batch_num: 42 loss: 2.5861 acc: 1.0\n",
      "epoch: 12 batch_num: 43 loss: 2.0082 acc: 1.0\n",
      "epoch: 12 batch_num: 44 loss: 2.0444 acc: 1.0\n",
      "epoch: 12 batch_num: 45 loss: 1.6874 acc: 1.0\n",
      "epoch: 12 batch_num: 46 loss: 1.6021 acc: 1.0\n",
      "epoch: 12 batch_num: 47 loss: 1.8928 acc: 1.0\n",
      "epoch: 12 batch_num: 48 loss: 0.8397 acc: 1.0\n",
      "epoch: 12 batch_num: 49 loss: 5.1635 acc: 0.96875\n",
      "epoch: 12 batch_num: 50 loss: 1.936 acc: 1.0\n",
      "epoch: 12 batch_num: 51 loss: 1.19 acc: 1.0\n",
      "epoch: 12 batch_num: 52 loss: 1.9246 acc: 1.0\n",
      "epoch: 12 batch_num: 53 loss: 1.3323 acc: 1.0\n",
      "epoch: 12 batch_num: 54 loss: 1.7734 acc: 1.0\n",
      "epoch: 12 batch_num: 55 loss: 4.3624 acc: 0.984375\n",
      "epoch: 12 batch_num: 56 loss: 1.868 acc: 1.0\n",
      "epoch: 12 batch_num: 57 loss: 2.1603 acc: 1.0\n",
      "epoch: 12 batch_num: 58 loss: 1.5251 acc: 1.0\n",
      "epoch: 12 batch_num: 59 loss: 2.409 acc: 0.984375\n",
      "epoch: 12 batch_num: 60 loss: 4.5536 acc: 0.96875\n",
      "epoch: 12 batch_num: 61 loss: 1.5107 acc: 1.0\n",
      "epoch: 12 batch_num: 62 loss: 1.0058 acc: 1.0\n",
      "epoch: 12 batch_num: 63 loss: 1.756 acc: 1.0\n",
      "epoch: 12 batch_num: 64 loss: 1.7818 acc: 1.0\n",
      "epoch: 12 batch_num: 65 loss: 2.5614 acc: 1.0\n",
      "epoch: 12 batch_num: 66 loss: 2.1304 acc: 1.0\n",
      "epoch: 12 batch_num: 67 loss: 1.7527 acc: 1.0\n",
      "epoch: 12 batch_num: 68 loss: 3.363 acc: 0.984375\n",
      "epoch: 12 batch_num: 69 loss: 2.7323 acc: 0.984375\n",
      "epoch: 12 batch_num: 70 loss: 1.7733 acc: 1.0\n",
      "epoch: 12 batch_num: 71 loss: 1.8005 acc: 1.0\n",
      "epoch: 12 batch_num: 72 loss: 2.8706 acc: 0.984375\n",
      "epoch: 12 batch_num: 73 loss: 1.7878 acc: 1.0\n",
      "epoch: 12 batch_num: 74 loss: 4.6596 acc: 0.984375\n",
      "epoch: 12 batch_num: 75 loss: 2.2673 acc: 1.0\n",
      "epoch: 12 batch_num: 76 loss: 1.7391 acc: 1.0\n",
      "epoch: 12 batch_num: 77 loss: 2.534 acc: 1.0\n",
      "epoch: 12 batch_num: 78 loss: 1.6112 acc: 1.0\n",
      "epoch: 12 batch_num: 79 loss: 1.5432 acc: 1.0\n",
      "epoch: 12 batch_num: 80 loss: 3.6449 acc: 0.984375\n",
      "epoch: 12 batch_num: 81 loss: 2.8734 acc: 1.0\n",
      "epoch: 12 batch_num: 82 loss: 1.4246 acc: 1.0\n",
      "epoch: 12 batch_num: 83 loss: 2.7844 acc: 1.0\n",
      "epoch: 12 batch_num: 84 loss: 2.1776 acc: 1.0\n",
      "epoch: 12 batch_num: 85 loss: 1.7129 acc: 1.0\n",
      "epoch: 12 batch_num: 86 loss: 3.9749 acc: 0.96875\n",
      "epoch: 12 batch_num: 87 loss: 5.0511 acc: 0.96875\n",
      "epoch: 12 batch_num: 88 loss: 3.3942 acc: 1.0\n",
      "epoch: 12 batch_num: 89 loss: 1.7095 acc: 1.0\n",
      "epoch: 12 batch_num: 90 loss: 3.687 acc: 0.984375\n",
      "epoch: 12 batch_num: 91 loss: 1.7516 acc: 1.0\n",
      "epoch: 12 batch_num: 92 loss: 2.7918 acc: 1.0\n",
      "epoch: 12 batch_num: 93 loss: 2.2062 acc: 1.0\n",
      "epoch: 12 batch_num: 94 loss: 2.1504 acc: 1.0\n",
      "epoch: 12 batch_num: 95 loss: 5.3895 acc: 0.96875\n",
      "epoch: 12 batch_num: 96 loss: 3.0864 acc: 0.984375\n",
      "epoch: 12 batch_num: 97 loss: 2.8222 acc: 1.0\n",
      "epoch: 12 batch_num: 98 loss: 2.9787 acc: 0.984375\n",
      "epoch: 12 batch_num: 99 loss: 3.4537 acc: 0.984375\n",
      "epoch: 12 batch_num: 100 loss: 3.9231 acc: 0.984375\n",
      "epoch: 12 batch_num: 101 loss: 3.3033 acc: 0.984375\n",
      "epoch: 12 batch_num: 102 loss: 2.7446 acc: 0.984375\n",
      "epoch: 12 batch_num: 103 loss: 3.0121 acc: 1.0\n",
      "epoch: 12 batch_num: 104 loss: 3.1934 acc: 1.0\n",
      "epoch: 12 batch_num: 105 loss: 1.2776 acc: 1.0\n",
      "epoch: 12 batch_num: 106 loss: 2.2855 acc: 1.0\n",
      "epoch: 12 batch_num: 107 loss: 1.8892 acc: 1.0\n",
      "epoch: 12 batch_num: 108 loss: 3.4681 acc: 1.0\n",
      "epoch: 12 batch_num: 109 loss: 1.9807 acc: 1.0\n",
      "epoch: 12 batch_num: 110 loss: 2.2072 acc: 0.984375\n",
      "epoch: 12 batch_num: 111 loss: 1.4744 acc: 1.0\n",
      "epoch: 12 batch_num: 112 loss: 1.5192 acc: 1.0\n",
      "epoch: 12 batch_num: 113 loss: 2.4501 acc: 1.0\n",
      "epoch: 12 batch_num: 114 loss: 2.126 acc: 1.0\n",
      "epoch: 12 batch_num: 115 loss: 1.8563 acc: 1.0\n",
      "epoch: 12 batch_num: 116 loss: 1.1506 acc: 1.0\n",
      "epoch: 13 batch_num: 1 loss: 1.1877 acc: 1.0\n",
      "epoch: 13 batch_num: 2 loss: 2.8048 acc: 0.984375\n",
      "epoch: 13 batch_num: 3 loss: 4.2986 acc: 0.984375\n",
      "epoch: 13 batch_num: 4 loss: 1.846 acc: 1.0\n",
      "epoch: 13 batch_num: 5 loss: 1.1567 acc: 1.0\n",
      "epoch: 13 batch_num: 6 loss: 2.5256 acc: 0.984375\n",
      "epoch: 13 batch_num: 7 loss: 1.6053 acc: 1.0\n",
      "epoch: 13 batch_num: 8 loss: 1.8714 acc: 1.0\n",
      "epoch: 13 batch_num: 9 loss: 1.0204 acc: 1.0\n",
      "epoch: 13 batch_num: 10 loss: 1.3797 acc: 1.0\n",
      "epoch: 13 batch_num: 11 loss: 1.8667 acc: 1.0\n",
      "epoch: 13 batch_num: 12 loss: 1.8952 acc: 0.984375\n",
      "epoch: 13 batch_num: 13 loss: 2.9617 acc: 0.984375\n",
      "epoch: 13 batch_num: 14 loss: 1.9468 acc: 1.0\n",
      "epoch: 13 batch_num: 15 loss: 1.1882 acc: 1.0\n",
      "epoch: 13 batch_num: 16 loss: 1.656 acc: 1.0\n",
      "epoch: 13 batch_num: 17 loss: 0.853 acc: 1.0\n",
      "epoch: 13 batch_num: 18 loss: 1.0377 acc: 1.0\n",
      "epoch: 13 batch_num: 19 loss: 2.2833 acc: 0.984375\n",
      "epoch: 13 batch_num: 20 loss: 1.7537 acc: 1.0\n",
      "epoch: 13 batch_num: 21 loss: 1.7005 acc: 1.0\n",
      "epoch: 13 batch_num: 22 loss: 1.0472 acc: 1.0\n",
      "epoch: 13 batch_num: 23 loss: 0.948 acc: 1.0\n",
      "epoch: 13 batch_num: 24 loss: 1.8965 acc: 0.984375\n",
      "epoch: 13 batch_num: 25 loss: 2.0831 acc: 1.0\n",
      "epoch: 13 batch_num: 26 loss: 1.1482 acc: 1.0\n",
      "epoch: 13 batch_num: 27 loss: 1.51 acc: 1.0\n",
      "epoch: 13 batch_num: 28 loss: 1.8559 acc: 1.0\n",
      "epoch: 13 batch_num: 29 loss: 0.9318 acc: 1.0\n",
      "epoch: 13 batch_num: 30 loss: 5.4548 acc: 0.953125\n",
      "epoch: 13 batch_num: 31 loss: 1.5827 acc: 1.0\n",
      "epoch: 13 batch_num: 32 loss: 3.115 acc: 0.984375\n",
      "epoch: 13 batch_num: 33 loss: 4.352 acc: 0.96875\n",
      "epoch: 13 batch_num: 34 loss: 2.4054 acc: 1.0\n",
      "epoch: 13 batch_num: 35 loss: 3.9118 acc: 0.984375\n",
      "epoch: 13 batch_num: 36 loss: 1.2915 acc: 1.0\n",
      "epoch: 13 batch_num: 37 loss: 1.1755 acc: 1.0\n",
      "epoch: 13 batch_num: 38 loss: 3.6759 acc: 0.984375\n",
      "epoch: 13 batch_num: 39 loss: 1.2861 acc: 1.0\n",
      "epoch: 13 batch_num: 40 loss: 2.3 acc: 1.0\n",
      "epoch: 13 batch_num: 41 loss: 0.9644 acc: 1.0\n",
      "epoch: 13 batch_num: 42 loss: 2.1171 acc: 0.984375\n",
      "epoch: 13 batch_num: 43 loss: 1.857 acc: 1.0\n",
      "epoch: 13 batch_num: 44 loss: 1.4732 acc: 1.0\n",
      "epoch: 13 batch_num: 45 loss: 2.0446 acc: 1.0\n",
      "epoch: 13 batch_num: 46 loss: 0.6893 acc: 1.0\n",
      "epoch: 13 batch_num: 47 loss: 2.2827 acc: 1.0\n",
      "epoch: 13 batch_num: 48 loss: 1.5118 acc: 1.0\n",
      "epoch: 13 batch_num: 49 loss: 1.9271 acc: 1.0\n",
      "epoch: 13 batch_num: 50 loss: 2.2179 acc: 0.984375\n",
      "epoch: 13 batch_num: 51 loss: 1.6727 acc: 1.0\n",
      "epoch: 13 batch_num: 52 loss: 1.0209 acc: 1.0\n",
      "epoch: 13 batch_num: 53 loss: 2.5054 acc: 1.0\n",
      "epoch: 13 batch_num: 54 loss: 1.281 acc: 1.0\n",
      "epoch: 13 batch_num: 55 loss: 2.6455 acc: 1.0\n",
      "epoch: 13 batch_num: 56 loss: 2.6735 acc: 0.984375\n",
      "epoch: 13 batch_num: 57 loss: 1.5273 acc: 1.0\n",
      "epoch: 13 batch_num: 58 loss: 1.6988 acc: 1.0\n",
      "epoch: 13 batch_num: 59 loss: 1.369 acc: 1.0\n",
      "epoch: 13 batch_num: 60 loss: 1.8452 acc: 1.0\n",
      "epoch: 13 batch_num: 61 loss: 1.1941 acc: 1.0\n",
      "epoch: 13 batch_num: 62 loss: 0.8974 acc: 1.0\n",
      "epoch: 13 batch_num: 63 loss: 1.4158 acc: 1.0\n",
      "epoch: 13 batch_num: 64 loss: 1.4259 acc: 1.0\n",
      "epoch: 13 batch_num: 65 loss: 1.8267 acc: 1.0\n",
      "epoch: 13 batch_num: 66 loss: 2.0709 acc: 1.0\n",
      "epoch: 13 batch_num: 67 loss: 1.0354 acc: 1.0\n",
      "epoch: 13 batch_num: 68 loss: 1.8492 acc: 1.0\n",
      "epoch: 13 batch_num: 69 loss: 1.7704 acc: 1.0\n",
      "epoch: 13 batch_num: 70 loss: 1.7053 acc: 1.0\n",
      "epoch: 13 batch_num: 71 loss: 1.8598 acc: 0.984375\n",
      "epoch: 13 batch_num: 72 loss: 1.0129 acc: 1.0\n",
      "epoch: 13 batch_num: 73 loss: 1.8076 acc: 1.0\n",
      "epoch: 13 batch_num: 74 loss: 2.777 acc: 1.0\n",
      "epoch: 13 batch_num: 75 loss: 0.8826 acc: 1.0\n",
      "epoch: 13 batch_num: 76 loss: 1.434 acc: 1.0\n",
      "epoch: 13 batch_num: 77 loss: 1.1929 acc: 1.0\n",
      "epoch: 13 batch_num: 78 loss: 1.4098 acc: 1.0\n",
      "epoch: 13 batch_num: 79 loss: 1.9575 acc: 1.0\n",
      "epoch: 13 batch_num: 80 loss: 1.8426 acc: 1.0\n",
      "epoch: 13 batch_num: 81 loss: 1.6971 acc: 0.984375\n",
      "epoch: 13 batch_num: 82 loss: 1.4086 acc: 1.0\n",
      "epoch: 13 batch_num: 83 loss: 1.8631 acc: 0.984375\n",
      "epoch: 13 batch_num: 84 loss: 0.9596 acc: 1.0\n",
      "epoch: 13 batch_num: 85 loss: 2.1333 acc: 1.0\n",
      "epoch: 13 batch_num: 86 loss: 1.0323 acc: 1.0\n",
      "epoch: 13 batch_num: 87 loss: 4.6564 acc: 0.984375\n",
      "epoch: 13 batch_num: 88 loss: 1.0631 acc: 1.0\n",
      "epoch: 13 batch_num: 89 loss: 1.399 acc: 1.0\n",
      "epoch: 13 batch_num: 90 loss: 1.67 acc: 1.0\n",
      "epoch: 13 batch_num: 91 loss: 2.4231 acc: 0.984375\n",
      "epoch: 13 batch_num: 92 loss: 1.2975 acc: 1.0\n",
      "epoch: 13 batch_num: 93 loss: 1.4589 acc: 1.0\n",
      "epoch: 13 batch_num: 94 loss: 2.655 acc: 0.984375\n",
      "epoch: 13 batch_num: 95 loss: 2.8462 acc: 0.984375\n",
      "epoch: 13 batch_num: 96 loss: 1.896 acc: 1.0\n",
      "epoch: 13 batch_num: 97 loss: 1.9313 acc: 1.0\n",
      "epoch: 13 batch_num: 98 loss: 5.109 acc: 0.96875\n",
      "epoch: 13 batch_num: 99 loss: 1.3212 acc: 1.0\n",
      "epoch: 13 batch_num: 100 loss: 1.6673 acc: 1.0\n",
      "epoch: 13 batch_num: 101 loss: 0.8467 acc: 1.0\n",
      "epoch: 13 batch_num: 102 loss: 2.7945 acc: 0.984375\n",
      "epoch: 13 batch_num: 103 loss: 0.7431 acc: 1.0\n",
      "epoch: 13 batch_num: 104 loss: 1.5613 acc: 1.0\n",
      "epoch: 13 batch_num: 105 loss: 4.9312 acc: 0.984375\n",
      "epoch: 13 batch_num: 106 loss: 2.4546 acc: 1.0\n",
      "epoch: 13 batch_num: 107 loss: 1.4347 acc: 1.0\n",
      "epoch: 13 batch_num: 108 loss: 2.2077 acc: 1.0\n",
      "epoch: 13 batch_num: 109 loss: 1.8951 acc: 1.0\n",
      "epoch: 13 batch_num: 110 loss: 1.4379 acc: 1.0\n",
      "epoch: 13 batch_num: 111 loss: 1.0442 acc: 1.0\n",
      "epoch: 13 batch_num: 112 loss: 1.6993 acc: 1.0\n",
      "epoch: 13 batch_num: 113 loss: 1.825 acc: 1.0\n",
      "epoch: 13 batch_num: 114 loss: 2.5723 acc: 0.984375\n",
      "epoch: 13 batch_num: 115 loss: 1.9616 acc: 1.0\n",
      "epoch: 13 batch_num: 116 loss: 2.4181 acc: 0.9655172413793104\n",
      "epoch: 14 batch_num: 1 loss: 2.1636 acc: 1.0\n",
      "epoch: 14 batch_num: 2 loss: 1.0324 acc: 1.0\n",
      "epoch: 14 batch_num: 3 loss: 1.1878 acc: 1.0\n",
      "epoch: 14 batch_num: 4 loss: 1.0447 acc: 1.0\n",
      "epoch: 14 batch_num: 5 loss: 1.2768 acc: 1.0\n",
      "epoch: 14 batch_num: 6 loss: 1.3756 acc: 1.0\n",
      "epoch: 14 batch_num: 7 loss: 0.8327 acc: 1.0\n",
      "epoch: 14 batch_num: 8 loss: 2.197 acc: 0.984375\n",
      "epoch: 14 batch_num: 9 loss: 1.7758 acc: 1.0\n",
      "epoch: 14 batch_num: 10 loss: 1.644 acc: 1.0\n",
      "epoch: 14 batch_num: 11 loss: 1.3674 acc: 1.0\n",
      "epoch: 14 batch_num: 12 loss: 2.5788 acc: 0.984375\n",
      "epoch: 14 batch_num: 13 loss: 1.6158 acc: 1.0\n",
      "epoch: 14 batch_num: 14 loss: 1.1308 acc: 1.0\n",
      "epoch: 14 batch_num: 15 loss: 1.3308 acc: 1.0\n",
      "epoch: 14 batch_num: 16 loss: 5.1456 acc: 0.984375\n",
      "epoch: 14 batch_num: 17 loss: 1.4096 acc: 1.0\n",
      "epoch: 14 batch_num: 18 loss: 1.1027 acc: 1.0\n",
      "epoch: 14 batch_num: 19 loss: 0.8095 acc: 1.0\n",
      "epoch: 14 batch_num: 20 loss: 2.1989 acc: 1.0\n",
      "epoch: 14 batch_num: 21 loss: 1.6115 acc: 1.0\n",
      "epoch: 14 batch_num: 22 loss: 0.9775 acc: 1.0\n",
      "epoch: 14 batch_num: 23 loss: 1.4521 acc: 1.0\n",
      "epoch: 14 batch_num: 24 loss: 1.91 acc: 1.0\n",
      "epoch: 14 batch_num: 25 loss: 1.0532 acc: 1.0\n",
      "epoch: 14 batch_num: 26 loss: 2.2456 acc: 0.984375\n",
      "epoch: 14 batch_num: 27 loss: 0.639 acc: 1.0\n",
      "epoch: 14 batch_num: 28 loss: 1.1279 acc: 1.0\n",
      "epoch: 14 batch_num: 29 loss: 0.8481 acc: 1.0\n",
      "epoch: 14 batch_num: 30 loss: 1.1587 acc: 1.0\n",
      "epoch: 14 batch_num: 31 loss: 1.4543 acc: 1.0\n",
      "epoch: 14 batch_num: 32 loss: 1.2725 acc: 1.0\n",
      "epoch: 14 batch_num: 33 loss: 1.0805 acc: 1.0\n",
      "epoch: 14 batch_num: 34 loss: 0.9063 acc: 1.0\n",
      "epoch: 14 batch_num: 35 loss: 0.9845 acc: 1.0\n",
      "epoch: 14 batch_num: 36 loss: 1.8067 acc: 1.0\n",
      "epoch: 14 batch_num: 37 loss: 1.0597 acc: 1.0\n",
      "epoch: 14 batch_num: 38 loss: 5.6136 acc: 0.96875\n",
      "epoch: 14 batch_num: 39 loss: 2.9175 acc: 0.984375\n",
      "epoch: 14 batch_num: 40 loss: 0.5802 acc: 1.0\n",
      "epoch: 14 batch_num: 41 loss: 5.5334 acc: 0.96875\n",
      "epoch: 14 batch_num: 42 loss: 3.1842 acc: 0.984375\n",
      "epoch: 14 batch_num: 43 loss: 1.3094 acc: 1.0\n",
      "epoch: 14 batch_num: 44 loss: 1.5148 acc: 1.0\n",
      "epoch: 14 batch_num: 45 loss: 1.1048 acc: 1.0\n",
      "epoch: 14 batch_num: 46 loss: 1.4331 acc: 1.0\n",
      "epoch: 14 batch_num: 47 loss: 1.2673 acc: 1.0\n",
      "epoch: 14 batch_num: 48 loss: 0.7012 acc: 1.0\n",
      "epoch: 14 batch_num: 49 loss: 1.4311 acc: 1.0\n",
      "epoch: 14 batch_num: 50 loss: 1.5584 acc: 0.984375\n",
      "epoch: 14 batch_num: 51 loss: 1.3564 acc: 1.0\n",
      "epoch: 14 batch_num: 52 loss: 0.6096 acc: 1.0\n",
      "epoch: 14 batch_num: 53 loss: 2.169 acc: 1.0\n",
      "epoch: 14 batch_num: 54 loss: 1.0739 acc: 1.0\n",
      "epoch: 14 batch_num: 55 loss: 1.2661 acc: 1.0\n",
      "epoch: 14 batch_num: 56 loss: 1.7322 acc: 1.0\n",
      "epoch: 14 batch_num: 57 loss: 2.6145 acc: 0.984375\n",
      "epoch: 14 batch_num: 58 loss: 1.251 acc: 1.0\n",
      "epoch: 14 batch_num: 59 loss: 4.3338 acc: 0.984375\n",
      "epoch: 14 batch_num: 60 loss: 0.7351 acc: 1.0\n",
      "epoch: 14 batch_num: 61 loss: 1.1656 acc: 1.0\n",
      "epoch: 14 batch_num: 62 loss: 0.8775 acc: 1.0\n",
      "epoch: 14 batch_num: 63 loss: 4.9995 acc: 0.984375\n",
      "epoch: 14 batch_num: 64 loss: 1.4788 acc: 1.0\n",
      "epoch: 14 batch_num: 65 loss: 1.7177 acc: 1.0\n",
      "epoch: 14 batch_num: 66 loss: 3.4613 acc: 0.984375\n",
      "epoch: 14 batch_num: 67 loss: 1.2711 acc: 1.0\n",
      "epoch: 14 batch_num: 68 loss: 1.5801 acc: 1.0\n",
      "epoch: 14 batch_num: 69 loss: 1.2321 acc: 1.0\n",
      "epoch: 14 batch_num: 70 loss: 1.1115 acc: 1.0\n",
      "epoch: 14 batch_num: 71 loss: 1.6726 acc: 1.0\n",
      "epoch: 14 batch_num: 72 loss: 1.8043 acc: 1.0\n",
      "epoch: 14 batch_num: 73 loss: 1.1588 acc: 1.0\n",
      "epoch: 14 batch_num: 74 loss: 1.7553 acc: 1.0\n",
      "epoch: 14 batch_num: 75 loss: 1.1893 acc: 1.0\n",
      "epoch: 14 batch_num: 76 loss: 0.805 acc: 1.0\n",
      "epoch: 14 batch_num: 77 loss: 4.3212 acc: 0.984375\n",
      "epoch: 14 batch_num: 78 loss: 2.458 acc: 0.984375\n",
      "epoch: 14 batch_num: 79 loss: 0.9841 acc: 1.0\n",
      "epoch: 14 batch_num: 80 loss: 1.3649 acc: 1.0\n",
      "epoch: 14 batch_num: 81 loss: 2.202 acc: 1.0\n",
      "epoch: 14 batch_num: 82 loss: 3.249 acc: 0.984375\n",
      "epoch: 14 batch_num: 83 loss: 1.72 acc: 1.0\n",
      "epoch: 14 batch_num: 84 loss: 1.0343 acc: 1.0\n",
      "epoch: 14 batch_num: 85 loss: 0.9762 acc: 1.0\n",
      "epoch: 14 batch_num: 86 loss: 0.8375 acc: 1.0\n",
      "epoch: 14 batch_num: 87 loss: 2.3192 acc: 0.984375\n",
      "epoch: 14 batch_num: 88 loss: 1.5871 acc: 1.0\n",
      "epoch: 14 batch_num: 89 loss: 1.1101 acc: 1.0\n",
      "epoch: 14 batch_num: 90 loss: 0.6795 acc: 1.0\n",
      "epoch: 14 batch_num: 91 loss: 1.746 acc: 1.0\n",
      "epoch: 14 batch_num: 92 loss: 1.2802 acc: 1.0\n",
      "epoch: 14 batch_num: 93 loss: 1.494 acc: 1.0\n",
      "epoch: 14 batch_num: 94 loss: 0.8552 acc: 1.0\n",
      "epoch: 14 batch_num: 95 loss: 2.1436 acc: 0.984375\n",
      "epoch: 14 batch_num: 96 loss: 2.1043 acc: 0.984375\n",
      "epoch: 14 batch_num: 97 loss: 2.552 acc: 0.984375\n",
      "epoch: 14 batch_num: 98 loss: 1.4026 acc: 1.0\n",
      "epoch: 14 batch_num: 99 loss: 1.9833 acc: 1.0\n",
      "epoch: 14 batch_num: 100 loss: 0.8509 acc: 1.0\n",
      "epoch: 14 batch_num: 101 loss: 1.4855 acc: 1.0\n",
      "epoch: 14 batch_num: 102 loss: 2.181 acc: 1.0\n",
      "epoch: 14 batch_num: 103 loss: 1.3488 acc: 1.0\n",
      "epoch: 14 batch_num: 104 loss: 2.9811 acc: 0.984375\n",
      "epoch: 14 batch_num: 105 loss: 1.4923 acc: 1.0\n",
      "epoch: 14 batch_num: 106 loss: 2.3235 acc: 0.984375\n",
      "epoch: 14 batch_num: 107 loss: 3.0124 acc: 0.984375\n",
      "epoch: 14 batch_num: 108 loss: 0.9951 acc: 1.0\n",
      "epoch: 14 batch_num: 109 loss: 1.5748 acc: 1.0\n",
      "epoch: 14 batch_num: 110 loss: 5.116 acc: 0.96875\n",
      "epoch: 14 batch_num: 111 loss: 3.6993 acc: 0.984375\n",
      "epoch: 14 batch_num: 112 loss: 0.9592 acc: 1.0\n",
      "epoch: 14 batch_num: 113 loss: 1.5761 acc: 1.0\n",
      "epoch: 14 batch_num: 114 loss: 1.7582 acc: 1.0\n",
      "epoch: 14 batch_num: 115 loss: 0.9575 acc: 1.0\n",
      "epoch: 14 batch_num: 116 loss: 0.58 acc: 1.0\n",
      "epoch: 15 batch_num: 1 loss: 1.0377 acc: 1.0\n",
      "epoch: 15 batch_num: 2 loss: 0.9982 acc: 1.0\n",
      "epoch: 15 batch_num: 3 loss: 1.0406 acc: 1.0\n",
      "epoch: 15 batch_num: 4 loss: 1.6322 acc: 1.0\n",
      "epoch: 15 batch_num: 5 loss: 1.2931 acc: 1.0\n",
      "epoch: 15 batch_num: 6 loss: 0.8819 acc: 1.0\n",
      "epoch: 15 batch_num: 7 loss: 0.7314 acc: 1.0\n",
      "epoch: 15 batch_num: 8 loss: 1.2009 acc: 1.0\n",
      "epoch: 15 batch_num: 9 loss: 1.6908 acc: 0.984375\n",
      "epoch: 15 batch_num: 10 loss: 0.9463 acc: 1.0\n",
      "epoch: 15 batch_num: 11 loss: 0.694 acc: 1.0\n",
      "epoch: 15 batch_num: 12 loss: 1.726 acc: 1.0\n",
      "epoch: 15 batch_num: 13 loss: 1.1005 acc: 1.0\n",
      "epoch: 15 batch_num: 14 loss: 0.8052 acc: 1.0\n",
      "epoch: 15 batch_num: 15 loss: 1.7368 acc: 1.0\n",
      "epoch: 15 batch_num: 16 loss: 1.2381 acc: 1.0\n",
      "epoch: 15 batch_num: 17 loss: 0.856 acc: 1.0\n",
      "epoch: 15 batch_num: 18 loss: 0.7141 acc: 1.0\n",
      "epoch: 15 batch_num: 19 loss: 1.6674 acc: 1.0\n",
      "epoch: 15 batch_num: 20 loss: 1.9603 acc: 0.984375\n",
      "epoch: 15 batch_num: 21 loss: 0.9185 acc: 1.0\n",
      "epoch: 15 batch_num: 22 loss: 1.5781 acc: 1.0\n",
      "epoch: 15 batch_num: 23 loss: 1.1102 acc: 1.0\n",
      "epoch: 15 batch_num: 24 loss: 1.1454 acc: 1.0\n",
      "epoch: 15 batch_num: 25 loss: 2.3951 acc: 0.984375\n",
      "epoch: 15 batch_num: 26 loss: 3.5026 acc: 0.984375\n",
      "epoch: 15 batch_num: 27 loss: 0.8703 acc: 1.0\n",
      "epoch: 15 batch_num: 28 loss: 1.1332 acc: 1.0\n",
      "epoch: 15 batch_num: 29 loss: 1.1476 acc: 1.0\n",
      "epoch: 15 batch_num: 30 loss: 1.3553 acc: 1.0\n",
      "epoch: 15 batch_num: 31 loss: 1.2413 acc: 1.0\n",
      "epoch: 15 batch_num: 32 loss: 2.4631 acc: 1.0\n",
      "epoch: 15 batch_num: 33 loss: 1.4423 acc: 1.0\n",
      "epoch: 15 batch_num: 34 loss: 1.2206 acc: 1.0\n",
      "epoch: 15 batch_num: 35 loss: 2.3645 acc: 0.984375\n",
      "epoch: 15 batch_num: 36 loss: 0.7047 acc: 1.0\n",
      "epoch: 15 batch_num: 37 loss: 0.9644 acc: 1.0\n",
      "epoch: 15 batch_num: 38 loss: 0.5836 acc: 1.0\n",
      "epoch: 15 batch_num: 39 loss: 1.424 acc: 1.0\n",
      "epoch: 15 batch_num: 40 loss: 0.613 acc: 1.0\n",
      "epoch: 15 batch_num: 41 loss: 2.2154 acc: 0.984375\n",
      "epoch: 15 batch_num: 42 loss: 0.7715 acc: 1.0\n",
      "epoch: 15 batch_num: 43 loss: 1.2246 acc: 1.0\n",
      "epoch: 15 batch_num: 44 loss: 1.4059 acc: 1.0\n",
      "epoch: 15 batch_num: 45 loss: 0.979 acc: 1.0\n",
      "epoch: 15 batch_num: 46 loss: 0.9886 acc: 1.0\n",
      "epoch: 15 batch_num: 47 loss: 6.3228 acc: 0.984375\n",
      "epoch: 15 batch_num: 48 loss: 0.6861 acc: 1.0\n",
      "epoch: 15 batch_num: 49 loss: 0.8936 acc: 1.0\n",
      "epoch: 15 batch_num: 50 loss: 0.8031 acc: 1.0\n",
      "epoch: 15 batch_num: 51 loss: 0.8977 acc: 1.0\n",
      "epoch: 15 batch_num: 52 loss: 3.2183 acc: 0.984375\n",
      "epoch: 15 batch_num: 53 loss: 1.9224 acc: 1.0\n",
      "epoch: 15 batch_num: 54 loss: 2.0484 acc: 1.0\n",
      "epoch: 15 batch_num: 55 loss: 0.897 acc: 1.0\n",
      "epoch: 15 batch_num: 56 loss: 3.7304 acc: 0.96875\n",
      "epoch: 15 batch_num: 57 loss: 2.6145 acc: 1.0\n",
      "epoch: 15 batch_num: 58 loss: 0.8949 acc: 1.0\n",
      "epoch: 15 batch_num: 59 loss: 1.192 acc: 1.0\n",
      "epoch: 15 batch_num: 60 loss: 1.1784 acc: 1.0\n",
      "epoch: 15 batch_num: 61 loss: 1.1846 acc: 1.0\n",
      "epoch: 15 batch_num: 62 loss: 1.689 acc: 0.984375\n",
      "epoch: 15 batch_num: 63 loss: 2.1748 acc: 1.0\n",
      "epoch: 15 batch_num: 64 loss: 0.5485 acc: 1.0\n",
      "epoch: 15 batch_num: 65 loss: 0.6154 acc: 1.0\n",
      "epoch: 15 batch_num: 66 loss: 0.8855 acc: 1.0\n",
      "epoch: 15 batch_num: 67 loss: 1.2939 acc: 1.0\n",
      "epoch: 15 batch_num: 68 loss: 0.6371 acc: 1.0\n",
      "epoch: 15 batch_num: 69 loss: 6.7858 acc: 0.984375\n",
      "epoch: 15 batch_num: 70 loss: 1.0592 acc: 1.0\n",
      "epoch: 15 batch_num: 71 loss: 0.591 acc: 1.0\n",
      "epoch: 15 batch_num: 72 loss: 0.5006 acc: 1.0\n",
      "epoch: 15 batch_num: 73 loss: 1.1029 acc: 1.0\n",
      "epoch: 15 batch_num: 74 loss: 1.0521 acc: 1.0\n",
      "epoch: 15 batch_num: 75 loss: 0.5838 acc: 1.0\n",
      "epoch: 15 batch_num: 76 loss: 1.4934 acc: 1.0\n",
      "epoch: 15 batch_num: 77 loss: 2.3195 acc: 1.0\n",
      "epoch: 15 batch_num: 78 loss: 1.1568 acc: 1.0\n",
      "epoch: 15 batch_num: 79 loss: 4.2574 acc: 0.96875\n",
      "epoch: 15 batch_num: 80 loss: 3.8839 acc: 0.984375\n",
      "epoch: 15 batch_num: 81 loss: 1.265 acc: 1.0\n",
      "epoch: 15 batch_num: 82 loss: 1.2408 acc: 1.0\n",
      "epoch: 15 batch_num: 83 loss: 1.1866 acc: 1.0\n",
      "epoch: 15 batch_num: 84 loss: 1.3135 acc: 1.0\n",
      "epoch: 15 batch_num: 85 loss: 0.7763 acc: 1.0\n",
      "epoch: 15 batch_num: 86 loss: 3.9615 acc: 0.984375\n",
      "epoch: 15 batch_num: 87 loss: 1.587 acc: 1.0\n",
      "epoch: 15 batch_num: 88 loss: 0.6856 acc: 1.0\n",
      "epoch: 15 batch_num: 89 loss: 0.739 acc: 1.0\n",
      "epoch: 15 batch_num: 90 loss: 1.5925 acc: 1.0\n",
      "epoch: 15 batch_num: 91 loss: 1.636 acc: 1.0\n",
      "epoch: 15 batch_num: 92 loss: 4.144 acc: 0.96875\n",
      "epoch: 15 batch_num: 93 loss: 1.9596 acc: 0.984375\n",
      "epoch: 15 batch_num: 94 loss: 0.41 acc: 1.0\n",
      "epoch: 15 batch_num: 95 loss: 0.6775 acc: 1.0\n",
      "epoch: 15 batch_num: 96 loss: 0.7264 acc: 1.0\n",
      "epoch: 15 batch_num: 97 loss: 1.0487 acc: 1.0\n",
      "epoch: 15 batch_num: 98 loss: 1.093 acc: 1.0\n",
      "epoch: 15 batch_num: 99 loss: 1.0019 acc: 1.0\n",
      "epoch: 15 batch_num: 100 loss: 4.2249 acc: 0.96875\n",
      "epoch: 15 batch_num: 101 loss: 1.545 acc: 1.0\n",
      "epoch: 15 batch_num: 102 loss: 0.8172 acc: 1.0\n",
      "epoch: 15 batch_num: 103 loss: 0.6924 acc: 1.0\n",
      "epoch: 15 batch_num: 104 loss: 1.0804 acc: 1.0\n",
      "epoch: 15 batch_num: 105 loss: 0.7754 acc: 1.0\n",
      "epoch: 15 batch_num: 106 loss: 1.413 acc: 1.0\n",
      "epoch: 15 batch_num: 107 loss: 1.3171 acc: 1.0\n",
      "epoch: 15 batch_num: 108 loss: 0.6792 acc: 1.0\n",
      "epoch: 15 batch_num: 109 loss: 1.9583 acc: 0.984375\n",
      "epoch: 15 batch_num: 110 loss: 1.1333 acc: 1.0\n",
      "epoch: 15 batch_num: 111 loss: 0.7546 acc: 1.0\n",
      "epoch: 15 batch_num: 112 loss: 1.6948 acc: 0.984375\n",
      "epoch: 15 batch_num: 113 loss: 1.0757 acc: 1.0\n",
      "epoch: 15 batch_num: 114 loss: 0.9187 acc: 1.0\n",
      "epoch: 15 batch_num: 115 loss: 2.9279 acc: 0.984375\n",
      "epoch: 15 batch_num: 116 loss: 0.3449 acc: 1.0\n",
      "epoch: 16 batch_num: 1 loss: 1.0584 acc: 1.0\n",
      "epoch: 16 batch_num: 2 loss: 1.1027 acc: 1.0\n",
      "epoch: 16 batch_num: 3 loss: 1.1233 acc: 1.0\n",
      "epoch: 16 batch_num: 4 loss: 1.0794 acc: 1.0\n",
      "epoch: 16 batch_num: 5 loss: 3.7594 acc: 0.984375\n",
      "epoch: 16 batch_num: 6 loss: 0.7457 acc: 1.0\n",
      "epoch: 16 batch_num: 7 loss: 1.4073 acc: 0.984375\n",
      "epoch: 16 batch_num: 8 loss: 2.0403 acc: 0.984375\n",
      "epoch: 16 batch_num: 9 loss: 1.1303 acc: 1.0\n",
      "epoch: 16 batch_num: 10 loss: 0.5342 acc: 1.0\n",
      "epoch: 16 batch_num: 11 loss: 1.1593 acc: 1.0\n",
      "epoch: 16 batch_num: 12 loss: 0.973 acc: 1.0\n",
      "epoch: 16 batch_num: 13 loss: 0.7181 acc: 1.0\n",
      "epoch: 16 batch_num: 14 loss: 0.6843 acc: 1.0\n",
      "epoch: 16 batch_num: 15 loss: 0.8726 acc: 1.0\n",
      "epoch: 16 batch_num: 16 loss: 1.2256 acc: 1.0\n",
      "epoch: 16 batch_num: 17 loss: 1.4942 acc: 1.0\n",
      "epoch: 16 batch_num: 18 loss: 1.033 acc: 1.0\n",
      "epoch: 16 batch_num: 19 loss: 1.5671 acc: 1.0\n",
      "epoch: 16 batch_num: 20 loss: 2.3873 acc: 0.984375\n",
      "epoch: 16 batch_num: 21 loss: 0.7663 acc: 1.0\n",
      "epoch: 16 batch_num: 22 loss: 1.2251 acc: 1.0\n",
      "epoch: 16 batch_num: 23 loss: 1.1641 acc: 1.0\n",
      "epoch: 16 batch_num: 24 loss: 1.1135 acc: 1.0\n",
      "epoch: 16 batch_num: 25 loss: 2.7218 acc: 0.984375\n",
      "epoch: 16 batch_num: 26 loss: 0.7391 acc: 1.0\n",
      "epoch: 16 batch_num: 27 loss: 0.6727 acc: 1.0\n",
      "epoch: 16 batch_num: 28 loss: 0.6197 acc: 1.0\n",
      "epoch: 16 batch_num: 29 loss: 1.1201 acc: 1.0\n",
      "epoch: 16 batch_num: 30 loss: 0.9897 acc: 1.0\n",
      "epoch: 16 batch_num: 31 loss: 0.7769 acc: 1.0\n",
      "epoch: 16 batch_num: 32 loss: 0.7896 acc: 1.0\n",
      "epoch: 16 batch_num: 33 loss: 0.8321 acc: 1.0\n",
      "epoch: 16 batch_num: 34 loss: 0.972 acc: 1.0\n",
      "epoch: 16 batch_num: 35 loss: 0.7436 acc: 1.0\n",
      "epoch: 16 batch_num: 36 loss: 0.941 acc: 1.0\n",
      "epoch: 16 batch_num: 37 loss: 0.539 acc: 1.0\n",
      "epoch: 16 batch_num: 38 loss: 1.0583 acc: 1.0\n",
      "epoch: 16 batch_num: 39 loss: 9.5974 acc: 0.96875\n",
      "epoch: 16 batch_num: 40 loss: 0.4137 acc: 1.0\n",
      "epoch: 16 batch_num: 41 loss: 0.7593 acc: 1.0\n",
      "epoch: 16 batch_num: 42 loss: 2.3604 acc: 0.984375\n",
      "epoch: 16 batch_num: 43 loss: 1.1248 acc: 1.0\n",
      "epoch: 16 batch_num: 44 loss: 0.6353 acc: 1.0\n",
      "epoch: 16 batch_num: 45 loss: 0.8125 acc: 1.0\n",
      "epoch: 16 batch_num: 46 loss: 1.3212 acc: 1.0\n",
      "epoch: 16 batch_num: 47 loss: 1.0709 acc: 1.0\n",
      "epoch: 16 batch_num: 48 loss: 0.5537 acc: 1.0\n",
      "epoch: 16 batch_num: 49 loss: 1.4287 acc: 1.0\n",
      "epoch: 16 batch_num: 50 loss: 1.1701 acc: 1.0\n",
      "epoch: 16 batch_num: 51 loss: 1.6419 acc: 0.984375\n",
      "epoch: 16 batch_num: 52 loss: 0.6781 acc: 1.0\n",
      "epoch: 16 batch_num: 53 loss: 2.3746 acc: 0.984375\n",
      "epoch: 16 batch_num: 54 loss: 1.0324 acc: 1.0\n",
      "epoch: 16 batch_num: 55 loss: 1.5589 acc: 1.0\n",
      "epoch: 16 batch_num: 56 loss: 1.2625 acc: 1.0\n",
      "epoch: 16 batch_num: 57 loss: 1.1406 acc: 1.0\n",
      "epoch: 16 batch_num: 58 loss: 1.3101 acc: 1.0\n",
      "epoch: 16 batch_num: 59 loss: 1.9657 acc: 0.984375\n",
      "epoch: 16 batch_num: 60 loss: 1.8728 acc: 1.0\n",
      "epoch: 16 batch_num: 61 loss: 1.3088 acc: 1.0\n",
      "epoch: 16 batch_num: 62 loss: 2.3269 acc: 0.984375\n",
      "epoch: 16 batch_num: 63 loss: 0.6883 acc: 1.0\n",
      "epoch: 16 batch_num: 64 loss: 0.7045 acc: 1.0\n",
      "epoch: 16 batch_num: 65 loss: 1.4228 acc: 1.0\n",
      "epoch: 16 batch_num: 66 loss: 0.8774 acc: 1.0\n",
      "epoch: 16 batch_num: 67 loss: 0.8556 acc: 1.0\n",
      "epoch: 16 batch_num: 68 loss: 1.0293 acc: 1.0\n",
      "epoch: 16 batch_num: 69 loss: 1.0166 acc: 1.0\n",
      "epoch: 16 batch_num: 70 loss: 0.8911 acc: 1.0\n",
      "epoch: 16 batch_num: 71 loss: 0.9406 acc: 1.0\n",
      "epoch: 16 batch_num: 72 loss: 0.787 acc: 1.0\n",
      "epoch: 16 batch_num: 73 loss: 0.6939 acc: 1.0\n",
      "epoch: 16 batch_num: 74 loss: 1.0966 acc: 1.0\n",
      "epoch: 16 batch_num: 75 loss: 1.3193 acc: 1.0\n",
      "epoch: 16 batch_num: 76 loss: 1.537 acc: 1.0\n",
      "epoch: 16 batch_num: 77 loss: 0.9583 acc: 1.0\n",
      "epoch: 16 batch_num: 78 loss: 0.9664 acc: 1.0\n",
      "epoch: 16 batch_num: 79 loss: 0.8829 acc: 1.0\n",
      "epoch: 16 batch_num: 80 loss: 1.0604 acc: 1.0\n",
      "epoch: 16 batch_num: 81 loss: 0.7534 acc: 1.0\n",
      "epoch: 16 batch_num: 82 loss: 1.1936 acc: 1.0\n",
      "epoch: 16 batch_num: 83 loss: 1.0268 acc: 1.0\n",
      "epoch: 16 batch_num: 84 loss: 0.546 acc: 1.0\n",
      "epoch: 16 batch_num: 85 loss: 0.6153 acc: 1.0\n",
      "epoch: 16 batch_num: 86 loss: 0.8651 acc: 1.0\n",
      "epoch: 16 batch_num: 87 loss: 3.6363 acc: 0.984375\n",
      "epoch: 16 batch_num: 88 loss: 1.0012 acc: 1.0\n",
      "epoch: 16 batch_num: 89 loss: 0.6368 acc: 1.0\n",
      "epoch: 16 batch_num: 90 loss: 0.8683 acc: 1.0\n",
      "epoch: 16 batch_num: 91 loss: 3.0582 acc: 0.984375\n",
      "epoch: 16 batch_num: 92 loss: 2.4277 acc: 0.984375\n",
      "epoch: 16 batch_num: 93 loss: 1.1812 acc: 1.0\n",
      "epoch: 16 batch_num: 94 loss: 0.7493 acc: 1.0\n",
      "epoch: 16 batch_num: 95 loss: 2.3606 acc: 0.984375\n",
      "epoch: 16 batch_num: 96 loss: 0.5082 acc: 1.0\n",
      "epoch: 16 batch_num: 97 loss: 1.3968 acc: 1.0\n",
      "epoch: 16 batch_num: 98 loss: 0.5443 acc: 1.0\n",
      "epoch: 16 batch_num: 99 loss: 0.9523 acc: 1.0\n",
      "epoch: 16 batch_num: 100 loss: 0.9284 acc: 1.0\n",
      "epoch: 16 batch_num: 101 loss: 0.7421 acc: 1.0\n",
      "epoch: 16 batch_num: 102 loss: 3.62 acc: 0.96875\n",
      "epoch: 16 batch_num: 103 loss: 0.8542 acc: 1.0\n",
      "epoch: 16 batch_num: 104 loss: 2.2747 acc: 0.984375\n",
      "epoch: 16 batch_num: 105 loss: 0.8261 acc: 1.0\n",
      "epoch: 16 batch_num: 106 loss: 2.0009 acc: 0.984375\n",
      "epoch: 16 batch_num: 107 loss: 0.5524 acc: 1.0\n",
      "epoch: 16 batch_num: 108 loss: 0.8972 acc: 1.0\n",
      "epoch: 16 batch_num: 109 loss: 1.0425 acc: 1.0\n",
      "epoch: 16 batch_num: 110 loss: 0.534 acc: 1.0\n",
      "epoch: 16 batch_num: 111 loss: 1.6646 acc: 0.984375\n",
      "epoch: 16 batch_num: 112 loss: 1.0476 acc: 1.0\n",
      "epoch: 16 batch_num: 113 loss: 0.9101 acc: 1.0\n",
      "epoch: 16 batch_num: 114 loss: 1.7685 acc: 0.984375\n",
      "epoch: 16 batch_num: 115 loss: 0.8424 acc: 1.0\n",
      "epoch: 16 batch_num: 116 loss: 0.8563 acc: 1.0\n",
      "epoch: 17 batch_num: 1 loss: 0.7126 acc: 1.0\n",
      "epoch: 17 batch_num: 2 loss: 0.5513 acc: 1.0\n",
      "epoch: 17 batch_num: 3 loss: 0.5081 acc: 1.0\n",
      "epoch: 17 batch_num: 4 loss: 1.3079 acc: 1.0\n",
      "epoch: 17 batch_num: 5 loss: 1.0711 acc: 1.0\n",
      "epoch: 17 batch_num: 6 loss: 0.5518 acc: 1.0\n",
      "epoch: 17 batch_num: 7 loss: 0.9754 acc: 1.0\n",
      "epoch: 17 batch_num: 8 loss: 0.7417 acc: 1.0\n",
      "epoch: 17 batch_num: 9 loss: 0.6599 acc: 1.0\n",
      "epoch: 17 batch_num: 10 loss: 0.7678 acc: 1.0\n",
      "epoch: 17 batch_num: 11 loss: 0.9451 acc: 1.0\n",
      "epoch: 17 batch_num: 12 loss: 1.1265 acc: 1.0\n",
      "epoch: 17 batch_num: 13 loss: 2.7344 acc: 0.984375\n",
      "epoch: 17 batch_num: 14 loss: 0.4867 acc: 1.0\n",
      "epoch: 17 batch_num: 15 loss: 3.2083 acc: 0.984375\n",
      "epoch: 17 batch_num: 16 loss: 0.638 acc: 1.0\n",
      "epoch: 17 batch_num: 17 loss: 0.756 acc: 1.0\n",
      "epoch: 17 batch_num: 18 loss: 0.2512 acc: 1.0\n",
      "epoch: 17 batch_num: 19 loss: 0.7314 acc: 1.0\n",
      "epoch: 17 batch_num: 20 loss: 1.9833 acc: 0.984375\n",
      "epoch: 17 batch_num: 21 loss: 1.5678 acc: 0.984375\n",
      "epoch: 17 batch_num: 22 loss: 0.9987 acc: 1.0\n",
      "epoch: 17 batch_num: 23 loss: 0.61 acc: 1.0\n",
      "epoch: 17 batch_num: 24 loss: 0.9854 acc: 1.0\n",
      "epoch: 17 batch_num: 25 loss: 0.6038 acc: 1.0\n",
      "epoch: 17 batch_num: 26 loss: 1.3081 acc: 1.0\n",
      "epoch: 17 batch_num: 27 loss: 0.4942 acc: 1.0\n",
      "epoch: 17 batch_num: 28 loss: 0.5575 acc: 1.0\n",
      "epoch: 17 batch_num: 29 loss: 0.7146 acc: 1.0\n",
      "epoch: 17 batch_num: 30 loss: 0.9872 acc: 1.0\n",
      "epoch: 17 batch_num: 31 loss: 0.68 acc: 1.0\n",
      "epoch: 17 batch_num: 32 loss: 0.7737 acc: 1.0\n",
      "epoch: 17 batch_num: 33 loss: 1.227 acc: 1.0\n",
      "epoch: 17 batch_num: 34 loss: 1.4148 acc: 0.984375\n",
      "epoch: 17 batch_num: 35 loss: 0.8294 acc: 1.0\n",
      "epoch: 17 batch_num: 36 loss: 0.4061 acc: 1.0\n",
      "epoch: 17 batch_num: 37 loss: 1.057 acc: 1.0\n",
      "epoch: 17 batch_num: 38 loss: 0.666 acc: 1.0\n",
      "epoch: 17 batch_num: 39 loss: 2.7669 acc: 0.984375\n",
      "epoch: 17 batch_num: 40 loss: 2.2906 acc: 0.984375\n",
      "epoch: 17 batch_num: 41 loss: 2.0135 acc: 0.984375\n",
      "epoch: 17 batch_num: 42 loss: 1.3339 acc: 1.0\n",
      "epoch: 17 batch_num: 43 loss: 0.8 acc: 1.0\n",
      "epoch: 17 batch_num: 44 loss: 0.4289 acc: 1.0\n",
      "epoch: 17 batch_num: 45 loss: 0.909 acc: 1.0\n",
      "epoch: 17 batch_num: 46 loss: 1.4236 acc: 1.0\n",
      "epoch: 17 batch_num: 47 loss: 1.1564 acc: 1.0\n",
      "epoch: 17 batch_num: 48 loss: 0.5449 acc: 1.0\n",
      "epoch: 17 batch_num: 49 loss: 0.7953 acc: 1.0\n",
      "epoch: 17 batch_num: 50 loss: 0.7485 acc: 1.0\n",
      "epoch: 17 batch_num: 51 loss: 0.7768 acc: 1.0\n",
      "epoch: 17 batch_num: 52 loss: 0.992 acc: 1.0\n",
      "epoch: 17 batch_num: 53 loss: 1.2533 acc: 1.0\n",
      "epoch: 17 batch_num: 54 loss: 1.0166 acc: 1.0\n",
      "epoch: 17 batch_num: 55 loss: 0.7514 acc: 1.0\n",
      "epoch: 17 batch_num: 56 loss: 0.883 acc: 1.0\n",
      "epoch: 17 batch_num: 57 loss: 0.8898 acc: 1.0\n",
      "epoch: 17 batch_num: 58 loss: 0.8712 acc: 1.0\n",
      "epoch: 17 batch_num: 59 loss: 1.2886 acc: 1.0\n",
      "epoch: 17 batch_num: 60 loss: 0.8806 acc: 1.0\n",
      "epoch: 17 batch_num: 61 loss: 2.7244 acc: 0.984375\n",
      "epoch: 17 batch_num: 62 loss: 0.9458 acc: 1.0\n",
      "epoch: 17 batch_num: 63 loss: 0.9135 acc: 1.0\n",
      "epoch: 17 batch_num: 64 loss: 5.5676 acc: 0.96875\n",
      "epoch: 17 batch_num: 65 loss: 0.6238 acc: 1.0\n",
      "epoch: 17 batch_num: 66 loss: 0.4226 acc: 1.0\n",
      "epoch: 17 batch_num: 67 loss: 0.9178 acc: 1.0\n",
      "epoch: 17 batch_num: 68 loss: 0.8155 acc: 1.0\n",
      "epoch: 17 batch_num: 69 loss: 0.9304 acc: 1.0\n",
      "epoch: 17 batch_num: 70 loss: 0.6069 acc: 1.0\n",
      "epoch: 17 batch_num: 71 loss: 0.941 acc: 1.0\n",
      "epoch: 17 batch_num: 72 loss: 1.0085 acc: 1.0\n",
      "epoch: 17 batch_num: 73 loss: 0.5174 acc: 1.0\n",
      "epoch: 17 batch_num: 74 loss: 0.4828 acc: 1.0\n",
      "epoch: 17 batch_num: 75 loss: 1.2238 acc: 1.0\n",
      "epoch: 17 batch_num: 76 loss: 0.811 acc: 1.0\n",
      "epoch: 17 batch_num: 77 loss: 0.5534 acc: 1.0\n",
      "epoch: 17 batch_num: 78 loss: 1.8458 acc: 1.0\n",
      "epoch: 17 batch_num: 79 loss: 0.5284 acc: 1.0\n",
      "epoch: 17 batch_num: 80 loss: 0.6385 acc: 1.0\n",
      "epoch: 17 batch_num: 81 loss: 1.3696 acc: 1.0\n",
      "epoch: 17 batch_num: 82 loss: 0.8135 acc: 1.0\n",
      "epoch: 17 batch_num: 83 loss: 0.7621 acc: 1.0\n",
      "epoch: 17 batch_num: 84 loss: 3.0562 acc: 0.984375\n",
      "epoch: 17 batch_num: 85 loss: 1.0301 acc: 1.0\n",
      "epoch: 17 batch_num: 86 loss: 0.7737 acc: 1.0\n",
      "epoch: 17 batch_num: 87 loss: 0.5421 acc: 1.0\n",
      "epoch: 17 batch_num: 88 loss: 1.2937 acc: 1.0\n",
      "epoch: 17 batch_num: 89 loss: 0.9128 acc: 1.0\n",
      "epoch: 17 batch_num: 90 loss: 0.8825 acc: 1.0\n",
      "epoch: 17 batch_num: 91 loss: 0.8566 acc: 1.0\n",
      "epoch: 17 batch_num: 92 loss: 0.6441 acc: 1.0\n",
      "epoch: 17 batch_num: 93 loss: 0.8402 acc: 1.0\n",
      "epoch: 17 batch_num: 94 loss: 0.9574 acc: 1.0\n",
      "epoch: 17 batch_num: 95 loss: 1.7509 acc: 1.0\n",
      "epoch: 17 batch_num: 96 loss: 0.8285 acc: 1.0\n",
      "epoch: 17 batch_num: 97 loss: 3.1454 acc: 0.984375\n",
      "epoch: 17 batch_num: 98 loss: 1.8111 acc: 0.984375\n",
      "epoch: 17 batch_num: 99 loss: 1.0505 acc: 1.0\n",
      "epoch: 17 batch_num: 100 loss: 0.811 acc: 1.0\n",
      "epoch: 17 batch_num: 101 loss: 0.868 acc: 1.0\n",
      "epoch: 17 batch_num: 102 loss: 0.8099 acc: 1.0\n",
      "epoch: 17 batch_num: 103 loss: 1.121 acc: 0.984375\n",
      "epoch: 17 batch_num: 104 loss: 1.5679 acc: 0.984375\n",
      "epoch: 17 batch_num: 105 loss: 5.2026 acc: 0.984375\n",
      "epoch: 17 batch_num: 106 loss: 2.1599 acc: 0.984375\n",
      "epoch: 17 batch_num: 107 loss: 0.9564 acc: 1.0\n",
      "epoch: 17 batch_num: 108 loss: 2.7948 acc: 0.984375\n",
      "epoch: 17 batch_num: 109 loss: 0.5437 acc: 1.0\n",
      "epoch: 17 batch_num: 110 loss: 4.4867 acc: 0.96875\n",
      "epoch: 17 batch_num: 111 loss: 0.7433 acc: 1.0\n",
      "epoch: 17 batch_num: 112 loss: 0.6171 acc: 1.0\n",
      "epoch: 17 batch_num: 113 loss: 0.4581 acc: 1.0\n",
      "epoch: 17 batch_num: 114 loss: 0.6272 acc: 1.0\n",
      "epoch: 17 batch_num: 115 loss: 1.2603 acc: 1.0\n",
      "epoch: 17 batch_num: 116 loss: 0.3139 acc: 1.0\n",
      "epoch: 18 batch_num: 1 loss: 1.1072 acc: 1.0\n",
      "epoch: 18 batch_num: 2 loss: 0.5508 acc: 1.0\n",
      "epoch: 18 batch_num: 3 loss: 0.556 acc: 1.0\n",
      "epoch: 18 batch_num: 4 loss: 1.3062 acc: 1.0\n",
      "epoch: 18 batch_num: 5 loss: 1.585 acc: 0.984375\n",
      "epoch: 18 batch_num: 6 loss: 0.8191 acc: 1.0\n",
      "epoch: 18 batch_num: 7 loss: 0.8184 acc: 1.0\n",
      "epoch: 18 batch_num: 8 loss: 0.4594 acc: 1.0\n",
      "epoch: 18 batch_num: 9 loss: 0.8983 acc: 1.0\n",
      "epoch: 18 batch_num: 10 loss: 0.915 acc: 1.0\n",
      "epoch: 18 batch_num: 11 loss: 0.7519 acc: 1.0\n",
      "epoch: 18 batch_num: 12 loss: 0.6892 acc: 1.0\n",
      "epoch: 18 batch_num: 13 loss: 0.7967 acc: 1.0\n",
      "epoch: 18 batch_num: 14 loss: 1.4385 acc: 1.0\n",
      "epoch: 18 batch_num: 15 loss: 0.9811 acc: 1.0\n",
      "epoch: 18 batch_num: 16 loss: 0.7106 acc: 1.0\n",
      "epoch: 18 batch_num: 17 loss: 0.471 acc: 1.0\n",
      "epoch: 18 batch_num: 18 loss: 0.7521 acc: 1.0\n",
      "epoch: 18 batch_num: 19 loss: 1.7117 acc: 1.0\n",
      "epoch: 18 batch_num: 20 loss: 0.6986 acc: 1.0\n",
      "epoch: 18 batch_num: 21 loss: 0.6772 acc: 1.0\n",
      "epoch: 18 batch_num: 22 loss: 0.8704 acc: 1.0\n",
      "epoch: 18 batch_num: 23 loss: 0.5557 acc: 1.0\n",
      "epoch: 18 batch_num: 24 loss: 2.316 acc: 0.984375\n",
      "epoch: 18 batch_num: 25 loss: 3.1794 acc: 0.984375\n",
      "epoch: 18 batch_num: 26 loss: 0.8912 acc: 1.0\n",
      "epoch: 18 batch_num: 27 loss: 0.8275 acc: 1.0\n",
      "epoch: 18 batch_num: 28 loss: 0.4584 acc: 1.0\n",
      "epoch: 18 batch_num: 29 loss: 1.0853 acc: 1.0\n",
      "epoch: 18 batch_num: 30 loss: 0.5838 acc: 1.0\n",
      "epoch: 18 batch_num: 31 loss: 0.5223 acc: 1.0\n",
      "epoch: 18 batch_num: 32 loss: 1.0407 acc: 1.0\n",
      "epoch: 18 batch_num: 33 loss: 1.0598 acc: 1.0\n",
      "epoch: 18 batch_num: 34 loss: 0.5493 acc: 1.0\n",
      "epoch: 18 batch_num: 35 loss: 0.9678 acc: 1.0\n",
      "epoch: 18 batch_num: 36 loss: 1.6075 acc: 0.984375\n",
      "epoch: 18 batch_num: 37 loss: 0.8666 acc: 1.0\n",
      "epoch: 18 batch_num: 38 loss: 1.0327 acc: 1.0\n",
      "epoch: 18 batch_num: 39 loss: 0.7595 acc: 1.0\n",
      "epoch: 18 batch_num: 40 loss: 0.7197 acc: 1.0\n",
      "epoch: 18 batch_num: 41 loss: 0.9687 acc: 1.0\n",
      "epoch: 18 batch_num: 42 loss: 4.5297 acc: 0.984375\n",
      "epoch: 18 batch_num: 43 loss: 2.332 acc: 0.984375\n",
      "epoch: 18 batch_num: 44 loss: 1.0799 acc: 1.0\n",
      "epoch: 18 batch_num: 45 loss: 0.6584 acc: 1.0\n",
      "epoch: 18 batch_num: 46 loss: 1.0332 acc: 1.0\n",
      "epoch: 18 batch_num: 47 loss: 0.923 acc: 1.0\n",
      "epoch: 18 batch_num: 48 loss: 1.2067 acc: 1.0\n",
      "epoch: 18 batch_num: 49 loss: 0.446 acc: 1.0\n",
      "epoch: 18 batch_num: 50 loss: 1.3553 acc: 0.984375\n",
      "epoch: 18 batch_num: 51 loss: 0.557 acc: 1.0\n",
      "epoch: 18 batch_num: 52 loss: 0.7021 acc: 1.0\n",
      "epoch: 18 batch_num: 53 loss: 0.5307 acc: 1.0\n",
      "epoch: 18 batch_num: 54 loss: 0.7439 acc: 1.0\n",
      "epoch: 18 batch_num: 55 loss: 1.1696 acc: 1.0\n",
      "epoch: 18 batch_num: 56 loss: 3.6051 acc: 0.984375\n",
      "epoch: 18 batch_num: 57 loss: 1.5629 acc: 0.984375\n",
      "epoch: 18 batch_num: 58 loss: 0.6082 acc: 1.0\n",
      "epoch: 18 batch_num: 59 loss: 0.8605 acc: 1.0\n",
      "epoch: 18 batch_num: 60 loss: 0.5947 acc: 1.0\n",
      "epoch: 18 batch_num: 61 loss: 0.6649 acc: 1.0\n",
      "epoch: 18 batch_num: 62 loss: 0.8751 acc: 1.0\n",
      "epoch: 18 batch_num: 63 loss: 0.9244 acc: 1.0\n",
      "epoch: 18 batch_num: 64 loss: 0.7411 acc: 1.0\n",
      "epoch: 18 batch_num: 65 loss: 0.6381 acc: 1.0\n",
      "epoch: 18 batch_num: 66 loss: 1.7027 acc: 0.984375\n",
      "epoch: 18 batch_num: 67 loss: 0.6178 acc: 1.0\n",
      "epoch: 18 batch_num: 68 loss: 0.7838 acc: 1.0\n",
      "epoch: 18 batch_num: 69 loss: 0.8058 acc: 1.0\n",
      "epoch: 18 batch_num: 70 loss: 0.8522 acc: 1.0\n",
      "epoch: 18 batch_num: 71 loss: 0.7237 acc: 1.0\n",
      "epoch: 18 batch_num: 72 loss: 0.427 acc: 1.0\n",
      "epoch: 18 batch_num: 73 loss: 1.0175 acc: 1.0\n",
      "epoch: 18 batch_num: 74 loss: 2.0472 acc: 1.0\n",
      "epoch: 18 batch_num: 75 loss: 0.4582 acc: 1.0\n",
      "epoch: 18 batch_num: 76 loss: 1.1088 acc: 1.0\n",
      "epoch: 18 batch_num: 77 loss: 0.741 acc: 1.0\n",
      "epoch: 18 batch_num: 78 loss: 0.6068 acc: 1.0\n",
      "epoch: 18 batch_num: 79 loss: 0.4562 acc: 1.0\n",
      "epoch: 18 batch_num: 80 loss: 1.54 acc: 1.0\n",
      "epoch: 18 batch_num: 81 loss: 2.5231 acc: 0.984375\n",
      "epoch: 18 batch_num: 82 loss: 1.3225 acc: 0.984375\n",
      "epoch: 18 batch_num: 83 loss: 0.8876 acc: 1.0\n",
      "epoch: 18 batch_num: 84 loss: 0.9339 acc: 1.0\n",
      "epoch: 18 batch_num: 85 loss: 2.9555 acc: 0.984375\n",
      "epoch: 18 batch_num: 86 loss: 0.4955 acc: 1.0\n",
      "epoch: 18 batch_num: 87 loss: 0.5343 acc: 1.0\n",
      "epoch: 18 batch_num: 88 loss: 0.9561 acc: 1.0\n",
      "epoch: 18 batch_num: 89 loss: 0.8168 acc: 1.0\n",
      "epoch: 18 batch_num: 90 loss: 0.6319 acc: 1.0\n",
      "epoch: 18 batch_num: 91 loss: 0.6539 acc: 1.0\n",
      "epoch: 18 batch_num: 92 loss: 0.4885 acc: 1.0\n",
      "epoch: 18 batch_num: 93 loss: 0.6073 acc: 1.0\n",
      "epoch: 18 batch_num: 94 loss: 0.6169 acc: 1.0\n",
      "epoch: 18 batch_num: 95 loss: 0.4871 acc: 1.0\n",
      "epoch: 18 batch_num: 96 loss: 2.1896 acc: 0.984375\n",
      "epoch: 18 batch_num: 97 loss: 0.3729 acc: 1.0\n",
      "epoch: 18 batch_num: 98 loss: 0.5373 acc: 1.0\n",
      "epoch: 18 batch_num: 99 loss: 1.124 acc: 1.0\n",
      "epoch: 18 batch_num: 100 loss: 1.4263 acc: 0.984375\n",
      "epoch: 18 batch_num: 101 loss: 4.8679 acc: 0.984375\n",
      "epoch: 18 batch_num: 102 loss: 1.739 acc: 0.984375\n",
      "epoch: 18 batch_num: 103 loss: 1.1106 acc: 1.0\n",
      "epoch: 18 batch_num: 104 loss: 0.827 acc: 1.0\n",
      "epoch: 18 batch_num: 105 loss: 0.6875 acc: 1.0\n",
      "epoch: 18 batch_num: 106 loss: 2.3523 acc: 0.984375\n",
      "epoch: 18 batch_num: 107 loss: 0.6098 acc: 1.0\n",
      "epoch: 18 batch_num: 108 loss: 0.6433 acc: 1.0\n",
      "epoch: 18 batch_num: 109 loss: 0.5602 acc: 1.0\n",
      "epoch: 18 batch_num: 110 loss: 0.6698 acc: 1.0\n",
      "epoch: 18 batch_num: 111 loss: 0.803 acc: 1.0\n",
      "epoch: 18 batch_num: 112 loss: 1.3173 acc: 1.0\n",
      "epoch: 18 batch_num: 113 loss: 0.6634 acc: 1.0\n",
      "epoch: 18 batch_num: 114 loss: 0.9369 acc: 1.0\n",
      "epoch: 18 batch_num: 115 loss: 4.3623 acc: 0.984375\n",
      "epoch: 18 batch_num: 116 loss: 0.4469 acc: 1.0\n",
      "epoch: 19 batch_num: 1 loss: 0.7006 acc: 1.0\n",
      "epoch: 19 batch_num: 2 loss: 1.1812 acc: 1.0\n",
      "epoch: 19 batch_num: 3 loss: 0.2177 acc: 1.0\n",
      "epoch: 19 batch_num: 4 loss: 0.3943 acc: 1.0\n",
      "epoch: 19 batch_num: 5 loss: 2.8179 acc: 0.984375\n",
      "epoch: 19 batch_num: 6 loss: 6.1422 acc: 0.96875\n",
      "epoch: 19 batch_num: 7 loss: 0.7764 acc: 1.0\n",
      "epoch: 19 batch_num: 8 loss: 0.5771 acc: 1.0\n",
      "epoch: 19 batch_num: 9 loss: 0.2875 acc: 1.0\n",
      "epoch: 19 batch_num: 10 loss: 1.3781 acc: 0.984375\n",
      "epoch: 19 batch_num: 11 loss: 0.5515 acc: 1.0\n",
      "epoch: 19 batch_num: 12 loss: 1.1659 acc: 1.0\n",
      "epoch: 19 batch_num: 13 loss: 0.5781 acc: 1.0\n",
      "epoch: 19 batch_num: 14 loss: 0.4083 acc: 1.0\n",
      "epoch: 19 batch_num: 15 loss: 0.842 acc: 1.0\n",
      "epoch: 19 batch_num: 16 loss: 0.6923 acc: 1.0\n",
      "epoch: 19 batch_num: 17 loss: 1.3394 acc: 1.0\n",
      "epoch: 19 batch_num: 18 loss: 0.6138 acc: 1.0\n",
      "epoch: 19 batch_num: 19 loss: 0.5885 acc: 1.0\n",
      "epoch: 19 batch_num: 20 loss: 0.5767 acc: 1.0\n",
      "epoch: 19 batch_num: 21 loss: 1.8713 acc: 0.984375\n",
      "epoch: 19 batch_num: 22 loss: 0.5305 acc: 1.0\n",
      "epoch: 19 batch_num: 23 loss: 0.6576 acc: 1.0\n",
      "epoch: 19 batch_num: 24 loss: 2.1938 acc: 0.984375\n",
      "epoch: 19 batch_num: 25 loss: 0.5245 acc: 1.0\n",
      "epoch: 19 batch_num: 26 loss: 1.4779 acc: 1.0\n",
      "epoch: 19 batch_num: 27 loss: 0.6296 acc: 1.0\n",
      "epoch: 19 batch_num: 28 loss: 0.6931 acc: 1.0\n",
      "epoch: 19 batch_num: 29 loss: 0.888 acc: 1.0\n",
      "epoch: 19 batch_num: 30 loss: 0.5814 acc: 1.0\n",
      "epoch: 19 batch_num: 31 loss: 1.0639 acc: 1.0\n",
      "epoch: 19 batch_num: 32 loss: 3.2181 acc: 0.984375\n",
      "epoch: 19 batch_num: 33 loss: 0.6157 acc: 1.0\n",
      "epoch: 19 batch_num: 34 loss: 0.4722 acc: 1.0\n",
      "epoch: 19 batch_num: 35 loss: 1.3123 acc: 1.0\n",
      "epoch: 19 batch_num: 36 loss: 0.958 acc: 1.0\n",
      "epoch: 19 batch_num: 37 loss: 0.5927 acc: 1.0\n",
      "epoch: 19 batch_num: 38 loss: 1.416 acc: 1.0\n",
      "epoch: 19 batch_num: 39 loss: 0.71 acc: 1.0\n",
      "epoch: 19 batch_num: 40 loss: 0.565 acc: 1.0\n",
      "epoch: 19 batch_num: 41 loss: 1.2475 acc: 1.0\n",
      "epoch: 19 batch_num: 42 loss: 0.2456 acc: 1.0\n",
      "epoch: 19 batch_num: 43 loss: 1.1976 acc: 0.984375\n",
      "epoch: 19 batch_num: 44 loss: 0.3676 acc: 1.0\n",
      "epoch: 19 batch_num: 45 loss: 0.5597 acc: 1.0\n",
      "epoch: 19 batch_num: 46 loss: 0.609 acc: 1.0\n",
      "epoch: 19 batch_num: 47 loss: 0.5478 acc: 1.0\n",
      "epoch: 19 batch_num: 48 loss: 0.7394 acc: 1.0\n",
      "epoch: 19 batch_num: 49 loss: 0.8623 acc: 1.0\n",
      "epoch: 19 batch_num: 50 loss: 0.5703 acc: 1.0\n",
      "epoch: 19 batch_num: 51 loss: 0.4492 acc: 1.0\n",
      "epoch: 19 batch_num: 52 loss: 0.5453 acc: 1.0\n",
      "epoch: 19 batch_num: 53 loss: 0.7593 acc: 1.0\n",
      "epoch: 19 batch_num: 54 loss: 2.1107 acc: 0.984375\n",
      "epoch: 19 batch_num: 55 loss: 1.037 acc: 1.0\n",
      "epoch: 19 batch_num: 56 loss: 2.8102 acc: 0.984375\n",
      "epoch: 19 batch_num: 57 loss: 0.6272 acc: 1.0\n",
      "epoch: 19 batch_num: 58 loss: 0.4914 acc: 1.0\n",
      "epoch: 19 batch_num: 59 loss: 0.6807 acc: 1.0\n",
      "epoch: 19 batch_num: 60 loss: 0.3635 acc: 1.0\n",
      "epoch: 19 batch_num: 61 loss: 0.7213 acc: 1.0\n",
      "epoch: 19 batch_num: 62 loss: 0.8649 acc: 1.0\n",
      "epoch: 19 batch_num: 63 loss: 0.5824 acc: 1.0\n",
      "epoch: 19 batch_num: 64 loss: 0.4812 acc: 1.0\n",
      "epoch: 19 batch_num: 65 loss: 0.5673 acc: 1.0\n",
      "epoch: 19 batch_num: 66 loss: 3.7508 acc: 0.96875\n",
      "epoch: 19 batch_num: 67 loss: 0.7097 acc: 1.0\n",
      "epoch: 19 batch_num: 68 loss: 0.4962 acc: 1.0\n",
      "epoch: 19 batch_num: 69 loss: 0.7126 acc: 1.0\n",
      "epoch: 19 batch_num: 70 loss: 0.3199 acc: 1.0\n",
      "epoch: 19 batch_num: 71 loss: 0.4393 acc: 1.0\n",
      "epoch: 19 batch_num: 72 loss: 0.9025 acc: 1.0\n",
      "epoch: 19 batch_num: 73 loss: 0.8813 acc: 1.0\n",
      "epoch: 19 batch_num: 74 loss: 0.5592 acc: 1.0\n",
      "epoch: 19 batch_num: 75 loss: 0.5781 acc: 1.0\n",
      "epoch: 19 batch_num: 76 loss: 0.6972 acc: 1.0\n",
      "epoch: 19 batch_num: 77 loss: 0.8463 acc: 1.0\n",
      "epoch: 19 batch_num: 78 loss: 0.9124 acc: 1.0\n",
      "epoch: 19 batch_num: 79 loss: 0.5752 acc: 1.0\n",
      "epoch: 19 batch_num: 80 loss: 2.8814 acc: 0.984375\n",
      "epoch: 19 batch_num: 81 loss: 0.3136 acc: 1.0\n",
      "epoch: 19 batch_num: 82 loss: 0.3357 acc: 1.0\n",
      "epoch: 19 batch_num: 83 loss: 1.483 acc: 0.984375\n",
      "epoch: 19 batch_num: 84 loss: 0.6044 acc: 1.0\n",
      "epoch: 19 batch_num: 85 loss: 0.4379 acc: 1.0\n",
      "epoch: 19 batch_num: 86 loss: 0.4147 acc: 1.0\n",
      "epoch: 19 batch_num: 87 loss: 0.9395 acc: 1.0\n",
      "epoch: 19 batch_num: 88 loss: 0.8923 acc: 1.0\n",
      "epoch: 19 batch_num: 89 loss: 0.6347 acc: 1.0\n",
      "epoch: 19 batch_num: 90 loss: 0.3164 acc: 1.0\n",
      "epoch: 19 batch_num: 91 loss: 0.9854 acc: 1.0\n",
      "epoch: 19 batch_num: 92 loss: 2.4852 acc: 0.984375\n",
      "epoch: 19 batch_num: 93 loss: 0.363 acc: 1.0\n",
      "epoch: 19 batch_num: 94 loss: 0.4257 acc: 1.0\n",
      "epoch: 19 batch_num: 95 loss: 0.8955 acc: 1.0\n",
      "epoch: 19 batch_num: 96 loss: 2.0212 acc: 0.984375\n",
      "epoch: 19 batch_num: 97 loss: 0.9227 acc: 1.0\n",
      "epoch: 19 batch_num: 98 loss: 0.465 acc: 1.0\n",
      "epoch: 19 batch_num: 99 loss: 0.2727 acc: 1.0\n",
      "epoch: 19 batch_num: 100 loss: 0.6788 acc: 1.0\n",
      "epoch: 19 batch_num: 101 loss: 0.2865 acc: 1.0\n",
      "epoch: 19 batch_num: 102 loss: 0.4947 acc: 1.0\n",
      "epoch: 19 batch_num: 103 loss: 0.4202 acc: 1.0\n",
      "epoch: 19 batch_num: 104 loss: 0.3342 acc: 1.0\n",
      "epoch: 19 batch_num: 105 loss: 1.7655 acc: 1.0\n",
      "epoch: 19 batch_num: 106 loss: 0.5694 acc: 1.0\n",
      "epoch: 19 batch_num: 107 loss: 1.2427 acc: 1.0\n",
      "epoch: 19 batch_num: 108 loss: 9.0962 acc: 0.96875\n",
      "epoch: 19 batch_num: 109 loss: 0.3928 acc: 1.0\n",
      "epoch: 19 batch_num: 110 loss: 1.7343 acc: 0.984375\n",
      "epoch: 19 batch_num: 111 loss: 1.0891 acc: 1.0\n",
      "epoch: 19 batch_num: 112 loss: 1.0575 acc: 1.0\n",
      "epoch: 19 batch_num: 113 loss: 0.6979 acc: 1.0\n",
      "epoch: 19 batch_num: 114 loss: 1.0969 acc: 1.0\n",
      "epoch: 19 batch_num: 115 loss: 0.8052 acc: 1.0\n",
      "epoch: 19 batch_num: 116 loss: 0.3101 acc: 1.0\n",
      "epoch: 20 batch_num: 1 loss: 0.8132 acc: 1.0\n",
      "epoch: 20 batch_num: 2 loss: 3.0655 acc: 0.984375\n",
      "epoch: 20 batch_num: 3 loss: 0.5487 acc: 1.0\n",
      "epoch: 20 batch_num: 4 loss: 0.8385 acc: 1.0\n",
      "epoch: 20 batch_num: 5 loss: 0.4728 acc: 1.0\n",
      "epoch: 20 batch_num: 6 loss: 0.4967 acc: 1.0\n",
      "epoch: 20 batch_num: 7 loss: 0.4501 acc: 1.0\n",
      "epoch: 20 batch_num: 8 loss: 0.8061 acc: 1.0\n",
      "epoch: 20 batch_num: 9 loss: 0.7764 acc: 1.0\n",
      "epoch: 20 batch_num: 10 loss: 0.8064 acc: 1.0\n",
      "epoch: 20 batch_num: 11 loss: 0.4285 acc: 1.0\n",
      "epoch: 20 batch_num: 12 loss: 0.4573 acc: 1.0\n",
      "epoch: 20 batch_num: 13 loss: 0.7663 acc: 1.0\n",
      "epoch: 20 batch_num: 14 loss: 0.7405 acc: 1.0\n",
      "epoch: 20 batch_num: 15 loss: 0.441 acc: 1.0\n",
      "epoch: 20 batch_num: 16 loss: 0.7583 acc: 1.0\n",
      "epoch: 20 batch_num: 17 loss: 0.5535 acc: 1.0\n",
      "epoch: 20 batch_num: 18 loss: 6.7025 acc: 0.984375\n",
      "epoch: 20 batch_num: 19 loss: 0.7095 acc: 1.0\n",
      "epoch: 20 batch_num: 20 loss: 0.323 acc: 1.0\n",
      "epoch: 20 batch_num: 21 loss: 0.9638 acc: 1.0\n",
      "epoch: 20 batch_num: 22 loss: 0.5246 acc: 1.0\n",
      "epoch: 20 batch_num: 23 loss: 0.4777 acc: 1.0\n",
      "epoch: 20 batch_num: 24 loss: 0.4193 acc: 1.0\n",
      "epoch: 20 batch_num: 25 loss: 0.2153 acc: 1.0\n",
      "epoch: 20 batch_num: 26 loss: 0.6872 acc: 1.0\n",
      "epoch: 20 batch_num: 27 loss: 0.9122 acc: 1.0\n",
      "epoch: 20 batch_num: 28 loss: 0.8939 acc: 1.0\n",
      "epoch: 20 batch_num: 29 loss: 0.4113 acc: 1.0\n",
      "epoch: 20 batch_num: 30 loss: 0.9254 acc: 1.0\n",
      "epoch: 20 batch_num: 31 loss: 0.8074 acc: 1.0\n",
      "epoch: 20 batch_num: 32 loss: 0.917 acc: 1.0\n",
      "epoch: 20 batch_num: 33 loss: 0.618 acc: 1.0\n",
      "epoch: 20 batch_num: 34 loss: 0.816 acc: 1.0\n",
      "epoch: 20 batch_num: 35 loss: 0.4965 acc: 1.0\n",
      "epoch: 20 batch_num: 36 loss: 0.3624 acc: 1.0\n",
      "epoch: 20 batch_num: 37 loss: 0.3297 acc: 1.0\n",
      "epoch: 20 batch_num: 38 loss: 0.532 acc: 1.0\n",
      "epoch: 20 batch_num: 39 loss: 0.4745 acc: 1.0\n",
      "epoch: 20 batch_num: 40 loss: 1.4612 acc: 0.984375\n",
      "epoch: 20 batch_num: 41 loss: 4.4051 acc: 0.984375\n",
      "epoch: 20 batch_num: 42 loss: 0.5285 acc: 1.0\n",
      "epoch: 20 batch_num: 43 loss: 2.3339 acc: 0.984375\n",
      "epoch: 20 batch_num: 44 loss: 0.8481 acc: 1.0\n",
      "epoch: 20 batch_num: 45 loss: 0.4116 acc: 1.0\n",
      "epoch: 20 batch_num: 46 loss: 0.7499 acc: 1.0\n",
      "epoch: 20 batch_num: 47 loss: 0.2479 acc: 1.0\n",
      "epoch: 20 batch_num: 48 loss: 0.4362 acc: 1.0\n",
      "epoch: 20 batch_num: 49 loss: 3.9978 acc: 0.984375\n",
      "epoch: 20 batch_num: 50 loss: 0.6079 acc: 1.0\n",
      "epoch: 20 batch_num: 51 loss: 0.9704 acc: 1.0\n",
      "epoch: 20 batch_num: 52 loss: 0.3489 acc: 1.0\n",
      "epoch: 20 batch_num: 53 loss: 0.3421 acc: 1.0\n",
      "epoch: 20 batch_num: 54 loss: 0.3667 acc: 1.0\n",
      "epoch: 20 batch_num: 55 loss: 0.7125 acc: 1.0\n",
      "epoch: 20 batch_num: 56 loss: 0.5413 acc: 1.0\n",
      "epoch: 20 batch_num: 57 loss: 0.5426 acc: 1.0\n",
      "epoch: 20 batch_num: 58 loss: 0.5311 acc: 1.0\n",
      "epoch: 20 batch_num: 59 loss: 1.5049 acc: 0.984375\n",
      "epoch: 20 batch_num: 60 loss: 0.3519 acc: 1.0\n",
      "epoch: 20 batch_num: 61 loss: 0.6536 acc: 1.0\n",
      "epoch: 20 batch_num: 62 loss: 0.8554 acc: 1.0\n",
      "epoch: 20 batch_num: 63 loss: 0.7427 acc: 1.0\n",
      "epoch: 20 batch_num: 64 loss: 0.5817 acc: 1.0\n",
      "epoch: 20 batch_num: 65 loss: 0.4845 acc: 1.0\n",
      "epoch: 20 batch_num: 66 loss: 1.526 acc: 1.0\n",
      "epoch: 20 batch_num: 67 loss: 0.4983 acc: 1.0\n",
      "epoch: 20 batch_num: 68 loss: 1.9254 acc: 0.984375\n",
      "epoch: 20 batch_num: 69 loss: 1.0184 acc: 1.0\n",
      "epoch: 20 batch_num: 70 loss: 0.6018 acc: 1.0\n",
      "epoch: 20 batch_num: 71 loss: 0.2613 acc: 1.0\n",
      "epoch: 20 batch_num: 72 loss: 8.0554 acc: 0.984375\n",
      "epoch: 20 batch_num: 73 loss: 0.5937 acc: 1.0\n",
      "epoch: 20 batch_num: 74 loss: 0.9666 acc: 1.0\n",
      "epoch: 20 batch_num: 75 loss: 0.417 acc: 1.0\n",
      "epoch: 20 batch_num: 76 loss: 0.2973 acc: 1.0\n",
      "epoch: 20 batch_num: 77 loss: 1.4518 acc: 1.0\n",
      "epoch: 20 batch_num: 78 loss: 2.984 acc: 0.984375\n",
      "epoch: 20 batch_num: 79 loss: 0.5174 acc: 1.0\n",
      "epoch: 20 batch_num: 80 loss: 0.3125 acc: 1.0\n",
      "epoch: 20 batch_num: 81 loss: 0.3394 acc: 1.0\n",
      "epoch: 20 batch_num: 82 loss: 0.5298 acc: 1.0\n",
      "epoch: 20 batch_num: 83 loss: 3.873 acc: 0.984375\n",
      "epoch: 20 batch_num: 84 loss: 0.3514 acc: 1.0\n",
      "epoch: 20 batch_num: 85 loss: 0.6329 acc: 1.0\n",
      "epoch: 20 batch_num: 86 loss: 0.5438 acc: 1.0\n",
      "epoch: 20 batch_num: 87 loss: 0.419 acc: 1.0\n",
      "epoch: 20 batch_num: 88 loss: 0.4832 acc: 1.0\n",
      "epoch: 20 batch_num: 89 loss: 0.7183 acc: 1.0\n",
      "epoch: 20 batch_num: 90 loss: 0.9307 acc: 1.0\n",
      "epoch: 20 batch_num: 91 loss: 0.6542 acc: 1.0\n",
      "epoch: 20 batch_num: 92 loss: 1.1432 acc: 1.0\n",
      "epoch: 20 batch_num: 93 loss: 0.4929 acc: 1.0\n",
      "epoch: 20 batch_num: 94 loss: 0.5203 acc: 1.0\n",
      "epoch: 20 batch_num: 95 loss: 0.569 acc: 1.0\n",
      "epoch: 20 batch_num: 96 loss: 1.1467 acc: 1.0\n",
      "epoch: 20 batch_num: 97 loss: 1.7544 acc: 0.984375\n",
      "epoch: 20 batch_num: 98 loss: 0.6517 acc: 1.0\n",
      "epoch: 20 batch_num: 99 loss: 0.4141 acc: 1.0\n",
      "epoch: 20 batch_num: 100 loss: 0.8391 acc: 1.0\n",
      "epoch: 20 batch_num: 101 loss: 0.3568 acc: 1.0\n",
      "epoch: 20 batch_num: 102 loss: 0.2668 acc: 1.0\n",
      "epoch: 20 batch_num: 103 loss: 0.3524 acc: 1.0\n",
      "epoch: 20 batch_num: 104 loss: 0.9713 acc: 1.0\n",
      "epoch: 20 batch_num: 105 loss: 0.9999 acc: 1.0\n",
      "epoch: 20 batch_num: 106 loss: 0.7353 acc: 1.0\n",
      "epoch: 20 batch_num: 107 loss: 4.2049 acc: 0.984375\n",
      "epoch: 20 batch_num: 108 loss: 0.6127 acc: 1.0\n",
      "epoch: 20 batch_num: 109 loss: 0.627 acc: 1.0\n",
      "epoch: 20 batch_num: 110 loss: 0.958 acc: 1.0\n",
      "epoch: 20 batch_num: 111 loss: 2.8962 acc: 0.984375\n",
      "epoch: 20 batch_num: 112 loss: 0.7534 acc: 1.0\n",
      "epoch: 20 batch_num: 113 loss: 1.9624 acc: 0.984375\n",
      "epoch: 20 batch_num: 114 loss: 0.7247 acc: 1.0\n",
      "epoch: 20 batch_num: 115 loss: 1.0881 acc: 1.0\n",
      "epoch: 20 batch_num: 116 loss: 0.9638 acc: 0.9655172413793104\n",
      "max_acc: 1.0\n",
      "test acc: 0.6393939393939394\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # loss function\n",
    "english_model = AnswerableClassifier(vocab_size=10004, num_labels=2, num_hidden=100).to('cuda')\n",
    "optimizer = torch.optim.Adam(english_model.parameters(), lr = lr, amsgrad=True)\n",
    "\n",
    "max_acc = train_features_model(model = english_model, train_loader=train_features_model_loader,\n",
    "                               criterion= criterion, optimizer=optimizer, model_file_name=\"english_model.pth\",\n",
    "                               epochs = 20)\n",
    "print(\"max_acc:\", max_acc)\n",
    "english_model.load_state_dict(torch.load(\"english_model.pth\"))\n",
    "english_model.eval()\n",
    "predict_label = english_model(val_features)\n",
    "pred = predict_label.max(-1, keepdim=True)[1]\n",
    "label = val_label\n",
    "test_acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "\n",
    "print(\"test acc:\", test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
