{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "'''\n",
    "@File    : data_processing.py\n",
    "@IDE     : PyCharm\n",
    "@Author  : Yaokun Li\n",
    "@Date    : 2022/10/18 20:30\n",
    "@Description :\n",
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import gensim\n",
    "from torchtext.vocab import Vocab\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import trigrams\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "\n",
    "def getLanguageDataSet(data, language):\n",
    "    return data.filter(lambda x: x['language'] == language)\n",
    "\n",
    "\n",
    "def getJapaneseDataSet(data):\n",
    "    return getLanguageDataSet(data, \"japanese\")\n",
    "\n",
    "\n",
    "def getEnglishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"english\")\n",
    "\n",
    "\n",
    "def getFinnishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"finnish\")\n",
    "\n",
    "\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "UNK, PAD = '<UNK>', '<PAD>'\n",
    "\n",
    "\n",
    "def build_vocab(sent_list, max_size, min_freq, tokenizer):\n",
    "    vocab_dic = {}\n",
    "    for sent in sent_list:\n",
    "        for word in tokenizer(sent):\n",
    "            vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[\n",
    "                 :max_size]\n",
    "    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n",
    "    return vocab_dic\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "\n",
    "class QADataSet():\n",
    "    def __init__(self, tokenizer, dataset):\n",
    "        self.vocabulary = None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.question = []\n",
    "        self.answer_text = []\n",
    "        self.answer_start = []\n",
    "        self.document = []\n",
    "        self.tokenized_question = []\n",
    "        self.tokenized_answer_text = []\n",
    "        self.tokenized_document = []\n",
    "        self.answer_label = []\n",
    "\n",
    "        for element in dataset:\n",
    "            self.question.append(element[\"question_text\"].lower())\n",
    "            self.answer_text.append(element[\"annotations\"][\"answer_text\"][0])\n",
    "            self.answer_start.append(element[\"annotations\"][\"answer_start\"])\n",
    "            self.document.append(element[\"document_plaintext\"].lower())\n",
    "            if (element[\"annotations\"][\"answer_start\"] == [-1]):\n",
    "                self.answer_label.append(torch.tensor([0], dtype=torch.int64).cuda())\n",
    "            else:\n",
    "                self.answer_label.append(torch.tensor([1], dtype=torch.int64).cuda())\n",
    "\n",
    "\n",
    "        for s in self.answer_text:\n",
    "            self.tokenized_answer_text.append(self.__tokenize(s))\n",
    "\n",
    "        for s in self.question:\n",
    "            self.tokenized_question.append(self.__tokenize(s))\n",
    "\n",
    "        for s in self.document:\n",
    "            self.tokenized_document.append(self.__tokenize(s))\n",
    "\n",
    "        self.get_vocab()\n",
    "        self.document_num = []\n",
    "        self.question_num = []\n",
    "        for sent in self.tokenized_document:\n",
    "            self.document_num.append([self.vocabulary.get(word, MAX_VOCAB_SIZE) for word in sent])\n",
    "        for sent in self.tokenized_question:\n",
    "            self.question_num.append([self.vocabulary.get(word, MAX_VOCAB_SIZE) for word in sent])\n",
    "\n",
    "    def get_vocab(self):\n",
    "        self.vocabulary = build_vocab(self.question + self.document, MAX_VOCAB_SIZE, 2, self.tokenizer)\n",
    "\n",
    "        return self.vocabulary\n",
    "\n",
    "    def __tokenize(self, l, with_stop_word=True):\n",
    "        return self.tokenizer(l)\n",
    "\n",
    "    def get_overlaps_words_num(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.question_num, self.document_num):\n",
    "            count = 0\n",
    "            for word in question:\n",
    "                if word in document:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_document_length(self):\n",
    "        return [len(document) for document in self.document_num]\n",
    "\n",
    "    def get_question_length(self):\n",
    "        return [len(question) for question in self.question_num]\n",
    "\n",
    "    def get_overlaps_2_gram(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.tokenized_question, self.tokenized_document):\n",
    "            count = 0\n",
    "            doc_bigrams = list(bigrams(document))\n",
    "            for word in bigrams(question):\n",
    "                if word in doc_bigrams:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_overlaps_3_gram(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.tokenized_question, self.tokenized_document):\n",
    "            count = 0\n",
    "            doc_bigrams = list(trigrams(document))\n",
    "            for word in trigrams(question):\n",
    "                if word in doc_bigrams:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_label(self):\n",
    "        return torch.cat(self.answer_label, dim=0)\n",
    "\n",
    "    def get_question_bow(self, vocab_size):\n",
    "        data = []\n",
    "        for ques in self.question_num:\n",
    "            bow = [0]*vocab_size\n",
    "            for word in ques:\n",
    "                bow[word] += 1\n",
    "            data.append(bow)\n",
    "        return data\n",
    "\n",
    "    def get_doc_bow(self, vocab_size):\n",
    "        data = []\n",
    "        for ques in self.document_num:\n",
    "            bow = [0] * vocab_size\n",
    "            for word in ques:\n",
    "                bow[word] += 1\n",
    "            data.append(bow)\n",
    "        return data\n",
    "\n",
    "    def get_features(self):\n",
    "        feature1 = self.get_overlaps_words_num()\n",
    "        feature2 = self.get_overlaps_2_gram()\n",
    "        feature5 = self.get_overlaps_3_gram()\n",
    "        # feature3 = self.get_document_length()\n",
    "        # feature4 = self.get_question_length()\n",
    "        feature_ques_bow = torch.Tensor(self.get_question_bow(MAX_VOCAB_SIZE + 1)).cuda()\n",
    "        feature_doc_bow = torch.Tensor(self.get_doc_bow(MAX_VOCAB_SIZE + 1)).cuda()\n",
    "        X = torch.Tensor([ feature2, feature5]).t().cuda()\n",
    "        return torch.cat([feature_ques_bow,feature_doc_bow, X], dim = 1)\n",
    "\n",
    "    def get_answer_text_vec(self):\n",
    "        w2vModel = gensim.models.KeyedVectors.load_word2vec_format(\"week1/vector.txt\", binary=False)\n",
    "        data = []\n",
    "        en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in self.tokenized_answer_text],\n",
    "                                  dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()\n",
    "        else:\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True)\n",
    "\n",
    "        data.append(en_tensor_)\n",
    "        return torch.cat(data, dim=0)\n",
    "\n",
    "    def get_document_vec(self):\n",
    "        w2vModel = gensim.models.KeyedVectors.load_word2vec_format(\"week1/vector.txt\", binary=False)\n",
    "        data = []\n",
    "        en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in self.tokenized_document],\n",
    "                                  dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()\n",
    "        else:\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True)\n",
    "\n",
    "        data.append(en_tensor_)\n",
    "        return torch.cat(data, dim=0)\n",
    "\n",
    "    def get_question_vec(self):\n",
    "        w2vModel = gensim.models.KeyedVectors.load_word2vec_format(\"week1/vector.txt\", binary=False)\n",
    "        data = []\n",
    "        en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in self.tokenized_question],\n",
    "                                  dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()\n",
    "        else:\n",
    "            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True)\n",
    "\n",
    "        data.append(en_tensor_)\n",
    "        return torch.cat(data, dim=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class AnswerableClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels = 2, num_hidden = 100):\n",
    "        super(AnswerableClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(num_hidden, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return self.final(self.nonlinear(self.dropout(self.linear(bow_vec))))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import tokenizer\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6\n",
      "Reusing dataset parquet (/home/lyk/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9780b9168c7a434a8823bee96d39f684"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 64\n",
    "lr = 0.0005"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyk/anaconda3/envs/nlp_gpu/lib/python3.6/site-packages/spacy/util.py:1504: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "spacy_tokenizer = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "torch_tokenizer = get_tokenizer('basic_english', language=\"en\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/lyk/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e961ad4e6a80dccd.arrow\n"
     ]
    }
   ],
   "source": [
    "train_english_qa_dataset = QADataSet(torch_tokenizer, tokenizer.getEnglishDataSet(train_set))\n",
    "train_features = train_english_qa_dataset.get_features()\n",
    "train_label = train_english_qa_dataset.get_label()\n",
    "train_features_model_dataset = Data.TensorDataset(train_features, train_label)\n",
    "train_features_model_loader = Data.DataLoader(dataset=train_features_model_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'ja_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-58-b46dee0952a2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtorch_japanese_tokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspacy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ja_core_news_sm\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/nlp_gpu/lib/python3.6/site-packages/spacy/__init__.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \"\"\"\n\u001B[1;32m     50\u001B[0m     return util.load_model(\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdisable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexclude\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mexclude\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m     )\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/nlp_gpu/lib/python3.6/site-packages/spacy/util.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    329\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mOLD_MODEL_SHORTCUTS\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mErrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mE941\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfull\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mOLD_MODEL_SHORTCUTS\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 331\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mErrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mE050\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    332\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'ja_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "torch_japanese_tokenizer = spacy.load(\"ja_core_news_sm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_japanese_qa_dataset = QADataSet(torch_japanese_tokenizer, tokenizer.getEnglishDataSet(train_set))\n",
    "train_features = train_english_qa_dataset.get_features()\n",
    "train_label = train_english_qa_dataset.get_label()\n",
    "train_features_model_dataset = Data.TensorDataset(train_features, train_label)\n",
    "train_features_model_loader = Data.DataLoader(dataset=train_features_model_dataset,\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/lyk/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5432f3a6e01e68fc.arrow\n"
     ]
    }
   ],
   "source": [
    "val_english_qa_dataset = QADataSet(torch_tokenizer,\n",
    "                                   tokenizer.getEnglishDataSet(validation_set))\n",
    "val_features = val_english_qa_dataset.get_features()\n",
    "val_label = val_english_qa_dataset.get_label()\n",
    "val_features_model_dataset = Data.TensorDataset(val_features, val_label)\n",
    "val_features_model_loader = Data.DataLoader(dataset=val_features_model_dataset,\n",
    "                                            batch_size= batch_size,\n",
    "                                            shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def train_features_model( model, train_loader, criterion, optimizer, model_file_name, epochs):\n",
    "    max_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_num = 0\n",
    "\n",
    "        for features, label in train_loader:\n",
    "            predict_label = model(features)\n",
    "            loss = criterion(predict_label, label)\n",
    "\n",
    "            pred = predict_label.max(-1, keepdim=True)[1]\n",
    "            acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            if (acc > max_acc):\n",
    "                max_acc = acc\n",
    "                torch.save(model.state_dict(), model_file_name)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_num += 1\n",
    "            print(\"epoch:\", epoch + 1, \"batch_num:\", batch_num, \"loss:\", round(loss.item(), 4), \"acc:\", acc)\n",
    "    return max_acc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 1 loss: 44.5477 acc: 0.46875\n",
      "epoch: 1 batch_num: 2 loss: 43.6776 acc: 0.546875\n",
      "epoch: 1 batch_num: 3 loss: 44.116 acc: 0.5\n",
      "epoch: 1 batch_num: 4 loss: 44.3665 acc: 0.53125\n",
      "epoch: 1 batch_num: 5 loss: 42.6895 acc: 0.546875\n",
      "epoch: 1 batch_num: 6 loss: 44.0525 acc: 0.484375\n",
      "epoch: 1 batch_num: 7 loss: 46.041 acc: 0.5\n",
      "epoch: 1 batch_num: 8 loss: 43.9476 acc: 0.4375\n",
      "epoch: 1 batch_num: 9 loss: 43.1505 acc: 0.546875\n",
      "epoch: 1 batch_num: 10 loss: 42.0313 acc: 0.734375\n",
      "epoch: 1 batch_num: 11 loss: 42.6853 acc: 0.5625\n",
      "epoch: 1 batch_num: 12 loss: 42.9471 acc: 0.5\n",
      "epoch: 1 batch_num: 13 loss: 44.6437 acc: 0.46875\n",
      "epoch: 1 batch_num: 14 loss: 41.808 acc: 0.546875\n",
      "epoch: 1 batch_num: 15 loss: 42.427 acc: 0.5\n",
      "epoch: 1 batch_num: 16 loss: 41.5709 acc: 0.625\n",
      "epoch: 1 batch_num: 17 loss: 43.6721 acc: 0.46875\n",
      "epoch: 1 batch_num: 18 loss: 42.8121 acc: 0.53125\n",
      "epoch: 1 batch_num: 19 loss: 41.2544 acc: 0.625\n",
      "epoch: 1 batch_num: 20 loss: 41.7122 acc: 0.671875\n",
      "epoch: 1 batch_num: 21 loss: 41.6466 acc: 0.671875\n",
      "epoch: 1 batch_num: 22 loss: 41.1913 acc: 0.65625\n",
      "epoch: 1 batch_num: 23 loss: 40.9608 acc: 0.703125\n",
      "epoch: 1 batch_num: 24 loss: 39.9754 acc: 0.71875\n",
      "epoch: 1 batch_num: 25 loss: 41.9644 acc: 0.65625\n",
      "epoch: 1 batch_num: 26 loss: 40.617 acc: 0.640625\n",
      "epoch: 1 batch_num: 27 loss: 40.2774 acc: 0.75\n",
      "epoch: 1 batch_num: 28 loss: 39.8997 acc: 0.765625\n",
      "epoch: 1 batch_num: 29 loss: 42.0595 acc: 0.65625\n",
      "epoch: 1 batch_num: 30 loss: 39.2992 acc: 0.765625\n",
      "epoch: 1 batch_num: 31 loss: 37.8997 acc: 0.734375\n",
      "epoch: 1 batch_num: 32 loss: 42.2128 acc: 0.65625\n",
      "epoch: 1 batch_num: 33 loss: 38.014 acc: 0.703125\n",
      "epoch: 1 batch_num: 34 loss: 39.9559 acc: 0.65625\n",
      "epoch: 1 batch_num: 35 loss: 39.385 acc: 0.6875\n",
      "epoch: 1 batch_num: 36 loss: 39.4706 acc: 0.6875\n",
      "epoch: 1 batch_num: 37 loss: 39.3901 acc: 0.703125\n",
      "epoch: 1 batch_num: 38 loss: 40.2624 acc: 0.609375\n",
      "epoch: 1 batch_num: 39 loss: 44.1954 acc: 0.578125\n",
      "epoch: 1 batch_num: 40 loss: 42.2003 acc: 0.625\n",
      "epoch: 1 batch_num: 41 loss: 37.3135 acc: 0.71875\n",
      "epoch: 1 batch_num: 42 loss: 39.1813 acc: 0.703125\n",
      "epoch: 1 batch_num: 43 loss: 39.3876 acc: 0.625\n",
      "epoch: 1 batch_num: 44 loss: 37.0219 acc: 0.75\n",
      "epoch: 1 batch_num: 45 loss: 37.8465 acc: 0.765625\n",
      "epoch: 1 batch_num: 46 loss: 36.4633 acc: 0.828125\n",
      "epoch: 1 batch_num: 47 loss: 38.4786 acc: 0.6875\n",
      "epoch: 1 batch_num: 48 loss: 38.2299 acc: 0.65625\n",
      "epoch: 1 batch_num: 49 loss: 34.8623 acc: 0.78125\n",
      "epoch: 1 batch_num: 50 loss: 35.54 acc: 0.703125\n",
      "epoch: 1 batch_num: 51 loss: 37.0443 acc: 0.734375\n",
      "epoch: 1 batch_num: 52 loss: 40.1889 acc: 0.734375\n",
      "epoch: 1 batch_num: 53 loss: 36.3205 acc: 0.8125\n",
      "epoch: 1 batch_num: 54 loss: 37.1286 acc: 0.765625\n",
      "epoch: 1 batch_num: 55 loss: 37.5781 acc: 0.734375\n",
      "epoch: 1 batch_num: 56 loss: 33.7247 acc: 0.734375\n",
      "epoch: 1 batch_num: 57 loss: 41.1986 acc: 0.765625\n",
      "epoch: 1 batch_num: 58 loss: 36.3701 acc: 0.75\n",
      "epoch: 1 batch_num: 59 loss: 40.6912 acc: 0.671875\n",
      "epoch: 1 batch_num: 60 loss: 36.572 acc: 0.75\n",
      "epoch: 1 batch_num: 61 loss: 37.4421 acc: 0.6875\n",
      "epoch: 1 batch_num: 62 loss: 34.509 acc: 0.78125\n",
      "epoch: 1 batch_num: 63 loss: 35.7327 acc: 0.71875\n",
      "epoch: 1 batch_num: 64 loss: 42.7921 acc: 0.734375\n",
      "epoch: 1 batch_num: 65 loss: 37.8818 acc: 0.75\n",
      "epoch: 1 batch_num: 66 loss: 35.0363 acc: 0.78125\n",
      "epoch: 1 batch_num: 67 loss: 38.4254 acc: 0.734375\n",
      "epoch: 1 batch_num: 68 loss: 31.2996 acc: 0.828125\n",
      "epoch: 1 batch_num: 69 loss: 36.2108 acc: 0.765625\n",
      "epoch: 1 batch_num: 70 loss: 41.8318 acc: 0.65625\n",
      "epoch: 1 batch_num: 71 loss: 39.191 acc: 0.765625\n",
      "epoch: 1 batch_num: 72 loss: 34.2573 acc: 0.78125\n",
      "epoch: 1 batch_num: 73 loss: 43.0491 acc: 0.671875\n",
      "epoch: 1 batch_num: 74 loss: 43.2799 acc: 0.65625\n",
      "epoch: 1 batch_num: 75 loss: 36.0779 acc: 0.703125\n",
      "epoch: 1 batch_num: 76 loss: 36.8211 acc: 0.6875\n",
      "epoch: 1 batch_num: 77 loss: 35.6467 acc: 0.75\n",
      "epoch: 1 batch_num: 78 loss: 37.869 acc: 0.671875\n",
      "epoch: 1 batch_num: 79 loss: 37.5616 acc: 0.734375\n",
      "epoch: 1 batch_num: 80 loss: 34.1075 acc: 0.78125\n",
      "epoch: 1 batch_num: 81 loss: 39.4905 acc: 0.640625\n",
      "epoch: 1 batch_num: 82 loss: 33.7755 acc: 0.640625\n",
      "epoch: 1 batch_num: 83 loss: 33.2324 acc: 0.8125\n",
      "epoch: 1 batch_num: 84 loss: 34.1503 acc: 0.765625\n",
      "epoch: 1 batch_num: 85 loss: 36.8844 acc: 0.75\n",
      "epoch: 1 batch_num: 86 loss: 34.5898 acc: 0.734375\n",
      "epoch: 1 batch_num: 87 loss: 32.2167 acc: 0.796875\n",
      "epoch: 1 batch_num: 88 loss: 34.8124 acc: 0.6875\n",
      "epoch: 1 batch_num: 89 loss: 31.8545 acc: 0.828125\n",
      "epoch: 1 batch_num: 90 loss: 36.5606 acc: 0.734375\n",
      "epoch: 1 batch_num: 91 loss: 35.8138 acc: 0.75\n",
      "epoch: 1 batch_num: 92 loss: 33.446 acc: 0.765625\n",
      "epoch: 1 batch_num: 93 loss: 32.1431 acc: 0.78125\n",
      "epoch: 1 batch_num: 94 loss: 36.4907 acc: 0.703125\n",
      "epoch: 1 batch_num: 95 loss: 33.8811 acc: 0.765625\n",
      "epoch: 1 batch_num: 96 loss: 51.1342 acc: 0.671875\n",
      "epoch: 1 batch_num: 97 loss: 38.9145 acc: 0.640625\n",
      "epoch: 1 batch_num: 98 loss: 27.4757 acc: 0.859375\n",
      "epoch: 1 batch_num: 99 loss: 30.5735 acc: 0.828125\n",
      "epoch: 1 batch_num: 100 loss: 27.9302 acc: 0.828125\n",
      "epoch: 1 batch_num: 101 loss: 35.0229 acc: 0.734375\n",
      "epoch: 1 batch_num: 102 loss: 40.2341 acc: 0.71875\n",
      "epoch: 1 batch_num: 103 loss: 34.2807 acc: 0.78125\n",
      "epoch: 1 batch_num: 104 loss: 30.7229 acc: 0.796875\n",
      "epoch: 1 batch_num: 105 loss: 36.7352 acc: 0.703125\n",
      "epoch: 1 batch_num: 106 loss: 37.7421 acc: 0.703125\n",
      "epoch: 1 batch_num: 107 loss: 35.766 acc: 0.71875\n",
      "epoch: 1 batch_num: 108 loss: 33.0645 acc: 0.75\n",
      "epoch: 1 batch_num: 109 loss: 34.8087 acc: 0.796875\n",
      "epoch: 1 batch_num: 110 loss: 36.448 acc: 0.796875\n",
      "epoch: 1 batch_num: 111 loss: 31.1301 acc: 0.8125\n",
      "epoch: 1 batch_num: 112 loss: 37.9571 acc: 0.734375\n",
      "epoch: 1 batch_num: 113 loss: 34.169 acc: 0.703125\n",
      "epoch: 1 batch_num: 114 loss: 34.9027 acc: 0.6875\n",
      "epoch: 1 batch_num: 115 loss: 35.7103 acc: 0.78125\n",
      "epoch: 1 batch_num: 116 loss: 15.188 acc: 0.7931034482758621\n",
      "epoch: 2 batch_num: 1 loss: 29.5732 acc: 0.8125\n",
      "epoch: 2 batch_num: 2 loss: 28.7817 acc: 0.8125\n",
      "epoch: 2 batch_num: 3 loss: 26.8159 acc: 0.875\n",
      "epoch: 2 batch_num: 4 loss: 28.9139 acc: 0.796875\n",
      "epoch: 2 batch_num: 5 loss: 24.6312 acc: 0.90625\n",
      "epoch: 2 batch_num: 6 loss: 26.1157 acc: 0.828125\n",
      "epoch: 2 batch_num: 7 loss: 28.1968 acc: 0.859375\n",
      "epoch: 2 batch_num: 8 loss: 27.5449 acc: 0.859375\n",
      "epoch: 2 batch_num: 9 loss: 26.1559 acc: 0.859375\n",
      "epoch: 2 batch_num: 10 loss: 24.2967 acc: 0.90625\n",
      "epoch: 2 batch_num: 11 loss: 24.7279 acc: 0.859375\n",
      "epoch: 2 batch_num: 12 loss: 27.9039 acc: 0.84375\n",
      "epoch: 2 batch_num: 13 loss: 26.9851 acc: 0.796875\n",
      "epoch: 2 batch_num: 14 loss: 33.4985 acc: 0.734375\n",
      "epoch: 2 batch_num: 15 loss: 26.3696 acc: 0.796875\n",
      "epoch: 2 batch_num: 16 loss: 26.8515 acc: 0.875\n",
      "epoch: 2 batch_num: 17 loss: 30.59 acc: 0.78125\n",
      "epoch: 2 batch_num: 18 loss: 26.6682 acc: 0.875\n",
      "epoch: 2 batch_num: 19 loss: 21.6239 acc: 0.90625\n",
      "epoch: 2 batch_num: 20 loss: 21.8363 acc: 0.90625\n",
      "epoch: 2 batch_num: 21 loss: 28.7321 acc: 0.828125\n",
      "epoch: 2 batch_num: 22 loss: 24.243 acc: 0.875\n",
      "epoch: 2 batch_num: 23 loss: 24.4214 acc: 0.859375\n",
      "epoch: 2 batch_num: 24 loss: 25.3818 acc: 0.859375\n",
      "epoch: 2 batch_num: 25 loss: 27.1764 acc: 0.84375\n",
      "epoch: 2 batch_num: 26 loss: 25.3235 acc: 0.828125\n",
      "epoch: 2 batch_num: 27 loss: 24.8739 acc: 0.875\n",
      "epoch: 2 batch_num: 28 loss: 28.0214 acc: 0.859375\n",
      "epoch: 2 batch_num: 29 loss: 26.3587 acc: 0.84375\n",
      "epoch: 2 batch_num: 30 loss: 23.5731 acc: 0.875\n",
      "epoch: 2 batch_num: 31 loss: 38.1093 acc: 0.6875\n",
      "epoch: 2 batch_num: 32 loss: 31.3703 acc: 0.8125\n",
      "epoch: 2 batch_num: 33 loss: 26.8735 acc: 0.84375\n",
      "epoch: 2 batch_num: 34 loss: 25.4251 acc: 0.859375\n",
      "epoch: 2 batch_num: 35 loss: 29.2856 acc: 0.828125\n",
      "epoch: 2 batch_num: 36 loss: 24.384 acc: 0.859375\n",
      "epoch: 2 batch_num: 37 loss: 30.9837 acc: 0.71875\n",
      "epoch: 2 batch_num: 38 loss: 21.4304 acc: 0.9375\n",
      "epoch: 2 batch_num: 39 loss: 26.0017 acc: 0.84375\n",
      "epoch: 2 batch_num: 40 loss: 20.6064 acc: 0.90625\n",
      "epoch: 2 batch_num: 41 loss: 28.7068 acc: 0.796875\n",
      "epoch: 2 batch_num: 42 loss: 25.4483 acc: 0.875\n",
      "epoch: 2 batch_num: 43 loss: 21.8249 acc: 0.828125\n",
      "epoch: 2 batch_num: 44 loss: 21.8631 acc: 0.90625\n",
      "epoch: 2 batch_num: 45 loss: 27.3647 acc: 0.828125\n",
      "epoch: 2 batch_num: 46 loss: 21.9663 acc: 0.859375\n",
      "epoch: 2 batch_num: 47 loss: 26.9034 acc: 0.8125\n",
      "epoch: 2 batch_num: 48 loss: 28.0356 acc: 0.765625\n",
      "epoch: 2 batch_num: 49 loss: 38.6627 acc: 0.78125\n",
      "epoch: 2 batch_num: 50 loss: 28.6124 acc: 0.8125\n",
      "epoch: 2 batch_num: 51 loss: 28.8305 acc: 0.796875\n",
      "epoch: 2 batch_num: 52 loss: 20.0153 acc: 0.890625\n",
      "epoch: 2 batch_num: 53 loss: 21.6318 acc: 0.859375\n",
      "epoch: 2 batch_num: 54 loss: 25.364 acc: 0.78125\n",
      "epoch: 2 batch_num: 55 loss: 30.2264 acc: 0.796875\n",
      "epoch: 2 batch_num: 56 loss: 26.433 acc: 0.796875\n",
      "epoch: 2 batch_num: 57 loss: 28.7886 acc: 0.875\n",
      "epoch: 2 batch_num: 58 loss: 23.0184 acc: 0.84375\n",
      "epoch: 2 batch_num: 59 loss: 20.3567 acc: 0.84375\n",
      "epoch: 2 batch_num: 60 loss: 25.5991 acc: 0.875\n",
      "epoch: 2 batch_num: 61 loss: 25.8305 acc: 0.84375\n",
      "epoch: 2 batch_num: 62 loss: 26.3194 acc: 0.828125\n",
      "epoch: 2 batch_num: 63 loss: 24.7989 acc: 0.8125\n",
      "epoch: 2 batch_num: 64 loss: 28.2573 acc: 0.8125\n",
      "epoch: 2 batch_num: 65 loss: 23.9342 acc: 0.859375\n",
      "epoch: 2 batch_num: 66 loss: 29.6489 acc: 0.734375\n",
      "epoch: 2 batch_num: 67 loss: 22.4042 acc: 0.90625\n",
      "epoch: 2 batch_num: 68 loss: 22.8833 acc: 0.859375\n",
      "epoch: 2 batch_num: 69 loss: 31.5148 acc: 0.859375\n",
      "epoch: 2 batch_num: 70 loss: 32.9318 acc: 0.796875\n",
      "epoch: 2 batch_num: 71 loss: 24.1134 acc: 0.875\n",
      "epoch: 2 batch_num: 72 loss: 26.8721 acc: 0.78125\n",
      "epoch: 2 batch_num: 73 loss: 28.5925 acc: 0.859375\n",
      "epoch: 2 batch_num: 74 loss: 30.7606 acc: 0.765625\n",
      "epoch: 2 batch_num: 75 loss: 25.5558 acc: 0.8125\n",
      "epoch: 2 batch_num: 76 loss: 21.6933 acc: 0.875\n",
      "epoch: 2 batch_num: 77 loss: 22.7821 acc: 0.890625\n",
      "epoch: 2 batch_num: 78 loss: 35.1498 acc: 0.75\n",
      "epoch: 2 batch_num: 79 loss: 26.0918 acc: 0.796875\n",
      "epoch: 2 batch_num: 80 loss: 24.4622 acc: 0.828125\n",
      "epoch: 2 batch_num: 81 loss: 31.6573 acc: 0.765625\n",
      "epoch: 2 batch_num: 82 loss: 30.4392 acc: 0.8125\n",
      "epoch: 2 batch_num: 83 loss: 21.2704 acc: 0.84375\n",
      "epoch: 2 batch_num: 84 loss: 27.5194 acc: 0.84375\n",
      "epoch: 2 batch_num: 85 loss: 26.9847 acc: 0.828125\n",
      "epoch: 2 batch_num: 86 loss: 31.1123 acc: 0.796875\n",
      "epoch: 2 batch_num: 87 loss: 24.7891 acc: 0.859375\n",
      "epoch: 2 batch_num: 88 loss: 24.4906 acc: 0.828125\n",
      "epoch: 2 batch_num: 89 loss: 24.0105 acc: 0.875\n",
      "epoch: 2 batch_num: 90 loss: 25.5051 acc: 0.84375\n",
      "epoch: 2 batch_num: 91 loss: 28.3597 acc: 0.84375\n",
      "epoch: 2 batch_num: 92 loss: 33.0017 acc: 0.8125\n",
      "epoch: 2 batch_num: 93 loss: 28.9821 acc: 0.765625\n",
      "epoch: 2 batch_num: 94 loss: 25.1255 acc: 0.859375\n",
      "epoch: 2 batch_num: 95 loss: 24.6091 acc: 0.84375\n",
      "epoch: 2 batch_num: 96 loss: 21.1948 acc: 0.875\n",
      "epoch: 2 batch_num: 97 loss: 28.1337 acc: 0.796875\n",
      "epoch: 2 batch_num: 98 loss: 23.105 acc: 0.84375\n",
      "epoch: 2 batch_num: 99 loss: 31.1748 acc: 0.75\n",
      "epoch: 2 batch_num: 100 loss: 33.1402 acc: 0.765625\n",
      "epoch: 2 batch_num: 101 loss: 29.658 acc: 0.796875\n",
      "epoch: 2 batch_num: 102 loss: 21.0555 acc: 0.890625\n",
      "epoch: 2 batch_num: 103 loss: 24.2129 acc: 0.828125\n",
      "epoch: 2 batch_num: 104 loss: 22.9718 acc: 0.828125\n",
      "epoch: 2 batch_num: 105 loss: 34.4175 acc: 0.6875\n",
      "epoch: 2 batch_num: 106 loss: 25.0905 acc: 0.828125\n",
      "epoch: 2 batch_num: 107 loss: 25.1051 acc: 0.859375\n",
      "epoch: 2 batch_num: 108 loss: 26.5476 acc: 0.796875\n",
      "epoch: 2 batch_num: 109 loss: 36.0734 acc: 0.734375\n",
      "epoch: 2 batch_num: 110 loss: 21.1928 acc: 0.9375\n",
      "epoch: 2 batch_num: 111 loss: 32.05 acc: 0.75\n",
      "epoch: 2 batch_num: 112 loss: 31.3013 acc: 0.78125\n",
      "epoch: 2 batch_num: 113 loss: 26.9753 acc: 0.796875\n",
      "epoch: 2 batch_num: 114 loss: 30.901 acc: 0.84375\n",
      "epoch: 2 batch_num: 115 loss: 24.5297 acc: 0.8125\n",
      "epoch: 2 batch_num: 116 loss: 12.6378 acc: 0.8275862068965517\n",
      "epoch: 3 batch_num: 1 loss: 19.041 acc: 0.890625\n",
      "epoch: 3 batch_num: 2 loss: 25.4769 acc: 0.84375\n",
      "epoch: 3 batch_num: 3 loss: 17.0951 acc: 0.9375\n",
      "epoch: 3 batch_num: 4 loss: 17.9635 acc: 0.921875\n",
      "epoch: 3 batch_num: 5 loss: 21.3364 acc: 0.84375\n",
      "epoch: 3 batch_num: 6 loss: 23.8629 acc: 0.8125\n",
      "epoch: 3 batch_num: 7 loss: 21.4709 acc: 0.828125\n",
      "epoch: 3 batch_num: 8 loss: 14.5209 acc: 0.9375\n",
      "epoch: 3 batch_num: 9 loss: 18.28 acc: 0.890625\n",
      "epoch: 3 batch_num: 10 loss: 22.6463 acc: 0.84375\n",
      "epoch: 3 batch_num: 11 loss: 20.4612 acc: 0.90625\n",
      "epoch: 3 batch_num: 12 loss: 15.2033 acc: 0.9375\n",
      "epoch: 3 batch_num: 13 loss: 13.7947 acc: 0.9375\n",
      "epoch: 3 batch_num: 14 loss: 19.5593 acc: 0.890625\n",
      "epoch: 3 batch_num: 15 loss: 18.6323 acc: 0.90625\n",
      "epoch: 3 batch_num: 16 loss: 18.2048 acc: 0.921875\n",
      "epoch: 3 batch_num: 17 loss: 22.8292 acc: 0.84375\n",
      "epoch: 3 batch_num: 18 loss: 19.2934 acc: 0.921875\n",
      "epoch: 3 batch_num: 19 loss: 10.9183 acc: 0.96875\n",
      "epoch: 3 batch_num: 20 loss: 23.1482 acc: 0.875\n",
      "epoch: 3 batch_num: 21 loss: 16.543 acc: 0.90625\n",
      "epoch: 3 batch_num: 22 loss: 15.7226 acc: 0.921875\n",
      "epoch: 3 batch_num: 23 loss: 22.1578 acc: 0.875\n",
      "epoch: 3 batch_num: 24 loss: 25.3653 acc: 0.890625\n",
      "epoch: 3 batch_num: 25 loss: 14.417 acc: 0.90625\n",
      "epoch: 3 batch_num: 26 loss: 29.4982 acc: 0.78125\n",
      "epoch: 3 batch_num: 27 loss: 18.882 acc: 0.875\n",
      "epoch: 3 batch_num: 28 loss: 20.3128 acc: 0.90625\n",
      "epoch: 3 batch_num: 29 loss: 18.398 acc: 0.890625\n",
      "epoch: 3 batch_num: 30 loss: 17.7963 acc: 0.90625\n",
      "epoch: 3 batch_num: 31 loss: 18.4466 acc: 0.890625\n",
      "epoch: 3 batch_num: 32 loss: 23.0595 acc: 0.859375\n",
      "epoch: 3 batch_num: 33 loss: 18.7218 acc: 0.921875\n",
      "epoch: 3 batch_num: 34 loss: 19.4696 acc: 0.890625\n",
      "epoch: 3 batch_num: 35 loss: 17.9447 acc: 0.921875\n",
      "epoch: 3 batch_num: 36 loss: 18.9115 acc: 0.875\n",
      "epoch: 3 batch_num: 37 loss: 19.9592 acc: 0.890625\n",
      "epoch: 3 batch_num: 38 loss: 15.5527 acc: 0.9375\n",
      "epoch: 3 batch_num: 39 loss: 22.4789 acc: 0.890625\n",
      "epoch: 3 batch_num: 40 loss: 17.9224 acc: 0.921875\n",
      "epoch: 3 batch_num: 41 loss: 18.8041 acc: 0.9375\n",
      "epoch: 3 batch_num: 42 loss: 22.4129 acc: 0.859375\n",
      "epoch: 3 batch_num: 43 loss: 17.0571 acc: 0.921875\n",
      "epoch: 3 batch_num: 44 loss: 21.776 acc: 0.890625\n",
      "epoch: 3 batch_num: 45 loss: 15.1718 acc: 0.921875\n",
      "epoch: 3 batch_num: 46 loss: 23.0782 acc: 0.875\n",
      "epoch: 3 batch_num: 47 loss: 15.623 acc: 0.921875\n",
      "epoch: 3 batch_num: 48 loss: 15.8137 acc: 0.9375\n",
      "epoch: 3 batch_num: 49 loss: 21.2508 acc: 0.8125\n",
      "epoch: 3 batch_num: 50 loss: 21.7174 acc: 0.875\n",
      "epoch: 3 batch_num: 51 loss: 14.1924 acc: 0.9375\n",
      "epoch: 3 batch_num: 52 loss: 13.8235 acc: 0.953125\n",
      "epoch: 3 batch_num: 53 loss: 13.0442 acc: 0.9375\n",
      "epoch: 3 batch_num: 54 loss: 13.1704 acc: 0.921875\n",
      "epoch: 3 batch_num: 55 loss: 16.0895 acc: 0.890625\n",
      "epoch: 3 batch_num: 56 loss: 18.2613 acc: 0.921875\n",
      "epoch: 3 batch_num: 57 loss: 14.9526 acc: 0.953125\n",
      "epoch: 3 batch_num: 58 loss: 15.5353 acc: 0.921875\n",
      "epoch: 3 batch_num: 59 loss: 24.1614 acc: 0.875\n",
      "epoch: 3 batch_num: 60 loss: 14.6456 acc: 0.9375\n",
      "epoch: 3 batch_num: 61 loss: 20.2067 acc: 0.84375\n",
      "epoch: 3 batch_num: 62 loss: 16.1798 acc: 0.90625\n",
      "epoch: 3 batch_num: 63 loss: 18.0632 acc: 0.875\n",
      "epoch: 3 batch_num: 64 loss: 20.4044 acc: 0.84375\n",
      "epoch: 3 batch_num: 65 loss: 16.998 acc: 0.890625\n",
      "epoch: 3 batch_num: 66 loss: 15.7728 acc: 0.875\n",
      "epoch: 3 batch_num: 67 loss: 17.5183 acc: 0.90625\n",
      "epoch: 3 batch_num: 68 loss: 20.1747 acc: 0.890625\n",
      "epoch: 3 batch_num: 69 loss: 19.3604 acc: 0.84375\n",
      "epoch: 3 batch_num: 70 loss: 24.2238 acc: 0.8125\n",
      "epoch: 3 batch_num: 71 loss: 17.4344 acc: 0.875\n",
      "epoch: 3 batch_num: 72 loss: 16.3354 acc: 0.921875\n",
      "epoch: 3 batch_num: 73 loss: 13.9719 acc: 0.921875\n",
      "epoch: 3 batch_num: 74 loss: 16.6307 acc: 0.90625\n",
      "epoch: 3 batch_num: 75 loss: 14.2645 acc: 0.9375\n",
      "epoch: 3 batch_num: 76 loss: 17.0102 acc: 0.890625\n",
      "epoch: 3 batch_num: 77 loss: 22.6319 acc: 0.84375\n",
      "epoch: 3 batch_num: 78 loss: 19.7048 acc: 0.84375\n",
      "epoch: 3 batch_num: 79 loss: 12.415 acc: 0.96875\n",
      "epoch: 3 batch_num: 80 loss: 16.9136 acc: 0.921875\n",
      "epoch: 3 batch_num: 81 loss: 17.8146 acc: 0.890625\n",
      "epoch: 3 batch_num: 82 loss: 18.0179 acc: 0.890625\n",
      "epoch: 3 batch_num: 83 loss: 18.3289 acc: 0.875\n",
      "epoch: 3 batch_num: 84 loss: 23.9883 acc: 0.859375\n",
      "epoch: 3 batch_num: 85 loss: 25.0283 acc: 0.828125\n",
      "epoch: 3 batch_num: 86 loss: 13.6794 acc: 0.953125\n",
      "epoch: 3 batch_num: 87 loss: 20.8393 acc: 0.875\n",
      "epoch: 3 batch_num: 88 loss: 31.9649 acc: 0.765625\n",
      "epoch: 3 batch_num: 89 loss: 12.5321 acc: 0.96875\n",
      "epoch: 3 batch_num: 90 loss: 21.0837 acc: 0.890625\n",
      "epoch: 3 batch_num: 91 loss: 20.7238 acc: 0.875\n",
      "epoch: 3 batch_num: 92 loss: 27.1462 acc: 0.84375\n",
      "epoch: 3 batch_num: 93 loss: 13.8065 acc: 0.90625\n",
      "epoch: 3 batch_num: 94 loss: 10.5607 acc: 0.953125\n",
      "epoch: 3 batch_num: 95 loss: 16.4321 acc: 0.859375\n",
      "epoch: 3 batch_num: 96 loss: 23.5008 acc: 0.796875\n",
      "epoch: 3 batch_num: 97 loss: 18.9416 acc: 0.890625\n",
      "epoch: 3 batch_num: 98 loss: 28.0756 acc: 0.78125\n",
      "epoch: 3 batch_num: 99 loss: 18.4044 acc: 0.875\n",
      "epoch: 3 batch_num: 100 loss: 20.9665 acc: 0.859375\n",
      "epoch: 3 batch_num: 101 loss: 28.6046 acc: 0.8125\n",
      "epoch: 3 batch_num: 102 loss: 22.1466 acc: 0.859375\n",
      "epoch: 3 batch_num: 103 loss: 26.3066 acc: 0.875\n",
      "epoch: 3 batch_num: 104 loss: 24.514 acc: 0.859375\n",
      "epoch: 3 batch_num: 105 loss: 27.7211 acc: 0.8125\n",
      "epoch: 3 batch_num: 106 loss: 21.4959 acc: 0.84375\n",
      "epoch: 3 batch_num: 107 loss: 15.9941 acc: 0.921875\n",
      "epoch: 3 batch_num: 108 loss: 18.6853 acc: 0.859375\n",
      "epoch: 3 batch_num: 109 loss: 18.6917 acc: 0.90625\n",
      "epoch: 3 batch_num: 110 loss: 17.2941 acc: 0.84375\n",
      "epoch: 3 batch_num: 111 loss: 22.1625 acc: 0.859375\n",
      "epoch: 3 batch_num: 112 loss: 20.3478 acc: 0.859375\n",
      "epoch: 3 batch_num: 113 loss: 22.2468 acc: 0.859375\n",
      "epoch: 3 batch_num: 114 loss: 23.6197 acc: 0.828125\n",
      "epoch: 3 batch_num: 115 loss: 21.9522 acc: 0.875\n",
      "epoch: 3 batch_num: 116 loss: 8.8734 acc: 0.896551724137931\n",
      "epoch: 4 batch_num: 1 loss: 12.7488 acc: 0.953125\n",
      "epoch: 4 batch_num: 2 loss: 12.8374 acc: 0.9375\n",
      "epoch: 4 batch_num: 3 loss: 12.0953 acc: 0.9375\n",
      "epoch: 4 batch_num: 4 loss: 12.7637 acc: 0.9375\n",
      "epoch: 4 batch_num: 5 loss: 8.957 acc: 1.0\n",
      "epoch: 4 batch_num: 6 loss: 11.2815 acc: 0.921875\n",
      "epoch: 4 batch_num: 7 loss: 15.0778 acc: 0.9375\n",
      "epoch: 4 batch_num: 8 loss: 10.4717 acc: 0.984375\n",
      "epoch: 4 batch_num: 9 loss: 13.5684 acc: 0.9375\n",
      "epoch: 4 batch_num: 10 loss: 14.3238 acc: 0.921875\n",
      "epoch: 4 batch_num: 11 loss: 14.8101 acc: 0.953125\n",
      "epoch: 4 batch_num: 12 loss: 11.1442 acc: 1.0\n",
      "epoch: 4 batch_num: 13 loss: 15.525 acc: 0.890625\n",
      "epoch: 4 batch_num: 14 loss: 12.4186 acc: 0.96875\n",
      "epoch: 4 batch_num: 15 loss: 8.6081 acc: 1.0\n",
      "epoch: 4 batch_num: 16 loss: 10.9055 acc: 0.96875\n",
      "epoch: 4 batch_num: 17 loss: 12.5953 acc: 0.953125\n",
      "epoch: 4 batch_num: 18 loss: 10.2023 acc: 0.953125\n",
      "epoch: 4 batch_num: 19 loss: 16.6472 acc: 0.875\n",
      "epoch: 4 batch_num: 20 loss: 13.6687 acc: 0.9375\n",
      "epoch: 4 batch_num: 21 loss: 16.2595 acc: 0.875\n",
      "epoch: 4 batch_num: 22 loss: 12.4446 acc: 0.921875\n",
      "epoch: 4 batch_num: 23 loss: 14.821 acc: 0.90625\n",
      "epoch: 4 batch_num: 24 loss: 10.3693 acc: 0.953125\n",
      "epoch: 4 batch_num: 25 loss: 15.2744 acc: 0.890625\n",
      "epoch: 4 batch_num: 26 loss: 13.7821 acc: 0.90625\n",
      "epoch: 4 batch_num: 27 loss: 13.2725 acc: 0.9375\n",
      "epoch: 4 batch_num: 28 loss: 12.1523 acc: 0.984375\n",
      "epoch: 4 batch_num: 29 loss: 11.7897 acc: 0.953125\n",
      "epoch: 4 batch_num: 30 loss: 9.6004 acc: 0.984375\n",
      "epoch: 4 batch_num: 31 loss: 11.157 acc: 0.9375\n",
      "epoch: 4 batch_num: 32 loss: 13.904 acc: 0.9375\n",
      "epoch: 4 batch_num: 33 loss: 10.2927 acc: 0.984375\n",
      "epoch: 4 batch_num: 34 loss: 16.5255 acc: 0.90625\n",
      "epoch: 4 batch_num: 35 loss: 12.4072 acc: 0.90625\n",
      "epoch: 4 batch_num: 36 loss: 12.414 acc: 0.9375\n",
      "epoch: 4 batch_num: 37 loss: 19.3339 acc: 0.875\n",
      "epoch: 4 batch_num: 38 loss: 10.0609 acc: 0.96875\n",
      "epoch: 4 batch_num: 39 loss: 18.243 acc: 0.9375\n",
      "epoch: 4 batch_num: 40 loss: 12.3664 acc: 0.953125\n",
      "epoch: 4 batch_num: 41 loss: 15.441 acc: 0.9375\n",
      "epoch: 4 batch_num: 42 loss: 11.511 acc: 0.953125\n",
      "epoch: 4 batch_num: 43 loss: 13.891 acc: 0.9375\n",
      "epoch: 4 batch_num: 44 loss: 14.4626 acc: 0.90625\n",
      "epoch: 4 batch_num: 45 loss: 17.6433 acc: 0.953125\n",
      "epoch: 4 batch_num: 46 loss: 18.4793 acc: 0.875\n",
      "epoch: 4 batch_num: 47 loss: 13.003 acc: 0.9375\n",
      "epoch: 4 batch_num: 48 loss: 14.4385 acc: 0.9375\n",
      "epoch: 4 batch_num: 49 loss: 13.3519 acc: 0.953125\n",
      "epoch: 4 batch_num: 50 loss: 10.7549 acc: 0.96875\n",
      "epoch: 4 batch_num: 51 loss: 8.2208 acc: 0.96875\n",
      "epoch: 4 batch_num: 52 loss: 9.5689 acc: 0.953125\n",
      "epoch: 4 batch_num: 53 loss: 18.3064 acc: 0.859375\n",
      "epoch: 4 batch_num: 54 loss: 13.5943 acc: 0.90625\n",
      "epoch: 4 batch_num: 55 loss: 9.8926 acc: 0.953125\n",
      "epoch: 4 batch_num: 56 loss: 13.1896 acc: 0.90625\n",
      "epoch: 4 batch_num: 57 loss: 11.0336 acc: 0.9375\n",
      "epoch: 4 batch_num: 58 loss: 13.222 acc: 0.921875\n",
      "epoch: 4 batch_num: 59 loss: 14.0546 acc: 0.9375\n",
      "epoch: 4 batch_num: 60 loss: 14.8194 acc: 0.90625\n",
      "epoch: 4 batch_num: 61 loss: 14.8012 acc: 0.90625\n",
      "epoch: 4 batch_num: 62 loss: 8.353 acc: 0.984375\n",
      "epoch: 4 batch_num: 63 loss: 12.9026 acc: 0.90625\n",
      "epoch: 4 batch_num: 64 loss: 13.9753 acc: 0.890625\n",
      "epoch: 4 batch_num: 65 loss: 10.0677 acc: 0.96875\n",
      "epoch: 4 batch_num: 66 loss: 18.9698 acc: 0.90625\n",
      "epoch: 4 batch_num: 67 loss: 14.7032 acc: 0.9375\n",
      "epoch: 4 batch_num: 68 loss: 15.1716 acc: 0.875\n",
      "epoch: 4 batch_num: 69 loss: 12.2329 acc: 0.953125\n",
      "epoch: 4 batch_num: 70 loss: 11.0729 acc: 0.953125\n",
      "epoch: 4 batch_num: 71 loss: 14.9166 acc: 0.890625\n",
      "epoch: 4 batch_num: 72 loss: 11.0396 acc: 0.953125\n",
      "epoch: 4 batch_num: 73 loss: 11.3786 acc: 0.953125\n",
      "epoch: 4 batch_num: 74 loss: 15.5543 acc: 0.890625\n",
      "epoch: 4 batch_num: 75 loss: 16.9359 acc: 0.90625\n",
      "epoch: 4 batch_num: 76 loss: 11.6266 acc: 0.953125\n",
      "epoch: 4 batch_num: 77 loss: 25.5194 acc: 0.8125\n",
      "epoch: 4 batch_num: 78 loss: 9.9863 acc: 0.9375\n",
      "epoch: 4 batch_num: 79 loss: 12.4538 acc: 0.953125\n",
      "epoch: 4 batch_num: 80 loss: 15.9896 acc: 0.9375\n",
      "epoch: 4 batch_num: 81 loss: 17.6355 acc: 0.90625\n",
      "epoch: 4 batch_num: 82 loss: 11.4162 acc: 0.953125\n",
      "epoch: 4 batch_num: 83 loss: 12.2 acc: 0.9375\n",
      "epoch: 4 batch_num: 84 loss: 11.8299 acc: 0.9375\n",
      "epoch: 4 batch_num: 85 loss: 12.9096 acc: 0.9375\n",
      "epoch: 4 batch_num: 86 loss: 15.4188 acc: 0.90625\n",
      "epoch: 4 batch_num: 87 loss: 13.7952 acc: 0.9375\n",
      "epoch: 4 batch_num: 88 loss: 10.7528 acc: 0.953125\n",
      "epoch: 4 batch_num: 89 loss: 16.3604 acc: 0.875\n",
      "epoch: 4 batch_num: 90 loss: 12.8641 acc: 0.921875\n",
      "epoch: 4 batch_num: 91 loss: 12.0117 acc: 0.90625\n",
      "epoch: 4 batch_num: 92 loss: 13.926 acc: 0.953125\n",
      "epoch: 4 batch_num: 93 loss: 9.7818 acc: 0.953125\n",
      "epoch: 4 batch_num: 94 loss: 15.2423 acc: 0.890625\n",
      "epoch: 4 batch_num: 95 loss: 12.6145 acc: 0.9375\n",
      "epoch: 4 batch_num: 96 loss: 15.4971 acc: 0.90625\n",
      "epoch: 4 batch_num: 97 loss: 16.609 acc: 0.875\n",
      "epoch: 4 batch_num: 98 loss: 13.4089 acc: 0.9375\n",
      "epoch: 4 batch_num: 99 loss: 8.8614 acc: 0.953125\n",
      "epoch: 4 batch_num: 100 loss: 13.0223 acc: 0.890625\n",
      "epoch: 4 batch_num: 101 loss: 20.2964 acc: 0.84375\n",
      "epoch: 4 batch_num: 102 loss: 12.9778 acc: 0.921875\n",
      "epoch: 4 batch_num: 103 loss: 16.9726 acc: 0.859375\n",
      "epoch: 4 batch_num: 104 loss: 10.2039 acc: 0.9375\n",
      "epoch: 4 batch_num: 105 loss: 13.811 acc: 0.921875\n",
      "epoch: 4 batch_num: 106 loss: 16.5108 acc: 0.890625\n",
      "epoch: 4 batch_num: 107 loss: 11.7025 acc: 0.953125\n",
      "epoch: 4 batch_num: 108 loss: 19.1073 acc: 0.921875\n",
      "epoch: 4 batch_num: 109 loss: 17.6422 acc: 0.875\n",
      "epoch: 4 batch_num: 110 loss: 13.9475 acc: 0.921875\n",
      "epoch: 4 batch_num: 111 loss: 12.5585 acc: 0.953125\n",
      "epoch: 4 batch_num: 112 loss: 15.0607 acc: 0.90625\n",
      "epoch: 4 batch_num: 113 loss: 11.4284 acc: 0.96875\n",
      "epoch: 4 batch_num: 114 loss: 15.1429 acc: 0.921875\n",
      "epoch: 4 batch_num: 115 loss: 19.6347 acc: 0.859375\n",
      "epoch: 4 batch_num: 116 loss: 7.0963 acc: 0.896551724137931\n",
      "epoch: 5 batch_num: 1 loss: 13.6724 acc: 0.9375\n",
      "epoch: 5 batch_num: 2 loss: 8.0356 acc: 0.96875\n",
      "epoch: 5 batch_num: 3 loss: 8.6604 acc: 0.953125\n",
      "epoch: 5 batch_num: 4 loss: 9.8128 acc: 0.984375\n",
      "epoch: 5 batch_num: 5 loss: 5.5737 acc: 0.984375\n",
      "epoch: 5 batch_num: 6 loss: 8.1863 acc: 0.96875\n",
      "epoch: 5 batch_num: 7 loss: 11.3505 acc: 0.96875\n",
      "epoch: 5 batch_num: 8 loss: 8.2429 acc: 0.953125\n",
      "epoch: 5 batch_num: 9 loss: 10.6096 acc: 0.921875\n",
      "epoch: 5 batch_num: 10 loss: 7.8674 acc: 0.96875\n",
      "epoch: 5 batch_num: 11 loss: 7.6888 acc: 0.984375\n",
      "epoch: 5 batch_num: 12 loss: 10.1672 acc: 0.96875\n",
      "epoch: 5 batch_num: 13 loss: 10.4936 acc: 0.953125\n",
      "epoch: 5 batch_num: 14 loss: 11.5647 acc: 0.9375\n",
      "epoch: 5 batch_num: 15 loss: 7.7652 acc: 0.984375\n",
      "epoch: 5 batch_num: 16 loss: 11.803 acc: 0.9375\n",
      "epoch: 5 batch_num: 17 loss: 7.2991 acc: 0.984375\n",
      "epoch: 5 batch_num: 18 loss: 9.1743 acc: 0.953125\n",
      "epoch: 5 batch_num: 19 loss: 7.5099 acc: 0.953125\n",
      "epoch: 5 batch_num: 20 loss: 14.3908 acc: 0.890625\n",
      "epoch: 5 batch_num: 21 loss: 7.6001 acc: 0.96875\n",
      "epoch: 5 batch_num: 22 loss: 6.6022 acc: 0.984375\n",
      "epoch: 5 batch_num: 23 loss: 11.1216 acc: 0.921875\n",
      "epoch: 5 batch_num: 24 loss: 10.0138 acc: 0.96875\n",
      "epoch: 5 batch_num: 25 loss: 8.2481 acc: 0.953125\n",
      "epoch: 5 batch_num: 26 loss: 6.8135 acc: 0.984375\n",
      "epoch: 5 batch_num: 27 loss: 7.3716 acc: 0.984375\n",
      "epoch: 5 batch_num: 28 loss: 10.2074 acc: 0.984375\n",
      "epoch: 5 batch_num: 29 loss: 8.3039 acc: 0.96875\n",
      "epoch: 5 batch_num: 30 loss: 7.2121 acc: 1.0\n",
      "epoch: 5 batch_num: 31 loss: 5.4964 acc: 1.0\n",
      "epoch: 5 batch_num: 32 loss: 9.3697 acc: 0.921875\n",
      "epoch: 5 batch_num: 33 loss: 8.4726 acc: 0.953125\n",
      "epoch: 5 batch_num: 34 loss: 5.3131 acc: 0.984375\n",
      "epoch: 5 batch_num: 35 loss: 7.1509 acc: 0.984375\n",
      "epoch: 5 batch_num: 36 loss: 7.4387 acc: 0.96875\n",
      "epoch: 5 batch_num: 37 loss: 8.196 acc: 0.953125\n",
      "epoch: 5 batch_num: 38 loss: 10.6105 acc: 0.9375\n",
      "epoch: 5 batch_num: 39 loss: 8.1177 acc: 0.984375\n",
      "epoch: 5 batch_num: 40 loss: 5.7836 acc: 1.0\n",
      "epoch: 5 batch_num: 41 loss: 15.0174 acc: 0.9375\n",
      "epoch: 5 batch_num: 42 loss: 8.199 acc: 0.96875\n",
      "epoch: 5 batch_num: 43 loss: 8.0553 acc: 1.0\n",
      "epoch: 5 batch_num: 44 loss: 9.7854 acc: 0.953125\n",
      "epoch: 5 batch_num: 45 loss: 11.8274 acc: 0.96875\n",
      "epoch: 5 batch_num: 46 loss: 11.0757 acc: 0.921875\n",
      "epoch: 5 batch_num: 47 loss: 9.7601 acc: 0.96875\n",
      "epoch: 5 batch_num: 48 loss: 11.2784 acc: 0.90625\n",
      "epoch: 5 batch_num: 49 loss: 11.1513 acc: 0.953125\n",
      "epoch: 5 batch_num: 50 loss: 8.0947 acc: 0.984375\n",
      "epoch: 5 batch_num: 51 loss: 8.2739 acc: 0.984375\n",
      "epoch: 5 batch_num: 52 loss: 7.6477 acc: 0.984375\n",
      "epoch: 5 batch_num: 53 loss: 7.8937 acc: 0.96875\n",
      "epoch: 5 batch_num: 54 loss: 7.2152 acc: 0.984375\n",
      "epoch: 5 batch_num: 55 loss: 5.4454 acc: 1.0\n",
      "epoch: 5 batch_num: 56 loss: 7.9584 acc: 0.984375\n",
      "epoch: 5 batch_num: 57 loss: 5.921 acc: 1.0\n",
      "epoch: 5 batch_num: 58 loss: 9.58 acc: 0.953125\n",
      "epoch: 5 batch_num: 59 loss: 8.7775 acc: 0.953125\n",
      "epoch: 5 batch_num: 60 loss: 7.0127 acc: 0.96875\n",
      "epoch: 5 batch_num: 61 loss: 9.4081 acc: 0.953125\n",
      "epoch: 5 batch_num: 62 loss: 6.8043 acc: 0.96875\n",
      "epoch: 5 batch_num: 63 loss: 9.8926 acc: 0.984375\n",
      "epoch: 5 batch_num: 64 loss: 9.9002 acc: 0.953125\n",
      "epoch: 5 batch_num: 65 loss: 8.9139 acc: 0.984375\n",
      "epoch: 5 batch_num: 66 loss: 4.9805 acc: 1.0\n",
      "epoch: 5 batch_num: 67 loss: 7.3101 acc: 0.96875\n",
      "epoch: 5 batch_num: 68 loss: 15.1328 acc: 0.90625\n",
      "epoch: 5 batch_num: 69 loss: 12.4465 acc: 0.9375\n",
      "epoch: 5 batch_num: 70 loss: 5.2928 acc: 0.984375\n",
      "epoch: 5 batch_num: 71 loss: 6.0096 acc: 1.0\n",
      "epoch: 5 batch_num: 72 loss: 4.76 acc: 1.0\n",
      "epoch: 5 batch_num: 73 loss: 14.7348 acc: 0.90625\n",
      "epoch: 5 batch_num: 74 loss: 10.1836 acc: 0.96875\n",
      "epoch: 5 batch_num: 75 loss: 10.6983 acc: 0.953125\n",
      "epoch: 5 batch_num: 76 loss: 9.4974 acc: 0.96875\n",
      "epoch: 5 batch_num: 77 loss: 4.694 acc: 1.0\n",
      "epoch: 5 batch_num: 78 loss: 13.3535 acc: 0.921875\n",
      "epoch: 5 batch_num: 79 loss: 12.2021 acc: 0.9375\n",
      "epoch: 5 batch_num: 80 loss: 6.3103 acc: 0.984375\n",
      "epoch: 5 batch_num: 81 loss: 11.6385 acc: 0.953125\n",
      "epoch: 5 batch_num: 82 loss: 12.6858 acc: 0.953125\n",
      "epoch: 5 batch_num: 83 loss: 9.8193 acc: 0.953125\n",
      "epoch: 5 batch_num: 84 loss: 13.2304 acc: 0.953125\n",
      "epoch: 5 batch_num: 85 loss: 9.9321 acc: 0.9375\n",
      "epoch: 5 batch_num: 86 loss: 11.2744 acc: 0.9375\n",
      "epoch: 5 batch_num: 87 loss: 8.2959 acc: 0.96875\n",
      "epoch: 5 batch_num: 88 loss: 11.3028 acc: 0.953125\n",
      "epoch: 5 batch_num: 89 loss: 6.5971 acc: 0.984375\n",
      "epoch: 5 batch_num: 90 loss: 7.8345 acc: 0.953125\n",
      "epoch: 5 batch_num: 91 loss: 8.2007 acc: 0.953125\n",
      "epoch: 5 batch_num: 92 loss: 8.6374 acc: 0.953125\n",
      "epoch: 5 batch_num: 93 loss: 11.7575 acc: 0.96875\n",
      "epoch: 5 batch_num: 94 loss: 7.137 acc: 0.984375\n",
      "epoch: 5 batch_num: 95 loss: 8.6159 acc: 0.984375\n",
      "epoch: 5 batch_num: 96 loss: 10.7697 acc: 0.953125\n",
      "epoch: 5 batch_num: 97 loss: 9.0566 acc: 0.953125\n",
      "epoch: 5 batch_num: 98 loss: 9.372 acc: 0.96875\n",
      "epoch: 5 batch_num: 99 loss: 9.5112 acc: 0.96875\n",
      "epoch: 5 batch_num: 100 loss: 11.4678 acc: 0.953125\n",
      "epoch: 5 batch_num: 101 loss: 12.6451 acc: 0.9375\n",
      "epoch: 5 batch_num: 102 loss: 10.0934 acc: 0.953125\n",
      "epoch: 5 batch_num: 103 loss: 10.5109 acc: 0.96875\n",
      "epoch: 5 batch_num: 104 loss: 10.5292 acc: 0.9375\n",
      "epoch: 5 batch_num: 105 loss: 9.5475 acc: 0.9375\n",
      "epoch: 5 batch_num: 106 loss: 11.2331 acc: 0.953125\n",
      "epoch: 5 batch_num: 107 loss: 11.2412 acc: 0.921875\n",
      "epoch: 5 batch_num: 108 loss: 15.3901 acc: 0.921875\n",
      "epoch: 5 batch_num: 109 loss: 6.0174 acc: 0.96875\n",
      "epoch: 5 batch_num: 110 loss: 11.5187 acc: 0.953125\n",
      "epoch: 5 batch_num: 111 loss: 11.6023 acc: 0.921875\n",
      "epoch: 5 batch_num: 112 loss: 16.179 acc: 0.90625\n",
      "epoch: 5 batch_num: 113 loss: 8.8016 acc: 0.953125\n",
      "epoch: 5 batch_num: 114 loss: 8.0312 acc: 0.96875\n",
      "epoch: 5 batch_num: 115 loss: 9.8093 acc: 0.9375\n",
      "epoch: 5 batch_num: 116 loss: 5.4367 acc: 0.896551724137931\n",
      "epoch: 6 batch_num: 1 loss: 5.145 acc: 1.0\n",
      "epoch: 6 batch_num: 2 loss: 8.5208 acc: 0.96875\n",
      "epoch: 6 batch_num: 3 loss: 5.8229 acc: 0.984375\n",
      "epoch: 6 batch_num: 4 loss: 8.0308 acc: 0.96875\n",
      "epoch: 6 batch_num: 5 loss: 6.4764 acc: 0.984375\n",
      "epoch: 6 batch_num: 6 loss: 9.2804 acc: 0.953125\n",
      "epoch: 6 batch_num: 7 loss: 6.6525 acc: 0.96875\n",
      "epoch: 6 batch_num: 8 loss: 5.3353 acc: 0.984375\n",
      "epoch: 6 batch_num: 9 loss: 2.9825 acc: 1.0\n",
      "epoch: 6 batch_num: 10 loss: 6.3677 acc: 0.96875\n",
      "epoch: 6 batch_num: 11 loss: 3.4539 acc: 1.0\n",
      "epoch: 6 batch_num: 12 loss: 4.988 acc: 1.0\n",
      "epoch: 6 batch_num: 13 loss: 4.8919 acc: 0.984375\n",
      "epoch: 6 batch_num: 14 loss: 4.2081 acc: 1.0\n",
      "epoch: 6 batch_num: 15 loss: 2.7923 acc: 1.0\n",
      "epoch: 6 batch_num: 16 loss: 6.7848 acc: 0.953125\n",
      "epoch: 6 batch_num: 17 loss: 5.2103 acc: 1.0\n",
      "epoch: 6 batch_num: 18 loss: 6.4444 acc: 0.984375\n",
      "epoch: 6 batch_num: 19 loss: 5.8022 acc: 0.96875\n",
      "epoch: 6 batch_num: 20 loss: 4.7743 acc: 1.0\n",
      "epoch: 6 batch_num: 21 loss: 6.9163 acc: 0.96875\n",
      "epoch: 6 batch_num: 22 loss: 5.2635 acc: 1.0\n",
      "epoch: 6 batch_num: 23 loss: 5.6066 acc: 0.984375\n",
      "epoch: 6 batch_num: 24 loss: 3.7481 acc: 1.0\n",
      "epoch: 6 batch_num: 25 loss: 7.5179 acc: 0.96875\n",
      "epoch: 6 batch_num: 26 loss: 8.4471 acc: 0.96875\n",
      "epoch: 6 batch_num: 27 loss: 8.0554 acc: 0.96875\n",
      "epoch: 6 batch_num: 28 loss: 4.6717 acc: 0.984375\n",
      "epoch: 6 batch_num: 29 loss: 9.304 acc: 0.96875\n",
      "epoch: 6 batch_num: 30 loss: 4.5304 acc: 0.984375\n",
      "epoch: 6 batch_num: 31 loss: 6.8163 acc: 0.96875\n",
      "epoch: 6 batch_num: 32 loss: 7.3802 acc: 0.953125\n",
      "epoch: 6 batch_num: 33 loss: 5.364 acc: 0.984375\n",
      "epoch: 6 batch_num: 34 loss: 6.2454 acc: 0.984375\n",
      "epoch: 6 batch_num: 35 loss: 5.0046 acc: 0.96875\n",
      "epoch: 6 batch_num: 36 loss: 5.5382 acc: 0.984375\n",
      "epoch: 6 batch_num: 37 loss: 4.2379 acc: 1.0\n",
      "epoch: 6 batch_num: 38 loss: 4.5347 acc: 0.96875\n",
      "epoch: 6 batch_num: 39 loss: 4.5206 acc: 1.0\n",
      "epoch: 6 batch_num: 40 loss: 6.0723 acc: 0.984375\n",
      "epoch: 6 batch_num: 41 loss: 5.8838 acc: 1.0\n",
      "epoch: 6 batch_num: 42 loss: 8.9915 acc: 0.953125\n",
      "epoch: 6 batch_num: 43 loss: 8.0362 acc: 0.953125\n",
      "epoch: 6 batch_num: 44 loss: 7.2835 acc: 0.953125\n",
      "epoch: 6 batch_num: 45 loss: 5.0303 acc: 0.96875\n",
      "epoch: 6 batch_num: 46 loss: 3.4411 acc: 1.0\n",
      "epoch: 6 batch_num: 47 loss: 3.3466 acc: 0.984375\n",
      "epoch: 6 batch_num: 48 loss: 7.5844 acc: 0.96875\n",
      "epoch: 6 batch_num: 49 loss: 11.5395 acc: 0.921875\n",
      "epoch: 6 batch_num: 50 loss: 6.4876 acc: 0.953125\n",
      "epoch: 6 batch_num: 51 loss: 8.1073 acc: 0.96875\n",
      "epoch: 6 batch_num: 52 loss: 6.8655 acc: 0.96875\n",
      "epoch: 6 batch_num: 53 loss: 5.175 acc: 1.0\n",
      "epoch: 6 batch_num: 54 loss: 9.1488 acc: 0.921875\n",
      "epoch: 6 batch_num: 55 loss: 9.4063 acc: 0.953125\n",
      "epoch: 6 batch_num: 56 loss: 6.8182 acc: 0.984375\n",
      "epoch: 6 batch_num: 57 loss: 8.9841 acc: 0.96875\n",
      "epoch: 6 batch_num: 58 loss: 5.7915 acc: 0.984375\n",
      "epoch: 6 batch_num: 59 loss: 5.8265 acc: 1.0\n",
      "epoch: 6 batch_num: 60 loss: 6.904 acc: 0.984375\n",
      "epoch: 6 batch_num: 61 loss: 5.2711 acc: 0.96875\n",
      "epoch: 6 batch_num: 62 loss: 4.1651 acc: 1.0\n",
      "epoch: 6 batch_num: 63 loss: 4.2878 acc: 1.0\n",
      "epoch: 6 batch_num: 64 loss: 4.241 acc: 0.984375\n",
      "epoch: 6 batch_num: 65 loss: 4.811 acc: 0.984375\n",
      "epoch: 6 batch_num: 66 loss: 4.7496 acc: 1.0\n",
      "epoch: 6 batch_num: 67 loss: 12.0736 acc: 0.96875\n",
      "epoch: 6 batch_num: 68 loss: 6.969 acc: 0.96875\n",
      "epoch: 6 batch_num: 69 loss: 7.815 acc: 0.984375\n",
      "epoch: 6 batch_num: 70 loss: 5.8231 acc: 0.984375\n",
      "epoch: 6 batch_num: 71 loss: 9.7128 acc: 0.953125\n",
      "epoch: 6 batch_num: 72 loss: 4.1805 acc: 0.984375\n",
      "epoch: 6 batch_num: 73 loss: 11.3605 acc: 0.953125\n",
      "epoch: 6 batch_num: 74 loss: 7.1538 acc: 0.96875\n",
      "epoch: 6 batch_num: 75 loss: 5.8368 acc: 0.984375\n",
      "epoch: 6 batch_num: 76 loss: 9.3359 acc: 0.96875\n",
      "epoch: 6 batch_num: 77 loss: 6.2059 acc: 0.96875\n",
      "epoch: 6 batch_num: 78 loss: 3.3758 acc: 1.0\n",
      "epoch: 6 batch_num: 79 loss: 3.7843 acc: 0.984375\n",
      "epoch: 6 batch_num: 80 loss: 5.0176 acc: 0.984375\n",
      "epoch: 6 batch_num: 81 loss: 7.8214 acc: 0.953125\n",
      "epoch: 6 batch_num: 82 loss: 3.7076 acc: 1.0\n",
      "epoch: 6 batch_num: 83 loss: 7.1354 acc: 0.953125\n",
      "epoch: 6 batch_num: 84 loss: 3.6997 acc: 1.0\n",
      "epoch: 6 batch_num: 85 loss: 8.9828 acc: 0.96875\n",
      "epoch: 6 batch_num: 86 loss: 3.6719 acc: 0.984375\n",
      "epoch: 6 batch_num: 87 loss: 4.629 acc: 1.0\n",
      "epoch: 6 batch_num: 88 loss: 4.2222 acc: 1.0\n",
      "epoch: 6 batch_num: 89 loss: 4.6074 acc: 1.0\n",
      "epoch: 6 batch_num: 90 loss: 6.2757 acc: 0.984375\n",
      "epoch: 6 batch_num: 91 loss: 7.8876 acc: 0.953125\n",
      "epoch: 6 batch_num: 92 loss: 8.8099 acc: 0.96875\n",
      "epoch: 6 batch_num: 93 loss: 5.4514 acc: 0.984375\n",
      "epoch: 6 batch_num: 94 loss: 6.7087 acc: 0.984375\n",
      "epoch: 6 batch_num: 95 loss: 4.667 acc: 1.0\n",
      "epoch: 6 batch_num: 96 loss: 5.9564 acc: 0.984375\n",
      "epoch: 6 batch_num: 97 loss: 6.2706 acc: 0.96875\n",
      "epoch: 6 batch_num: 98 loss: 5.9959 acc: 0.96875\n",
      "epoch: 6 batch_num: 99 loss: 7.5333 acc: 0.96875\n",
      "epoch: 6 batch_num: 100 loss: 6.6653 acc: 0.96875\n",
      "epoch: 6 batch_num: 101 loss: 5.2743 acc: 0.984375\n",
      "epoch: 6 batch_num: 102 loss: 9.1697 acc: 0.9375\n",
      "epoch: 6 batch_num: 103 loss: 4.6865 acc: 1.0\n",
      "epoch: 6 batch_num: 104 loss: 4.3261 acc: 0.984375\n",
      "epoch: 6 batch_num: 105 loss: 9.5721 acc: 0.953125\n",
      "epoch: 6 batch_num: 106 loss: 13.4118 acc: 0.921875\n",
      "epoch: 6 batch_num: 107 loss: 3.4518 acc: 1.0\n",
      "epoch: 6 batch_num: 108 loss: 9.6927 acc: 0.953125\n",
      "epoch: 6 batch_num: 109 loss: 8.7798 acc: 0.953125\n",
      "epoch: 6 batch_num: 110 loss: 6.8603 acc: 0.984375\n",
      "epoch: 6 batch_num: 111 loss: 8.0747 acc: 0.953125\n",
      "epoch: 6 batch_num: 112 loss: 10.337 acc: 0.9375\n",
      "epoch: 6 batch_num: 113 loss: 8.9165 acc: 0.96875\n",
      "epoch: 6 batch_num: 114 loss: 4.5471 acc: 0.984375\n",
      "epoch: 6 batch_num: 115 loss: 9.7122 acc: 0.953125\n",
      "epoch: 6 batch_num: 116 loss: 1.3479 acc: 1.0\n",
      "epoch: 7 batch_num: 1 loss: 5.0018 acc: 0.984375\n",
      "epoch: 7 batch_num: 2 loss: 3.495 acc: 1.0\n",
      "epoch: 7 batch_num: 3 loss: 4.2397 acc: 0.96875\n",
      "epoch: 7 batch_num: 4 loss: 3.3946 acc: 1.0\n",
      "epoch: 7 batch_num: 5 loss: 3.4245 acc: 1.0\n",
      "epoch: 7 batch_num: 6 loss: 2.4144 acc: 1.0\n",
      "epoch: 7 batch_num: 7 loss: 3.678 acc: 1.0\n",
      "epoch: 7 batch_num: 8 loss: 5.1829 acc: 0.984375\n",
      "epoch: 7 batch_num: 9 loss: 4.0228 acc: 0.984375\n",
      "epoch: 7 batch_num: 10 loss: 2.5973 acc: 1.0\n",
      "epoch: 7 batch_num: 11 loss: 4.2047 acc: 0.984375\n",
      "epoch: 7 batch_num: 12 loss: 2.9017 acc: 1.0\n",
      "epoch: 7 batch_num: 13 loss: 2.6356 acc: 1.0\n",
      "epoch: 7 batch_num: 14 loss: 4.6282 acc: 0.984375\n",
      "epoch: 7 batch_num: 15 loss: 3.9949 acc: 1.0\n",
      "epoch: 7 batch_num: 16 loss: 4.2267 acc: 1.0\n",
      "epoch: 7 batch_num: 17 loss: 4.4458 acc: 0.96875\n",
      "epoch: 7 batch_num: 18 loss: 6.083 acc: 0.984375\n",
      "epoch: 7 batch_num: 19 loss: 2.8361 acc: 1.0\n",
      "epoch: 7 batch_num: 20 loss: 3.2023 acc: 1.0\n",
      "epoch: 7 batch_num: 21 loss: 2.8115 acc: 1.0\n",
      "epoch: 7 batch_num: 22 loss: 4.4085 acc: 0.984375\n",
      "epoch: 7 batch_num: 23 loss: 3.5754 acc: 1.0\n",
      "epoch: 7 batch_num: 24 loss: 2.5269 acc: 0.984375\n",
      "epoch: 7 batch_num: 25 loss: 3.2436 acc: 1.0\n",
      "epoch: 7 batch_num: 26 loss: 3.9369 acc: 1.0\n",
      "epoch: 7 batch_num: 27 loss: 2.6746 acc: 1.0\n",
      "epoch: 7 batch_num: 28 loss: 5.3502 acc: 0.96875\n",
      "epoch: 7 batch_num: 29 loss: 3.9215 acc: 0.984375\n",
      "epoch: 7 batch_num: 30 loss: 4.4922 acc: 0.984375\n",
      "epoch: 7 batch_num: 31 loss: 3.5424 acc: 1.0\n",
      "epoch: 7 batch_num: 32 loss: 3.1101 acc: 0.984375\n",
      "epoch: 7 batch_num: 33 loss: 2.4208 acc: 1.0\n",
      "epoch: 7 batch_num: 34 loss: 5.4258 acc: 0.96875\n",
      "epoch: 7 batch_num: 35 loss: 6.0274 acc: 0.984375\n",
      "epoch: 7 batch_num: 36 loss: 3.6167 acc: 0.984375\n",
      "epoch: 7 batch_num: 37 loss: 3.8153 acc: 1.0\n",
      "epoch: 7 batch_num: 38 loss: 2.9008 acc: 1.0\n",
      "epoch: 7 batch_num: 39 loss: 3.3711 acc: 1.0\n",
      "epoch: 7 batch_num: 40 loss: 5.9804 acc: 0.984375\n",
      "epoch: 7 batch_num: 41 loss: 3.1156 acc: 1.0\n",
      "epoch: 7 batch_num: 42 loss: 3.2583 acc: 1.0\n",
      "epoch: 7 batch_num: 43 loss: 7.1055 acc: 0.96875\n",
      "epoch: 7 batch_num: 44 loss: 3.3996 acc: 1.0\n",
      "epoch: 7 batch_num: 45 loss: 4.4161 acc: 0.984375\n",
      "epoch: 7 batch_num: 46 loss: 2.5683 acc: 1.0\n",
      "epoch: 7 batch_num: 47 loss: 2.5407 acc: 1.0\n",
      "epoch: 7 batch_num: 48 loss: 4.2906 acc: 0.984375\n",
      "epoch: 7 batch_num: 49 loss: 4.7044 acc: 0.984375\n",
      "epoch: 7 batch_num: 50 loss: 2.5189 acc: 1.0\n",
      "epoch: 7 batch_num: 51 loss: 4.8313 acc: 0.984375\n",
      "epoch: 7 batch_num: 52 loss: 2.535 acc: 1.0\n",
      "epoch: 7 batch_num: 53 loss: 4.6541 acc: 1.0\n",
      "epoch: 7 batch_num: 54 loss: 6.0314 acc: 0.984375\n",
      "epoch: 7 batch_num: 55 loss: 3.0131 acc: 1.0\n",
      "epoch: 7 batch_num: 56 loss: 4.1014 acc: 0.984375\n",
      "epoch: 7 batch_num: 57 loss: 5.9299 acc: 0.96875\n",
      "epoch: 7 batch_num: 58 loss: 3.1876 acc: 0.984375\n",
      "epoch: 7 batch_num: 59 loss: 5.8937 acc: 0.984375\n",
      "epoch: 7 batch_num: 60 loss: 5.5214 acc: 0.96875\n",
      "epoch: 7 batch_num: 61 loss: 10.0428 acc: 0.9375\n",
      "epoch: 7 batch_num: 62 loss: 3.9388 acc: 0.984375\n",
      "epoch: 7 batch_num: 63 loss: 3.8377 acc: 1.0\n",
      "epoch: 7 batch_num: 64 loss: 2.1164 acc: 1.0\n",
      "epoch: 7 batch_num: 65 loss: 5.2496 acc: 0.96875\n",
      "epoch: 7 batch_num: 66 loss: 2.7544 acc: 0.984375\n",
      "epoch: 7 batch_num: 67 loss: 5.8616 acc: 0.96875\n",
      "epoch: 7 batch_num: 68 loss: 3.0355 acc: 1.0\n",
      "epoch: 7 batch_num: 69 loss: 3.1088 acc: 0.984375\n",
      "epoch: 7 batch_num: 70 loss: 7.2667 acc: 0.953125\n",
      "epoch: 7 batch_num: 71 loss: 5.0944 acc: 0.96875\n",
      "epoch: 7 batch_num: 72 loss: 4.4551 acc: 0.984375\n",
      "epoch: 7 batch_num: 73 loss: 2.6851 acc: 1.0\n",
      "epoch: 7 batch_num: 74 loss: 2.0521 acc: 1.0\n",
      "epoch: 7 batch_num: 75 loss: 2.2866 acc: 1.0\n",
      "epoch: 7 batch_num: 76 loss: 3.596 acc: 0.984375\n",
      "epoch: 7 batch_num: 77 loss: 3.8418 acc: 0.984375\n",
      "epoch: 7 batch_num: 78 loss: 4.1231 acc: 0.984375\n",
      "epoch: 7 batch_num: 79 loss: 8.3611 acc: 0.96875\n",
      "epoch: 7 batch_num: 80 loss: 4.496 acc: 0.984375\n",
      "epoch: 7 batch_num: 81 loss: 6.0664 acc: 0.96875\n",
      "epoch: 7 batch_num: 82 loss: 5.933 acc: 0.96875\n",
      "epoch: 7 batch_num: 83 loss: 5.4481 acc: 0.984375\n",
      "epoch: 7 batch_num: 84 loss: 9.3047 acc: 0.953125\n",
      "epoch: 7 batch_num: 85 loss: 3.2564 acc: 1.0\n",
      "epoch: 7 batch_num: 86 loss: 4.2435 acc: 0.984375\n",
      "epoch: 7 batch_num: 87 loss: 2.9112 acc: 1.0\n",
      "epoch: 7 batch_num: 88 loss: 5.1433 acc: 0.953125\n",
      "epoch: 7 batch_num: 89 loss: 5.7514 acc: 0.953125\n",
      "epoch: 7 batch_num: 90 loss: 1.6419 acc: 1.0\n",
      "epoch: 7 batch_num: 91 loss: 4.8987 acc: 0.984375\n",
      "epoch: 7 batch_num: 92 loss: 2.5802 acc: 1.0\n",
      "epoch: 7 batch_num: 93 loss: 5.7944 acc: 0.953125\n",
      "epoch: 7 batch_num: 94 loss: 4.9973 acc: 0.984375\n",
      "epoch: 7 batch_num: 95 loss: 7.5658 acc: 0.96875\n",
      "epoch: 7 batch_num: 96 loss: 8.4375 acc: 0.96875\n",
      "epoch: 7 batch_num: 97 loss: 3.4103 acc: 1.0\n",
      "epoch: 7 batch_num: 98 loss: 1.8011 acc: 1.0\n",
      "epoch: 7 batch_num: 99 loss: 3.4881 acc: 0.984375\n",
      "epoch: 7 batch_num: 100 loss: 5.1365 acc: 0.984375\n",
      "epoch: 7 batch_num: 101 loss: 4.9062 acc: 1.0\n",
      "epoch: 7 batch_num: 102 loss: 3.9342 acc: 0.984375\n",
      "epoch: 7 batch_num: 103 loss: 3.607 acc: 1.0\n",
      "epoch: 7 batch_num: 104 loss: 4.3827 acc: 0.984375\n",
      "epoch: 7 batch_num: 105 loss: 5.6309 acc: 0.984375\n",
      "epoch: 7 batch_num: 106 loss: 4.0518 acc: 0.984375\n",
      "epoch: 7 batch_num: 107 loss: 2.6726 acc: 1.0\n",
      "epoch: 7 batch_num: 108 loss: 2.5167 acc: 1.0\n",
      "epoch: 7 batch_num: 109 loss: 5.521 acc: 0.984375\n",
      "epoch: 7 batch_num: 110 loss: 5.0592 acc: 1.0\n",
      "epoch: 7 batch_num: 111 loss: 4.6195 acc: 0.96875\n",
      "epoch: 7 batch_num: 112 loss: 5.1538 acc: 0.96875\n",
      "epoch: 7 batch_num: 113 loss: 3.3883 acc: 0.984375\n",
      "epoch: 7 batch_num: 114 loss: 4.2007 acc: 0.984375\n",
      "epoch: 7 batch_num: 115 loss: 9.8806 acc: 0.96875\n",
      "epoch: 7 batch_num: 116 loss: 1.4119 acc: 1.0\n",
      "epoch: 8 batch_num: 1 loss: 1.9496 acc: 1.0\n",
      "epoch: 8 batch_num: 2 loss: 5.4507 acc: 0.984375\n",
      "epoch: 8 batch_num: 3 loss: 2.5175 acc: 1.0\n",
      "epoch: 8 batch_num: 4 loss: 2.056 acc: 1.0\n",
      "epoch: 8 batch_num: 5 loss: 3.3033 acc: 1.0\n",
      "epoch: 8 batch_num: 6 loss: 2.2272 acc: 1.0\n",
      "epoch: 8 batch_num: 7 loss: 4.2208 acc: 0.984375\n",
      "epoch: 8 batch_num: 8 loss: 2.531 acc: 0.984375\n",
      "epoch: 8 batch_num: 9 loss: 1.3883 acc: 1.0\n",
      "epoch: 8 batch_num: 10 loss: 3.991 acc: 1.0\n",
      "epoch: 8 batch_num: 11 loss: 2.9119 acc: 1.0\n",
      "epoch: 8 batch_num: 12 loss: 2.5609 acc: 1.0\n",
      "epoch: 8 batch_num: 13 loss: 2.327 acc: 1.0\n",
      "epoch: 8 batch_num: 14 loss: 3.2348 acc: 0.984375\n",
      "epoch: 8 batch_num: 15 loss: 1.7704 acc: 1.0\n",
      "epoch: 8 batch_num: 16 loss: 2.6174 acc: 1.0\n",
      "epoch: 8 batch_num: 17 loss: 1.8717 acc: 1.0\n",
      "epoch: 8 batch_num: 18 loss: 2.3451 acc: 1.0\n",
      "epoch: 8 batch_num: 19 loss: 2.6287 acc: 1.0\n",
      "epoch: 8 batch_num: 20 loss: 3.5997 acc: 0.984375\n",
      "epoch: 8 batch_num: 21 loss: 2.3395 acc: 1.0\n",
      "epoch: 8 batch_num: 22 loss: 1.5224 acc: 1.0\n",
      "epoch: 8 batch_num: 23 loss: 2.5074 acc: 1.0\n",
      "epoch: 8 batch_num: 24 loss: 2.1323 acc: 1.0\n",
      "epoch: 8 batch_num: 25 loss: 4.541 acc: 0.984375\n",
      "epoch: 8 batch_num: 26 loss: 2.9274 acc: 1.0\n",
      "epoch: 8 batch_num: 27 loss: 2.6344 acc: 1.0\n",
      "epoch: 8 batch_num: 28 loss: 2.6728 acc: 1.0\n",
      "epoch: 8 batch_num: 29 loss: 2.8764 acc: 1.0\n",
      "epoch: 8 batch_num: 30 loss: 3.4074 acc: 0.984375\n",
      "epoch: 8 batch_num: 31 loss: 2.9398 acc: 1.0\n",
      "epoch: 8 batch_num: 32 loss: 2.4816 acc: 1.0\n",
      "epoch: 8 batch_num: 33 loss: 3.8421 acc: 0.984375\n",
      "epoch: 8 batch_num: 34 loss: 3.9715 acc: 0.984375\n",
      "epoch: 8 batch_num: 35 loss: 1.7198 acc: 1.0\n",
      "epoch: 8 batch_num: 36 loss: 2.719 acc: 1.0\n",
      "epoch: 8 batch_num: 37 loss: 6.5102 acc: 0.984375\n",
      "epoch: 8 batch_num: 38 loss: 1.7485 acc: 1.0\n",
      "epoch: 8 batch_num: 39 loss: 2.9687 acc: 1.0\n",
      "epoch: 8 batch_num: 40 loss: 7.6648 acc: 0.953125\n",
      "epoch: 8 batch_num: 41 loss: 2.4024 acc: 1.0\n",
      "epoch: 8 batch_num: 42 loss: 3.8155 acc: 0.984375\n",
      "epoch: 8 batch_num: 43 loss: 2.9968 acc: 1.0\n",
      "epoch: 8 batch_num: 44 loss: 2.3902 acc: 1.0\n",
      "epoch: 8 batch_num: 45 loss: 3.267 acc: 0.984375\n",
      "epoch: 8 batch_num: 46 loss: 1.6223 acc: 1.0\n",
      "epoch: 8 batch_num: 47 loss: 3.0049 acc: 0.984375\n",
      "epoch: 8 batch_num: 48 loss: 1.6671 acc: 1.0\n",
      "epoch: 8 batch_num: 49 loss: 5.3315 acc: 0.984375\n",
      "epoch: 8 batch_num: 50 loss: 4.8206 acc: 0.96875\n",
      "epoch: 8 batch_num: 51 loss: 2.0119 acc: 1.0\n",
      "epoch: 8 batch_num: 52 loss: 2.4787 acc: 1.0\n",
      "epoch: 8 batch_num: 53 loss: 2.3395 acc: 1.0\n",
      "epoch: 8 batch_num: 54 loss: 3.1801 acc: 1.0\n",
      "epoch: 8 batch_num: 55 loss: 4.1889 acc: 0.984375\n",
      "epoch: 8 batch_num: 56 loss: 7.093 acc: 0.984375\n",
      "epoch: 8 batch_num: 57 loss: 4.0869 acc: 0.984375\n",
      "epoch: 8 batch_num: 58 loss: 4.3984 acc: 0.984375\n",
      "epoch: 8 batch_num: 59 loss: 2.133 acc: 0.984375\n",
      "epoch: 8 batch_num: 60 loss: 2.6627 acc: 0.984375\n",
      "epoch: 8 batch_num: 61 loss: 3.0907 acc: 0.984375\n",
      "epoch: 8 batch_num: 62 loss: 5.4115 acc: 0.96875\n",
      "epoch: 8 batch_num: 63 loss: 3.196 acc: 0.984375\n",
      "epoch: 8 batch_num: 64 loss: 11.5738 acc: 0.953125\n",
      "epoch: 8 batch_num: 65 loss: 2.2959 acc: 1.0\n",
      "epoch: 8 batch_num: 66 loss: 3.6761 acc: 1.0\n",
      "epoch: 8 batch_num: 67 loss: 2.2368 acc: 1.0\n",
      "epoch: 8 batch_num: 68 loss: 1.3651 acc: 1.0\n",
      "epoch: 8 batch_num: 69 loss: 4.239 acc: 1.0\n",
      "epoch: 8 batch_num: 70 loss: 2.4644 acc: 1.0\n",
      "epoch: 8 batch_num: 71 loss: 2.7142 acc: 0.984375\n",
      "epoch: 8 batch_num: 72 loss: 2.0745 acc: 1.0\n",
      "epoch: 8 batch_num: 73 loss: 2.5764 acc: 1.0\n",
      "epoch: 8 batch_num: 74 loss: 3.0658 acc: 0.984375\n",
      "epoch: 8 batch_num: 75 loss: 3.1556 acc: 1.0\n",
      "epoch: 8 batch_num: 76 loss: 1.4823 acc: 1.0\n",
      "epoch: 8 batch_num: 77 loss: 2.4978 acc: 0.984375\n",
      "epoch: 8 batch_num: 78 loss: 5.8883 acc: 0.953125\n",
      "epoch: 8 batch_num: 79 loss: 5.012 acc: 0.984375\n",
      "epoch: 8 batch_num: 80 loss: 4.8849 acc: 0.96875\n",
      "epoch: 8 batch_num: 81 loss: 1.7841 acc: 1.0\n",
      "epoch: 8 batch_num: 82 loss: 2.7125 acc: 1.0\n",
      "epoch: 8 batch_num: 83 loss: 3.8613 acc: 0.984375\n",
      "epoch: 8 batch_num: 84 loss: 2.3631 acc: 1.0\n",
      "epoch: 8 batch_num: 85 loss: 2.0306 acc: 1.0\n",
      "epoch: 8 batch_num: 86 loss: 2.5157 acc: 1.0\n",
      "epoch: 8 batch_num: 87 loss: 4.3163 acc: 0.984375\n",
      "epoch: 8 batch_num: 88 loss: 2.105 acc: 1.0\n",
      "epoch: 8 batch_num: 89 loss: 2.4189 acc: 1.0\n",
      "epoch: 8 batch_num: 90 loss: 1.864 acc: 1.0\n",
      "epoch: 8 batch_num: 91 loss: 3.4902 acc: 0.984375\n",
      "epoch: 8 batch_num: 92 loss: 1.8879 acc: 1.0\n",
      "epoch: 8 batch_num: 93 loss: 5.3561 acc: 0.984375\n",
      "epoch: 8 batch_num: 94 loss: 2.7286 acc: 0.984375\n",
      "epoch: 8 batch_num: 95 loss: 1.8425 acc: 1.0\n",
      "epoch: 8 batch_num: 96 loss: 2.4296 acc: 1.0\n",
      "epoch: 8 batch_num: 97 loss: 1.7052 acc: 1.0\n",
      "epoch: 8 batch_num: 98 loss: 3.1269 acc: 1.0\n",
      "epoch: 8 batch_num: 99 loss: 1.896 acc: 1.0\n",
      "epoch: 8 batch_num: 100 loss: 2.2244 acc: 1.0\n",
      "epoch: 8 batch_num: 101 loss: 2.7185 acc: 1.0\n",
      "epoch: 8 batch_num: 102 loss: 1.875 acc: 1.0\n",
      "epoch: 8 batch_num: 103 loss: 5.5984 acc: 0.96875\n",
      "epoch: 8 batch_num: 104 loss: 2.1034 acc: 1.0\n",
      "epoch: 8 batch_num: 105 loss: 2.0003 acc: 1.0\n",
      "epoch: 8 batch_num: 106 loss: 3.7051 acc: 0.984375\n",
      "epoch: 8 batch_num: 107 loss: 3.9609 acc: 0.984375\n",
      "epoch: 8 batch_num: 108 loss: 7.597 acc: 0.9375\n",
      "epoch: 8 batch_num: 109 loss: 3.9138 acc: 0.96875\n",
      "epoch: 8 batch_num: 110 loss: 2.6074 acc: 1.0\n",
      "epoch: 8 batch_num: 111 loss: 2.2915 acc: 1.0\n",
      "epoch: 8 batch_num: 112 loss: 3.6284 acc: 0.984375\n",
      "epoch: 8 batch_num: 113 loss: 1.946 acc: 1.0\n",
      "epoch: 8 batch_num: 114 loss: 6.3027 acc: 0.96875\n",
      "epoch: 8 batch_num: 115 loss: 2.5418 acc: 0.984375\n",
      "epoch: 8 batch_num: 116 loss: 1.3448 acc: 0.9655172413793104\n",
      "epoch: 9 batch_num: 1 loss: 2.492 acc: 0.984375\n",
      "epoch: 9 batch_num: 2 loss: 1.9376 acc: 1.0\n",
      "epoch: 9 batch_num: 3 loss: 1.5774 acc: 1.0\n",
      "epoch: 9 batch_num: 4 loss: 4.8027 acc: 0.984375\n",
      "epoch: 9 batch_num: 5 loss: 1.2425 acc: 1.0\n",
      "epoch: 9 batch_num: 6 loss: 1.7842 acc: 1.0\n",
      "epoch: 9 batch_num: 7 loss: 1.9893 acc: 1.0\n",
      "epoch: 9 batch_num: 8 loss: 1.3043 acc: 1.0\n",
      "epoch: 9 batch_num: 9 loss: 1.6745 acc: 1.0\n",
      "epoch: 9 batch_num: 10 loss: 1.4202 acc: 1.0\n",
      "epoch: 9 batch_num: 11 loss: 1.2997 acc: 1.0\n",
      "epoch: 9 batch_num: 12 loss: 1.5705 acc: 1.0\n",
      "epoch: 9 batch_num: 13 loss: 4.0397 acc: 0.984375\n",
      "epoch: 9 batch_num: 14 loss: 1.9756 acc: 1.0\n",
      "epoch: 9 batch_num: 15 loss: 3.0166 acc: 1.0\n",
      "epoch: 9 batch_num: 16 loss: 1.9014 acc: 1.0\n",
      "epoch: 9 batch_num: 17 loss: 0.9878 acc: 1.0\n",
      "epoch: 9 batch_num: 18 loss: 1.3643 acc: 1.0\n",
      "epoch: 9 batch_num: 19 loss: 1.3445 acc: 1.0\n",
      "epoch: 9 batch_num: 20 loss: 1.8888 acc: 1.0\n",
      "epoch: 9 batch_num: 21 loss: 1.737 acc: 1.0\n",
      "epoch: 9 batch_num: 22 loss: 1.6024 acc: 1.0\n",
      "epoch: 9 batch_num: 23 loss: 1.8784 acc: 1.0\n",
      "epoch: 9 batch_num: 24 loss: 1.3409 acc: 1.0\n",
      "epoch: 9 batch_num: 25 loss: 3.0911 acc: 0.984375\n",
      "epoch: 9 batch_num: 26 loss: 1.7638 acc: 1.0\n",
      "epoch: 9 batch_num: 27 loss: 3.4561 acc: 0.984375\n",
      "epoch: 9 batch_num: 28 loss: 1.0979 acc: 1.0\n",
      "epoch: 9 batch_num: 29 loss: 3.6583 acc: 0.96875\n",
      "epoch: 9 batch_num: 30 loss: 4.8325 acc: 0.96875\n",
      "epoch: 9 batch_num: 31 loss: 1.277 acc: 1.0\n",
      "epoch: 9 batch_num: 32 loss: 1.231 acc: 1.0\n",
      "epoch: 9 batch_num: 33 loss: 5.2117 acc: 0.96875\n",
      "epoch: 9 batch_num: 34 loss: 0.9395 acc: 1.0\n",
      "epoch: 9 batch_num: 35 loss: 1.4822 acc: 1.0\n",
      "epoch: 9 batch_num: 36 loss: 3.512 acc: 0.984375\n",
      "epoch: 9 batch_num: 37 loss: 1.69 acc: 1.0\n",
      "epoch: 9 batch_num: 38 loss: 1.9487 acc: 1.0\n",
      "epoch: 9 batch_num: 39 loss: 2.0781 acc: 1.0\n",
      "epoch: 9 batch_num: 40 loss: 2.8006 acc: 0.984375\n",
      "epoch: 9 batch_num: 41 loss: 1.2031 acc: 1.0\n",
      "epoch: 9 batch_num: 42 loss: 1.6166 acc: 1.0\n",
      "epoch: 9 batch_num: 43 loss: 1.5705 acc: 1.0\n",
      "epoch: 9 batch_num: 44 loss: 1.9922 acc: 0.984375\n",
      "epoch: 9 batch_num: 45 loss: 1.7806 acc: 1.0\n",
      "epoch: 9 batch_num: 46 loss: 1.5289 acc: 1.0\n",
      "epoch: 9 batch_num: 47 loss: 1.8156 acc: 1.0\n",
      "epoch: 9 batch_num: 48 loss: 1.506 acc: 1.0\n",
      "epoch: 9 batch_num: 49 loss: 1.3521 acc: 1.0\n",
      "epoch: 9 batch_num: 50 loss: 1.317 acc: 1.0\n",
      "epoch: 9 batch_num: 51 loss: 1.4216 acc: 1.0\n",
      "epoch: 9 batch_num: 52 loss: 1.5671 acc: 1.0\n",
      "epoch: 9 batch_num: 53 loss: 2.3366 acc: 1.0\n",
      "epoch: 9 batch_num: 54 loss: 2.1785 acc: 0.984375\n",
      "epoch: 9 batch_num: 55 loss: 2.5521 acc: 0.984375\n",
      "epoch: 9 batch_num: 56 loss: 1.3692 acc: 1.0\n",
      "epoch: 9 batch_num: 57 loss: 3.2941 acc: 0.984375\n",
      "epoch: 9 batch_num: 58 loss: 2.0271 acc: 1.0\n",
      "epoch: 9 batch_num: 59 loss: 1.6481 acc: 1.0\n",
      "epoch: 9 batch_num: 60 loss: 2.8139 acc: 0.984375\n",
      "epoch: 9 batch_num: 61 loss: 2.1947 acc: 1.0\n",
      "epoch: 9 batch_num: 62 loss: 0.8022 acc: 1.0\n",
      "epoch: 9 batch_num: 63 loss: 2.6789 acc: 1.0\n",
      "epoch: 9 batch_num: 64 loss: 3.4579 acc: 0.984375\n",
      "epoch: 9 batch_num: 65 loss: 1.6438 acc: 1.0\n",
      "epoch: 9 batch_num: 66 loss: 1.4836 acc: 1.0\n",
      "epoch: 9 batch_num: 67 loss: 2.498 acc: 0.984375\n",
      "epoch: 9 batch_num: 68 loss: 3.1337 acc: 0.984375\n",
      "epoch: 9 batch_num: 69 loss: 1.448 acc: 1.0\n",
      "epoch: 9 batch_num: 70 loss: 2.9063 acc: 0.984375\n",
      "epoch: 9 batch_num: 71 loss: 1.9731 acc: 0.984375\n",
      "epoch: 9 batch_num: 72 loss: 2.1821 acc: 1.0\n",
      "epoch: 9 batch_num: 73 loss: 1.6763 acc: 1.0\n",
      "epoch: 9 batch_num: 74 loss: 1.9052 acc: 1.0\n",
      "epoch: 9 batch_num: 75 loss: 2.0405 acc: 1.0\n",
      "epoch: 9 batch_num: 76 loss: 0.9744 acc: 1.0\n",
      "epoch: 9 batch_num: 77 loss: 2.8001 acc: 1.0\n",
      "epoch: 9 batch_num: 78 loss: 2.2554 acc: 0.984375\n",
      "epoch: 9 batch_num: 79 loss: 1.2501 acc: 1.0\n",
      "epoch: 9 batch_num: 80 loss: 1.3732 acc: 1.0\n",
      "epoch: 9 batch_num: 81 loss: 2.6831 acc: 1.0\n",
      "epoch: 9 batch_num: 82 loss: 2.3822 acc: 1.0\n",
      "epoch: 9 batch_num: 83 loss: 1.7273 acc: 1.0\n",
      "epoch: 9 batch_num: 84 loss: 5.0831 acc: 0.96875\n",
      "epoch: 9 batch_num: 85 loss: 2.4994 acc: 0.984375\n",
      "epoch: 9 batch_num: 86 loss: 1.4646 acc: 1.0\n",
      "epoch: 9 batch_num: 87 loss: 1.6122 acc: 1.0\n",
      "epoch: 9 batch_num: 88 loss: 1.8793 acc: 1.0\n",
      "epoch: 9 batch_num: 89 loss: 2.0387 acc: 0.984375\n",
      "epoch: 9 batch_num: 90 loss: 2.3528 acc: 0.984375\n",
      "epoch: 9 batch_num: 91 loss: 1.6254 acc: 1.0\n",
      "epoch: 9 batch_num: 92 loss: 1.3605 acc: 1.0\n",
      "epoch: 9 batch_num: 93 loss: 1.7072 acc: 1.0\n",
      "epoch: 9 batch_num: 94 loss: 1.9546 acc: 1.0\n",
      "epoch: 9 batch_num: 95 loss: 2.0214 acc: 0.984375\n",
      "epoch: 9 batch_num: 96 loss: 1.7388 acc: 1.0\n",
      "epoch: 9 batch_num: 97 loss: 1.6488 acc: 1.0\n",
      "epoch: 9 batch_num: 98 loss: 3.979 acc: 0.984375\n",
      "epoch: 9 batch_num: 99 loss: 5.05 acc: 0.984375\n",
      "epoch: 9 batch_num: 100 loss: 1.9575 acc: 1.0\n",
      "epoch: 9 batch_num: 101 loss: 1.7674 acc: 1.0\n",
      "epoch: 9 batch_num: 102 loss: 3.1411 acc: 0.984375\n",
      "epoch: 9 batch_num: 103 loss: 1.8389 acc: 1.0\n",
      "epoch: 9 batch_num: 104 loss: 1.424 acc: 1.0\n",
      "epoch: 9 batch_num: 105 loss: 3.7615 acc: 0.984375\n",
      "epoch: 9 batch_num: 106 loss: 4.507 acc: 0.984375\n",
      "epoch: 9 batch_num: 107 loss: 2.578 acc: 0.984375\n",
      "epoch: 9 batch_num: 108 loss: 1.2846 acc: 1.0\n",
      "epoch: 9 batch_num: 109 loss: 1.1856 acc: 1.0\n",
      "epoch: 9 batch_num: 110 loss: 2.2307 acc: 1.0\n",
      "epoch: 9 batch_num: 111 loss: 2.4436 acc: 0.984375\n",
      "epoch: 9 batch_num: 112 loss: 3.0433 acc: 0.984375\n",
      "epoch: 9 batch_num: 113 loss: 3.3873 acc: 0.984375\n",
      "epoch: 9 batch_num: 114 loss: 3.2538 acc: 0.984375\n",
      "epoch: 9 batch_num: 115 loss: 3.5317 acc: 0.96875\n",
      "epoch: 9 batch_num: 116 loss: 1.0855 acc: 1.0\n",
      "epoch: 10 batch_num: 1 loss: 1.1116 acc: 1.0\n",
      "epoch: 10 batch_num: 2 loss: 2.6178 acc: 0.984375\n",
      "epoch: 10 batch_num: 3 loss: 1.8924 acc: 0.984375\n",
      "epoch: 10 batch_num: 4 loss: 0.941 acc: 1.0\n",
      "epoch: 10 batch_num: 5 loss: 1.3017 acc: 1.0\n",
      "epoch: 10 batch_num: 6 loss: 1.1809 acc: 1.0\n",
      "epoch: 10 batch_num: 7 loss: 0.9779 acc: 1.0\n",
      "epoch: 10 batch_num: 8 loss: 1.2407 acc: 1.0\n",
      "epoch: 10 batch_num: 9 loss: 1.1366 acc: 1.0\n",
      "epoch: 10 batch_num: 10 loss: 2.8548 acc: 0.984375\n",
      "epoch: 10 batch_num: 11 loss: 1.0874 acc: 1.0\n",
      "epoch: 10 batch_num: 12 loss: 0.94 acc: 1.0\n",
      "epoch: 10 batch_num: 13 loss: 0.8171 acc: 1.0\n",
      "epoch: 10 batch_num: 14 loss: 1.9268 acc: 1.0\n",
      "epoch: 10 batch_num: 15 loss: 2.143 acc: 1.0\n",
      "epoch: 10 batch_num: 16 loss: 1.9759 acc: 1.0\n",
      "epoch: 10 batch_num: 17 loss: 1.1255 acc: 1.0\n",
      "epoch: 10 batch_num: 18 loss: 3.0946 acc: 0.984375\n",
      "epoch: 10 batch_num: 19 loss: 2.1938 acc: 1.0\n",
      "epoch: 10 batch_num: 20 loss: 2.3859 acc: 0.984375\n",
      "epoch: 10 batch_num: 21 loss: 1.9285 acc: 1.0\n",
      "epoch: 10 batch_num: 22 loss: 2.5876 acc: 0.984375\n",
      "epoch: 10 batch_num: 23 loss: 2.8361 acc: 0.984375\n",
      "epoch: 10 batch_num: 24 loss: 1.7255 acc: 1.0\n",
      "epoch: 10 batch_num: 25 loss: 1.242 acc: 1.0\n",
      "epoch: 10 batch_num: 26 loss: 1.6436 acc: 0.984375\n",
      "epoch: 10 batch_num: 27 loss: 0.8384 acc: 1.0\n",
      "epoch: 10 batch_num: 28 loss: 0.908 acc: 1.0\n",
      "epoch: 10 batch_num: 29 loss: 5.6476 acc: 0.96875\n",
      "epoch: 10 batch_num: 30 loss: 1.0349 acc: 1.0\n",
      "epoch: 10 batch_num: 31 loss: 2.5391 acc: 1.0\n",
      "epoch: 10 batch_num: 32 loss: 0.8599 acc: 1.0\n",
      "epoch: 10 batch_num: 33 loss: 1.3924 acc: 1.0\n",
      "epoch: 10 batch_num: 34 loss: 2.4368 acc: 1.0\n",
      "epoch: 10 batch_num: 35 loss: 2.1415 acc: 0.984375\n",
      "epoch: 10 batch_num: 36 loss: 1.5327 acc: 0.984375\n",
      "epoch: 10 batch_num: 37 loss: 1.3969 acc: 1.0\n",
      "epoch: 10 batch_num: 38 loss: 1.0839 acc: 1.0\n",
      "epoch: 10 batch_num: 39 loss: 1.3137 acc: 1.0\n",
      "epoch: 10 batch_num: 40 loss: 1.9384 acc: 1.0\n",
      "epoch: 10 batch_num: 41 loss: 1.3255 acc: 1.0\n",
      "epoch: 10 batch_num: 42 loss: 1.5658 acc: 1.0\n",
      "epoch: 10 batch_num: 43 loss: 1.2243 acc: 1.0\n",
      "epoch: 10 batch_num: 44 loss: 2.1861 acc: 0.984375\n",
      "epoch: 10 batch_num: 45 loss: 1.6501 acc: 1.0\n",
      "epoch: 10 batch_num: 46 loss: 1.0333 acc: 1.0\n",
      "epoch: 10 batch_num: 47 loss: 1.5332 acc: 1.0\n",
      "epoch: 10 batch_num: 48 loss: 1.3945 acc: 1.0\n",
      "epoch: 10 batch_num: 49 loss: 0.7037 acc: 1.0\n",
      "epoch: 10 batch_num: 50 loss: 0.9808 acc: 1.0\n",
      "epoch: 10 batch_num: 51 loss: 1.676 acc: 1.0\n",
      "epoch: 10 batch_num: 52 loss: 1.7849 acc: 1.0\n",
      "epoch: 10 batch_num: 53 loss: 5.2105 acc: 0.984375\n",
      "epoch: 10 batch_num: 54 loss: 1.5882 acc: 1.0\n",
      "epoch: 10 batch_num: 55 loss: 1.5088 acc: 1.0\n",
      "epoch: 10 batch_num: 56 loss: 2.9814 acc: 0.984375\n",
      "epoch: 10 batch_num: 57 loss: 2.0098 acc: 0.984375\n",
      "epoch: 10 batch_num: 58 loss: 0.6609 acc: 1.0\n",
      "epoch: 10 batch_num: 59 loss: 1.2282 acc: 1.0\n",
      "epoch: 10 batch_num: 60 loss: 3.6267 acc: 0.984375\n",
      "epoch: 10 batch_num: 61 loss: 0.9719 acc: 1.0\n",
      "epoch: 10 batch_num: 62 loss: 1.5155 acc: 1.0\n",
      "epoch: 10 batch_num: 63 loss: 1.338 acc: 1.0\n",
      "epoch: 10 batch_num: 64 loss: 1.308 acc: 1.0\n",
      "epoch: 10 batch_num: 65 loss: 2.7485 acc: 0.984375\n",
      "epoch: 10 batch_num: 66 loss: 1.3021 acc: 1.0\n",
      "epoch: 10 batch_num: 67 loss: 0.8359 acc: 1.0\n",
      "epoch: 10 batch_num: 68 loss: 0.9919 acc: 1.0\n",
      "epoch: 10 batch_num: 69 loss: 1.2008 acc: 1.0\n",
      "epoch: 10 batch_num: 70 loss: 1.3747 acc: 1.0\n",
      "epoch: 10 batch_num: 71 loss: 1.8814 acc: 0.984375\n",
      "epoch: 10 batch_num: 72 loss: 1.686 acc: 1.0\n",
      "epoch: 10 batch_num: 73 loss: 2.7544 acc: 0.984375\n",
      "epoch: 10 batch_num: 74 loss: 0.6666 acc: 1.0\n",
      "epoch: 10 batch_num: 75 loss: 0.6609 acc: 1.0\n",
      "epoch: 10 batch_num: 76 loss: 1.1214 acc: 1.0\n",
      "epoch: 10 batch_num: 77 loss: 5.0506 acc: 0.96875\n",
      "epoch: 10 batch_num: 78 loss: 1.5605 acc: 1.0\n",
      "epoch: 10 batch_num: 79 loss: 4.5575 acc: 0.984375\n",
      "epoch: 10 batch_num: 80 loss: 2.5476 acc: 0.984375\n",
      "epoch: 10 batch_num: 81 loss: 1.1591 acc: 1.0\n",
      "epoch: 10 batch_num: 82 loss: 2.1708 acc: 0.984375\n",
      "epoch: 10 batch_num: 83 loss: 2.0779 acc: 1.0\n",
      "epoch: 10 batch_num: 84 loss: 2.4819 acc: 1.0\n",
      "epoch: 10 batch_num: 85 loss: 1.6001 acc: 1.0\n",
      "epoch: 10 batch_num: 86 loss: 2.0328 acc: 1.0\n",
      "epoch: 10 batch_num: 87 loss: 1.1846 acc: 1.0\n",
      "epoch: 10 batch_num: 88 loss: 1.1517 acc: 1.0\n",
      "epoch: 10 batch_num: 89 loss: 1.895 acc: 0.984375\n",
      "epoch: 10 batch_num: 90 loss: 0.9279 acc: 1.0\n",
      "epoch: 10 batch_num: 91 loss: 1.2025 acc: 1.0\n",
      "epoch: 10 batch_num: 92 loss: 3.7875 acc: 0.984375\n",
      "epoch: 10 batch_num: 93 loss: 1.2289 acc: 1.0\n",
      "epoch: 10 batch_num: 94 loss: 1.9038 acc: 1.0\n",
      "epoch: 10 batch_num: 95 loss: 2.854 acc: 0.984375\n",
      "epoch: 10 batch_num: 96 loss: 1.8244 acc: 1.0\n",
      "epoch: 10 batch_num: 97 loss: 1.6689 acc: 1.0\n",
      "epoch: 10 batch_num: 98 loss: 1.3416 acc: 1.0\n",
      "epoch: 10 batch_num: 99 loss: 1.5694 acc: 0.984375\n",
      "epoch: 10 batch_num: 100 loss: 2.848 acc: 0.984375\n",
      "epoch: 10 batch_num: 101 loss: 1.6731 acc: 1.0\n",
      "epoch: 10 batch_num: 102 loss: 2.4023 acc: 0.984375\n",
      "epoch: 10 batch_num: 103 loss: 0.7802 acc: 1.0\n",
      "epoch: 10 batch_num: 104 loss: 2.7506 acc: 0.984375\n",
      "epoch: 10 batch_num: 105 loss: 2.4327 acc: 0.984375\n",
      "epoch: 10 batch_num: 106 loss: 2.5362 acc: 0.984375\n",
      "epoch: 10 batch_num: 107 loss: 2.2065 acc: 0.984375\n",
      "epoch: 10 batch_num: 108 loss: 1.0597 acc: 1.0\n",
      "epoch: 10 batch_num: 109 loss: 1.7855 acc: 1.0\n",
      "epoch: 10 batch_num: 110 loss: 1.6095 acc: 1.0\n",
      "epoch: 10 batch_num: 111 loss: 2.1725 acc: 0.984375\n",
      "epoch: 10 batch_num: 112 loss: 1.288 acc: 1.0\n",
      "epoch: 10 batch_num: 113 loss: 1.2284 acc: 1.0\n",
      "epoch: 10 batch_num: 114 loss: 4.7133 acc: 0.984375\n",
      "epoch: 10 batch_num: 115 loss: 2.5351 acc: 0.984375\n",
      "epoch: 10 batch_num: 116 loss: 0.5329 acc: 1.0\n",
      "epoch: 11 batch_num: 1 loss: 0.8606 acc: 1.0\n",
      "epoch: 11 batch_num: 2 loss: 0.9889 acc: 1.0\n",
      "epoch: 11 batch_num: 3 loss: 1.7512 acc: 1.0\n",
      "epoch: 11 batch_num: 4 loss: 1.5216 acc: 1.0\n",
      "epoch: 11 batch_num: 5 loss: 1.8685 acc: 1.0\n",
      "epoch: 11 batch_num: 6 loss: 0.8606 acc: 1.0\n",
      "epoch: 11 batch_num: 7 loss: 0.6664 acc: 1.0\n",
      "epoch: 11 batch_num: 8 loss: 1.9061 acc: 0.984375\n",
      "epoch: 11 batch_num: 9 loss: 1.0278 acc: 1.0\n",
      "epoch: 11 batch_num: 10 loss: 1.4026 acc: 1.0\n",
      "epoch: 11 batch_num: 11 loss: 0.5874 acc: 1.0\n",
      "epoch: 11 batch_num: 12 loss: 1.7524 acc: 0.984375\n",
      "epoch: 11 batch_num: 13 loss: 0.7716 acc: 1.0\n",
      "epoch: 11 batch_num: 14 loss: 0.8624 acc: 1.0\n",
      "epoch: 11 batch_num: 15 loss: 1.8616 acc: 1.0\n",
      "epoch: 11 batch_num: 16 loss: 0.7872 acc: 1.0\n",
      "epoch: 11 batch_num: 17 loss: 2.5832 acc: 0.984375\n",
      "epoch: 11 batch_num: 18 loss: 1.0949 acc: 1.0\n",
      "epoch: 11 batch_num: 19 loss: 1.6103 acc: 1.0\n",
      "epoch: 11 batch_num: 20 loss: 0.9093 acc: 1.0\n",
      "epoch: 11 batch_num: 21 loss: 0.7979 acc: 1.0\n",
      "epoch: 11 batch_num: 22 loss: 1.1402 acc: 1.0\n",
      "epoch: 11 batch_num: 23 loss: 0.8659 acc: 1.0\n",
      "epoch: 11 batch_num: 24 loss: 2.4429 acc: 0.984375\n",
      "epoch: 11 batch_num: 25 loss: 1.0636 acc: 1.0\n",
      "epoch: 11 batch_num: 26 loss: 0.7883 acc: 1.0\n",
      "epoch: 11 batch_num: 27 loss: 1.0561 acc: 1.0\n",
      "epoch: 11 batch_num: 28 loss: 1.2776 acc: 1.0\n",
      "epoch: 11 batch_num: 29 loss: 0.5375 acc: 1.0\n",
      "epoch: 11 batch_num: 30 loss: 1.9951 acc: 0.984375\n",
      "epoch: 11 batch_num: 31 loss: 3.3374 acc: 0.984375\n",
      "epoch: 11 batch_num: 32 loss: 0.6376 acc: 1.0\n",
      "epoch: 11 batch_num: 33 loss: 3.8951 acc: 0.984375\n",
      "epoch: 11 batch_num: 34 loss: 1.1465 acc: 1.0\n",
      "epoch: 11 batch_num: 35 loss: 1.5189 acc: 1.0\n",
      "epoch: 11 batch_num: 36 loss: 2.1269 acc: 1.0\n",
      "epoch: 11 batch_num: 37 loss: 2.46 acc: 0.984375\n",
      "epoch: 11 batch_num: 38 loss: 2.0651 acc: 0.984375\n",
      "epoch: 11 batch_num: 39 loss: 1.0361 acc: 1.0\n",
      "epoch: 11 batch_num: 40 loss: 1.2847 acc: 1.0\n",
      "epoch: 11 batch_num: 41 loss: 0.646 acc: 1.0\n",
      "epoch: 11 batch_num: 42 loss: 0.7591 acc: 1.0\n",
      "epoch: 11 batch_num: 43 loss: 1.0 acc: 1.0\n",
      "epoch: 11 batch_num: 44 loss: 1.2759 acc: 1.0\n",
      "epoch: 11 batch_num: 45 loss: 1.1297 acc: 1.0\n",
      "epoch: 11 batch_num: 46 loss: 0.762 acc: 1.0\n",
      "epoch: 11 batch_num: 47 loss: 0.9909 acc: 1.0\n",
      "epoch: 11 batch_num: 48 loss: 1.2782 acc: 1.0\n",
      "epoch: 11 batch_num: 49 loss: 1.6592 acc: 1.0\n",
      "epoch: 11 batch_num: 50 loss: 3.1569 acc: 0.984375\n",
      "epoch: 11 batch_num: 51 loss: 8.2223 acc: 0.984375\n",
      "epoch: 11 batch_num: 52 loss: 5.5325 acc: 0.984375\n",
      "epoch: 11 batch_num: 53 loss: 3.2212 acc: 0.984375\n",
      "epoch: 11 batch_num: 54 loss: 0.5896 acc: 1.0\n",
      "epoch: 11 batch_num: 55 loss: 1.2551 acc: 1.0\n",
      "epoch: 11 batch_num: 56 loss: 0.7773 acc: 1.0\n",
      "epoch: 11 batch_num: 57 loss: 0.842 acc: 1.0\n",
      "epoch: 11 batch_num: 58 loss: 2.5667 acc: 0.984375\n",
      "epoch: 11 batch_num: 59 loss: 1.5216 acc: 0.984375\n",
      "epoch: 11 batch_num: 60 loss: 0.6955 acc: 1.0\n",
      "epoch: 11 batch_num: 61 loss: 4.6033 acc: 0.984375\n",
      "epoch: 11 batch_num: 62 loss: 1.4951 acc: 1.0\n",
      "epoch: 11 batch_num: 63 loss: 1.2493 acc: 1.0\n",
      "epoch: 11 batch_num: 64 loss: 1.8691 acc: 0.984375\n",
      "epoch: 11 batch_num: 65 loss: 1.1375 acc: 1.0\n",
      "epoch: 11 batch_num: 66 loss: 1.3763 acc: 1.0\n",
      "epoch: 11 batch_num: 67 loss: 1.029 acc: 1.0\n",
      "epoch: 11 batch_num: 68 loss: 0.9278 acc: 1.0\n",
      "epoch: 11 batch_num: 69 loss: 1.2653 acc: 0.984375\n",
      "epoch: 11 batch_num: 70 loss: 1.6805 acc: 1.0\n",
      "epoch: 11 batch_num: 71 loss: 2.7794 acc: 0.984375\n",
      "epoch: 11 batch_num: 72 loss: 1.2262 acc: 1.0\n",
      "epoch: 11 batch_num: 73 loss: 1.7038 acc: 0.984375\n",
      "epoch: 11 batch_num: 74 loss: 1.4454 acc: 1.0\n",
      "epoch: 11 batch_num: 75 loss: 1.1813 acc: 1.0\n",
      "epoch: 11 batch_num: 76 loss: 0.5087 acc: 1.0\n",
      "epoch: 11 batch_num: 77 loss: 1.8176 acc: 0.984375\n",
      "epoch: 11 batch_num: 78 loss: 1.0936 acc: 1.0\n",
      "epoch: 11 batch_num: 79 loss: 2.1604 acc: 0.984375\n",
      "epoch: 11 batch_num: 80 loss: 1.0281 acc: 1.0\n",
      "epoch: 11 batch_num: 81 loss: 1.6826 acc: 0.984375\n",
      "epoch: 11 batch_num: 82 loss: 1.698 acc: 0.984375\n",
      "epoch: 11 batch_num: 83 loss: 1.4362 acc: 1.0\n",
      "epoch: 11 batch_num: 84 loss: 2.0816 acc: 0.984375\n",
      "epoch: 11 batch_num: 85 loss: 1.0001 acc: 1.0\n",
      "epoch: 11 batch_num: 86 loss: 2.3445 acc: 0.96875\n",
      "epoch: 11 batch_num: 87 loss: 0.7385 acc: 1.0\n",
      "epoch: 11 batch_num: 88 loss: 0.8447 acc: 1.0\n",
      "epoch: 11 batch_num: 89 loss: 0.9981 acc: 1.0\n",
      "epoch: 11 batch_num: 90 loss: 0.8443 acc: 1.0\n",
      "epoch: 11 batch_num: 91 loss: 2.6104 acc: 0.984375\n",
      "epoch: 11 batch_num: 92 loss: 1.2833 acc: 1.0\n",
      "epoch: 11 batch_num: 93 loss: 0.9283 acc: 1.0\n",
      "epoch: 11 batch_num: 94 loss: 0.7805 acc: 1.0\n",
      "epoch: 11 batch_num: 95 loss: 1.4254 acc: 1.0\n",
      "epoch: 11 batch_num: 96 loss: 1.5713 acc: 1.0\n",
      "epoch: 11 batch_num: 97 loss: 5.3842 acc: 0.96875\n",
      "epoch: 11 batch_num: 98 loss: 1.4627 acc: 1.0\n",
      "epoch: 11 batch_num: 99 loss: 8.216 acc: 0.96875\n",
      "epoch: 11 batch_num: 100 loss: 1.2207 acc: 1.0\n",
      "epoch: 11 batch_num: 101 loss: 1.0741 acc: 1.0\n",
      "epoch: 11 batch_num: 102 loss: 0.6175 acc: 1.0\n",
      "epoch: 11 batch_num: 103 loss: 1.2283 acc: 1.0\n",
      "epoch: 11 batch_num: 104 loss: 0.7884 acc: 1.0\n",
      "epoch: 11 batch_num: 105 loss: 3.6455 acc: 0.984375\n",
      "epoch: 11 batch_num: 106 loss: 1.0821 acc: 1.0\n",
      "epoch: 11 batch_num: 107 loss: 0.9368 acc: 1.0\n",
      "epoch: 11 batch_num: 108 loss: 1.5376 acc: 1.0\n",
      "epoch: 11 batch_num: 109 loss: 2.6817 acc: 0.984375\n",
      "epoch: 11 batch_num: 110 loss: 1.0297 acc: 1.0\n",
      "epoch: 11 batch_num: 111 loss: 6.4608 acc: 0.96875\n",
      "epoch: 11 batch_num: 112 loss: 1.7975 acc: 0.984375\n",
      "epoch: 11 batch_num: 113 loss: 0.9929 acc: 1.0\n",
      "epoch: 11 batch_num: 114 loss: 1.3146 acc: 1.0\n",
      "epoch: 11 batch_num: 115 loss: 0.8553 acc: 1.0\n",
      "epoch: 11 batch_num: 116 loss: 2.935 acc: 0.9655172413793104\n",
      "epoch: 12 batch_num: 1 loss: 0.6989 acc: 1.0\n",
      "epoch: 12 batch_num: 2 loss: 1.0205 acc: 1.0\n",
      "epoch: 12 batch_num: 3 loss: 0.7605 acc: 1.0\n",
      "epoch: 12 batch_num: 4 loss: 1.222 acc: 1.0\n",
      "epoch: 12 batch_num: 5 loss: 0.5134 acc: 1.0\n",
      "epoch: 12 batch_num: 6 loss: 0.9282 acc: 1.0\n",
      "epoch: 12 batch_num: 7 loss: 1.3692 acc: 1.0\n",
      "epoch: 12 batch_num: 8 loss: 1.249 acc: 1.0\n",
      "epoch: 12 batch_num: 9 loss: 0.9807 acc: 1.0\n",
      "epoch: 12 batch_num: 10 loss: 1.0458 acc: 1.0\n",
      "epoch: 12 batch_num: 11 loss: 0.7248 acc: 1.0\n",
      "epoch: 12 batch_num: 12 loss: 0.398 acc: 1.0\n",
      "epoch: 12 batch_num: 13 loss: 3.7822 acc: 0.96875\n",
      "epoch: 12 batch_num: 14 loss: 2.3364 acc: 0.984375\n",
      "epoch: 12 batch_num: 15 loss: 1.0944 acc: 1.0\n",
      "epoch: 12 batch_num: 16 loss: 1.049 acc: 1.0\n",
      "epoch: 12 batch_num: 17 loss: 1.2408 acc: 1.0\n",
      "epoch: 12 batch_num: 18 loss: 0.4192 acc: 1.0\n",
      "epoch: 12 batch_num: 19 loss: 0.7333 acc: 1.0\n",
      "epoch: 12 batch_num: 20 loss: 0.922 acc: 1.0\n",
      "epoch: 12 batch_num: 21 loss: 1.155 acc: 1.0\n",
      "epoch: 12 batch_num: 22 loss: 0.5667 acc: 1.0\n",
      "epoch: 12 batch_num: 23 loss: 1.6633 acc: 1.0\n",
      "epoch: 12 batch_num: 24 loss: 5.6097 acc: 0.96875\n",
      "epoch: 12 batch_num: 25 loss: 0.4876 acc: 1.0\n",
      "epoch: 12 batch_num: 26 loss: 1.3104 acc: 0.984375\n",
      "epoch: 12 batch_num: 27 loss: 0.8633 acc: 1.0\n",
      "epoch: 12 batch_num: 28 loss: 1.0273 acc: 1.0\n",
      "epoch: 12 batch_num: 29 loss: 0.5842 acc: 1.0\n",
      "epoch: 12 batch_num: 30 loss: 1.1263 acc: 1.0\n",
      "epoch: 12 batch_num: 31 loss: 0.62 acc: 1.0\n",
      "epoch: 12 batch_num: 32 loss: 1.4111 acc: 1.0\n",
      "epoch: 12 batch_num: 33 loss: 0.7917 acc: 1.0\n",
      "epoch: 12 batch_num: 34 loss: 1.3441 acc: 1.0\n",
      "epoch: 12 batch_num: 35 loss: 0.6611 acc: 1.0\n",
      "epoch: 12 batch_num: 36 loss: 0.7504 acc: 1.0\n",
      "epoch: 12 batch_num: 37 loss: 0.8851 acc: 1.0\n",
      "epoch: 12 batch_num: 38 loss: 1.8283 acc: 0.984375\n",
      "epoch: 12 batch_num: 39 loss: 0.4786 acc: 1.0\n",
      "epoch: 12 batch_num: 40 loss: 0.9763 acc: 1.0\n",
      "epoch: 12 batch_num: 41 loss: 2.8065 acc: 0.96875\n",
      "epoch: 12 batch_num: 42 loss: 0.9528 acc: 1.0\n",
      "epoch: 12 batch_num: 43 loss: 4.6531 acc: 0.984375\n",
      "epoch: 12 batch_num: 44 loss: 0.5935 acc: 1.0\n",
      "epoch: 12 batch_num: 45 loss: 0.7072 acc: 1.0\n",
      "epoch: 12 batch_num: 46 loss: 2.9785 acc: 0.984375\n",
      "epoch: 12 batch_num: 47 loss: 0.7213 acc: 1.0\n",
      "epoch: 12 batch_num: 48 loss: 0.5019 acc: 1.0\n",
      "epoch: 12 batch_num: 49 loss: 0.7652 acc: 1.0\n",
      "epoch: 12 batch_num: 50 loss: 0.6727 acc: 1.0\n",
      "epoch: 12 batch_num: 51 loss: 1.3366 acc: 1.0\n",
      "epoch: 12 batch_num: 52 loss: 0.9415 acc: 1.0\n",
      "epoch: 12 batch_num: 53 loss: 1.4041 acc: 0.984375\n",
      "epoch: 12 batch_num: 54 loss: 1.0627 acc: 1.0\n",
      "epoch: 12 batch_num: 55 loss: 1.1839 acc: 1.0\n",
      "epoch: 12 batch_num: 56 loss: 1.2512 acc: 1.0\n",
      "epoch: 12 batch_num: 57 loss: 1.0621 acc: 1.0\n",
      "epoch: 12 batch_num: 58 loss: 0.6146 acc: 1.0\n",
      "epoch: 12 batch_num: 59 loss: 1.3341 acc: 1.0\n",
      "epoch: 12 batch_num: 60 loss: 12.3101 acc: 0.96875\n",
      "epoch: 12 batch_num: 61 loss: 1.1763 acc: 1.0\n",
      "epoch: 12 batch_num: 62 loss: 0.5988 acc: 1.0\n",
      "epoch: 12 batch_num: 63 loss: 0.6381 acc: 1.0\n",
      "epoch: 12 batch_num: 64 loss: 1.0091 acc: 1.0\n",
      "epoch: 12 batch_num: 65 loss: 0.6362 acc: 1.0\n",
      "epoch: 12 batch_num: 66 loss: 0.6383 acc: 1.0\n",
      "epoch: 12 batch_num: 67 loss: 0.9103 acc: 1.0\n",
      "epoch: 12 batch_num: 68 loss: 0.7166 acc: 1.0\n",
      "epoch: 12 batch_num: 69 loss: 0.5637 acc: 1.0\n",
      "epoch: 12 batch_num: 70 loss: 0.813 acc: 1.0\n",
      "epoch: 12 batch_num: 71 loss: 0.5739 acc: 1.0\n",
      "epoch: 12 batch_num: 72 loss: 0.4516 acc: 1.0\n",
      "epoch: 12 batch_num: 73 loss: 1.0424 acc: 1.0\n",
      "epoch: 12 batch_num: 74 loss: 3.1233 acc: 0.984375\n",
      "epoch: 12 batch_num: 75 loss: 0.5181 acc: 1.0\n",
      "epoch: 12 batch_num: 76 loss: 1.7505 acc: 1.0\n",
      "epoch: 12 batch_num: 77 loss: 0.9916 acc: 1.0\n",
      "epoch: 12 batch_num: 78 loss: 1.9584 acc: 0.984375\n",
      "epoch: 12 batch_num: 79 loss: 6.2008 acc: 0.96875\n",
      "epoch: 12 batch_num: 80 loss: 0.8364 acc: 1.0\n",
      "epoch: 12 batch_num: 81 loss: 1.3671 acc: 1.0\n",
      "epoch: 12 batch_num: 82 loss: 1.5089 acc: 1.0\n",
      "epoch: 12 batch_num: 83 loss: 0.6192 acc: 1.0\n",
      "epoch: 12 batch_num: 84 loss: 1.2704 acc: 1.0\n",
      "epoch: 12 batch_num: 85 loss: 0.6062 acc: 1.0\n",
      "epoch: 12 batch_num: 86 loss: 1.0571 acc: 1.0\n",
      "epoch: 12 batch_num: 87 loss: 1.0495 acc: 1.0\n",
      "epoch: 12 batch_num: 88 loss: 2.7396 acc: 0.984375\n",
      "epoch: 12 batch_num: 89 loss: 0.5869 acc: 1.0\n",
      "epoch: 12 batch_num: 90 loss: 1.3164 acc: 1.0\n",
      "epoch: 12 batch_num: 91 loss: 0.9038 acc: 1.0\n",
      "epoch: 12 batch_num: 92 loss: 1.307 acc: 0.984375\n",
      "epoch: 12 batch_num: 93 loss: 2.6992 acc: 0.984375\n",
      "epoch: 12 batch_num: 94 loss: 0.7009 acc: 1.0\n",
      "epoch: 12 batch_num: 95 loss: 2.8118 acc: 0.984375\n",
      "epoch: 12 batch_num: 96 loss: 1.0525 acc: 1.0\n",
      "epoch: 12 batch_num: 97 loss: 3.5843 acc: 0.984375\n",
      "epoch: 12 batch_num: 98 loss: 0.9047 acc: 1.0\n",
      "epoch: 12 batch_num: 99 loss: 0.9247 acc: 1.0\n",
      "epoch: 12 batch_num: 100 loss: 1.1299 acc: 1.0\n",
      "epoch: 12 batch_num: 101 loss: 2.0565 acc: 0.984375\n",
      "epoch: 12 batch_num: 102 loss: 0.4201 acc: 1.0\n",
      "epoch: 12 batch_num: 103 loss: 0.6666 acc: 1.0\n",
      "epoch: 12 batch_num: 104 loss: 0.7064 acc: 1.0\n",
      "epoch: 12 batch_num: 105 loss: 1.1139 acc: 1.0\n",
      "epoch: 12 batch_num: 106 loss: 2.0787 acc: 0.984375\n",
      "epoch: 12 batch_num: 107 loss: 0.4959 acc: 1.0\n",
      "epoch: 12 batch_num: 108 loss: 5.856 acc: 0.96875\n",
      "epoch: 12 batch_num: 109 loss: 0.6766 acc: 1.0\n",
      "epoch: 12 batch_num: 110 loss: 1.1049 acc: 1.0\n",
      "epoch: 12 batch_num: 111 loss: 6.3154 acc: 0.96875\n",
      "epoch: 12 batch_num: 112 loss: 0.7675 acc: 1.0\n",
      "epoch: 12 batch_num: 113 loss: 1.0109 acc: 1.0\n",
      "epoch: 12 batch_num: 114 loss: 2.2458 acc: 0.984375\n",
      "epoch: 12 batch_num: 115 loss: 1.5228 acc: 1.0\n",
      "epoch: 12 batch_num: 116 loss: 0.7002 acc: 1.0\n",
      "epoch: 13 batch_num: 1 loss: 0.475 acc: 1.0\n",
      "epoch: 13 batch_num: 2 loss: 0.7391 acc: 1.0\n",
      "epoch: 13 batch_num: 3 loss: 1.2378 acc: 0.984375\n",
      "epoch: 13 batch_num: 4 loss: 0.8191 acc: 1.0\n",
      "epoch: 13 batch_num: 5 loss: 0.5698 acc: 1.0\n",
      "epoch: 13 batch_num: 6 loss: 1.0075 acc: 1.0\n",
      "epoch: 13 batch_num: 7 loss: 0.7495 acc: 1.0\n",
      "epoch: 13 batch_num: 8 loss: 0.6578 acc: 1.0\n",
      "epoch: 13 batch_num: 9 loss: 0.7872 acc: 1.0\n",
      "epoch: 13 batch_num: 10 loss: 0.81 acc: 1.0\n",
      "epoch: 13 batch_num: 11 loss: 0.7072 acc: 1.0\n",
      "epoch: 13 batch_num: 12 loss: 0.528 acc: 1.0\n",
      "epoch: 13 batch_num: 13 loss: 1.3889 acc: 1.0\n",
      "epoch: 13 batch_num: 14 loss: 4.1377 acc: 0.984375\n",
      "epoch: 13 batch_num: 15 loss: 5.5381 acc: 0.984375\n",
      "epoch: 13 batch_num: 16 loss: 0.4125 acc: 1.0\n",
      "epoch: 13 batch_num: 17 loss: 0.5631 acc: 1.0\n",
      "epoch: 13 batch_num: 18 loss: 0.4346 acc: 1.0\n",
      "epoch: 13 batch_num: 19 loss: 2.5464 acc: 0.984375\n",
      "epoch: 13 batch_num: 20 loss: 0.7987 acc: 1.0\n",
      "epoch: 13 batch_num: 21 loss: 0.7148 acc: 1.0\n",
      "epoch: 13 batch_num: 22 loss: 0.7422 acc: 1.0\n",
      "epoch: 13 batch_num: 23 loss: 0.5344 acc: 1.0\n",
      "epoch: 13 batch_num: 24 loss: 0.778 acc: 1.0\n",
      "epoch: 13 batch_num: 25 loss: 1.0699 acc: 1.0\n",
      "epoch: 13 batch_num: 26 loss: 1.4928 acc: 1.0\n",
      "epoch: 13 batch_num: 27 loss: 2.3227 acc: 0.984375\n",
      "epoch: 13 batch_num: 28 loss: 0.5913 acc: 1.0\n",
      "epoch: 13 batch_num: 29 loss: 0.923 acc: 1.0\n",
      "epoch: 13 batch_num: 30 loss: 0.5697 acc: 1.0\n",
      "epoch: 13 batch_num: 31 loss: 1.5171 acc: 1.0\n",
      "epoch: 13 batch_num: 32 loss: 3.4941 acc: 0.984375\n",
      "epoch: 13 batch_num: 33 loss: 0.5873 acc: 1.0\n",
      "epoch: 13 batch_num: 34 loss: 0.4576 acc: 1.0\n",
      "epoch: 13 batch_num: 35 loss: 1.1018 acc: 1.0\n",
      "epoch: 13 batch_num: 36 loss: 0.8072 acc: 1.0\n",
      "epoch: 13 batch_num: 37 loss: 0.7843 acc: 1.0\n",
      "epoch: 13 batch_num: 38 loss: 1.3237 acc: 1.0\n",
      "epoch: 13 batch_num: 39 loss: 1.1554 acc: 1.0\n",
      "epoch: 13 batch_num: 40 loss: 1.0505 acc: 1.0\n",
      "epoch: 13 batch_num: 41 loss: 0.8868 acc: 1.0\n",
      "epoch: 13 batch_num: 42 loss: 0.5581 acc: 1.0\n",
      "epoch: 13 batch_num: 43 loss: 1.3574 acc: 1.0\n",
      "epoch: 13 batch_num: 44 loss: 0.6091 acc: 1.0\n",
      "epoch: 13 batch_num: 45 loss: 0.5648 acc: 1.0\n",
      "epoch: 13 batch_num: 46 loss: 1.0014 acc: 1.0\n",
      "epoch: 13 batch_num: 47 loss: 0.5696 acc: 1.0\n",
      "epoch: 13 batch_num: 48 loss: 4.8814 acc: 0.984375\n",
      "epoch: 13 batch_num: 49 loss: 0.4664 acc: 1.0\n",
      "epoch: 13 batch_num: 50 loss: 0.8504 acc: 1.0\n",
      "epoch: 13 batch_num: 51 loss: 0.6057 acc: 1.0\n",
      "epoch: 13 batch_num: 52 loss: 0.5956 acc: 1.0\n",
      "epoch: 13 batch_num: 53 loss: 0.3945 acc: 1.0\n",
      "epoch: 13 batch_num: 54 loss: 0.4418 acc: 1.0\n",
      "epoch: 13 batch_num: 55 loss: 0.3298 acc: 1.0\n",
      "epoch: 13 batch_num: 56 loss: 2.4153 acc: 0.984375\n",
      "epoch: 13 batch_num: 57 loss: 1.9197 acc: 0.984375\n",
      "epoch: 13 batch_num: 58 loss: 0.9831 acc: 1.0\n",
      "epoch: 13 batch_num: 59 loss: 1.0888 acc: 1.0\n",
      "epoch: 13 batch_num: 60 loss: 0.3898 acc: 1.0\n",
      "epoch: 13 batch_num: 61 loss: 3.8398 acc: 0.984375\n",
      "epoch: 13 batch_num: 62 loss: 0.8104 acc: 1.0\n",
      "epoch: 13 batch_num: 63 loss: 1.3499 acc: 0.984375\n",
      "epoch: 13 batch_num: 64 loss: 1.4482 acc: 1.0\n",
      "epoch: 13 batch_num: 65 loss: 0.8773 acc: 1.0\n",
      "epoch: 13 batch_num: 66 loss: 0.5968 acc: 1.0\n",
      "epoch: 13 batch_num: 67 loss: 3.0243 acc: 0.984375\n",
      "epoch: 13 batch_num: 68 loss: 1.4989 acc: 1.0\n",
      "epoch: 13 batch_num: 69 loss: 0.9987 acc: 1.0\n",
      "epoch: 13 batch_num: 70 loss: 0.7374 acc: 1.0\n",
      "epoch: 13 batch_num: 71 loss: 0.6415 acc: 1.0\n",
      "epoch: 13 batch_num: 72 loss: 2.8397 acc: 0.984375\n",
      "epoch: 13 batch_num: 73 loss: 0.8925 acc: 1.0\n",
      "epoch: 13 batch_num: 74 loss: 1.1474 acc: 1.0\n",
      "epoch: 13 batch_num: 75 loss: 0.6635 acc: 1.0\n",
      "epoch: 13 batch_num: 76 loss: 0.9935 acc: 1.0\n",
      "epoch: 13 batch_num: 77 loss: 3.5595 acc: 0.984375\n",
      "epoch: 13 batch_num: 78 loss: 0.4696 acc: 1.0\n",
      "epoch: 13 batch_num: 79 loss: 1.24 acc: 1.0\n",
      "epoch: 13 batch_num: 80 loss: 0.3558 acc: 1.0\n",
      "epoch: 13 batch_num: 81 loss: 5.7691 acc: 0.984375\n",
      "epoch: 13 batch_num: 82 loss: 0.526 acc: 1.0\n",
      "epoch: 13 batch_num: 83 loss: 0.984 acc: 1.0\n",
      "epoch: 13 batch_num: 84 loss: 1.629 acc: 0.984375\n",
      "epoch: 13 batch_num: 85 loss: 0.5652 acc: 1.0\n",
      "epoch: 13 batch_num: 86 loss: 1.3688 acc: 1.0\n",
      "epoch: 13 batch_num: 87 loss: 0.5862 acc: 1.0\n",
      "epoch: 13 batch_num: 88 loss: 1.0851 acc: 1.0\n",
      "epoch: 13 batch_num: 89 loss: 0.5948 acc: 1.0\n",
      "epoch: 13 batch_num: 90 loss: 0.8388 acc: 1.0\n",
      "epoch: 13 batch_num: 91 loss: 2.3184 acc: 0.984375\n",
      "epoch: 13 batch_num: 92 loss: 0.879 acc: 1.0\n",
      "epoch: 13 batch_num: 93 loss: 1.3863 acc: 0.984375\n",
      "epoch: 13 batch_num: 94 loss: 0.8635 acc: 1.0\n",
      "epoch: 13 batch_num: 95 loss: 1.758 acc: 0.984375\n",
      "epoch: 13 batch_num: 96 loss: 0.516 acc: 1.0\n",
      "epoch: 13 batch_num: 97 loss: 0.5226 acc: 1.0\n",
      "epoch: 13 batch_num: 98 loss: 0.7907 acc: 1.0\n",
      "epoch: 13 batch_num: 99 loss: 3.5175 acc: 0.984375\n",
      "epoch: 13 batch_num: 100 loss: 0.5201 acc: 1.0\n",
      "epoch: 13 batch_num: 101 loss: 2.0766 acc: 0.984375\n",
      "epoch: 13 batch_num: 102 loss: 0.6451 acc: 1.0\n",
      "epoch: 13 batch_num: 103 loss: 1.3171 acc: 0.984375\n",
      "epoch: 13 batch_num: 104 loss: 0.892 acc: 1.0\n",
      "epoch: 13 batch_num: 105 loss: 0.7852 acc: 1.0\n",
      "epoch: 13 batch_num: 106 loss: 1.4328 acc: 1.0\n",
      "epoch: 13 batch_num: 107 loss: 0.6439 acc: 1.0\n",
      "epoch: 13 batch_num: 108 loss: 0.513 acc: 1.0\n",
      "epoch: 13 batch_num: 109 loss: 0.7227 acc: 1.0\n",
      "epoch: 13 batch_num: 110 loss: 0.522 acc: 1.0\n",
      "epoch: 13 batch_num: 111 loss: 0.9029 acc: 1.0\n",
      "epoch: 13 batch_num: 112 loss: 0.6223 acc: 1.0\n",
      "epoch: 13 batch_num: 113 loss: 5.4222 acc: 0.96875\n",
      "epoch: 13 batch_num: 114 loss: 0.4903 acc: 1.0\n",
      "epoch: 13 batch_num: 115 loss: 0.8664 acc: 1.0\n",
      "epoch: 13 batch_num: 116 loss: 0.2075 acc: 1.0\n",
      "epoch: 14 batch_num: 1 loss: 0.5018 acc: 1.0\n",
      "epoch: 14 batch_num: 2 loss: 1.1033 acc: 1.0\n",
      "epoch: 14 batch_num: 3 loss: 0.9981 acc: 1.0\n",
      "epoch: 14 batch_num: 4 loss: 0.6999 acc: 1.0\n",
      "epoch: 14 batch_num: 5 loss: 0.8492 acc: 1.0\n",
      "epoch: 14 batch_num: 6 loss: 0.4681 acc: 1.0\n",
      "epoch: 14 batch_num: 7 loss: 0.3993 acc: 1.0\n",
      "epoch: 14 batch_num: 8 loss: 0.5624 acc: 1.0\n",
      "epoch: 14 batch_num: 9 loss: 0.5117 acc: 1.0\n",
      "epoch: 14 batch_num: 10 loss: 0.7666 acc: 1.0\n",
      "epoch: 14 batch_num: 11 loss: 0.3794 acc: 1.0\n",
      "epoch: 14 batch_num: 12 loss: 0.8005 acc: 1.0\n",
      "epoch: 14 batch_num: 13 loss: 0.4252 acc: 1.0\n",
      "epoch: 14 batch_num: 14 loss: 0.5335 acc: 1.0\n",
      "epoch: 14 batch_num: 15 loss: 0.6051 acc: 1.0\n",
      "epoch: 14 batch_num: 16 loss: 0.883 acc: 1.0\n",
      "epoch: 14 batch_num: 17 loss: 0.6117 acc: 1.0\n",
      "epoch: 14 batch_num: 18 loss: 0.5759 acc: 1.0\n",
      "epoch: 14 batch_num: 19 loss: 0.5274 acc: 1.0\n",
      "epoch: 14 batch_num: 20 loss: 0.6619 acc: 1.0\n",
      "epoch: 14 batch_num: 21 loss: 0.4548 acc: 1.0\n",
      "epoch: 14 batch_num: 22 loss: 2.265 acc: 0.984375\n",
      "epoch: 14 batch_num: 23 loss: 0.7039 acc: 1.0\n",
      "epoch: 14 batch_num: 24 loss: 0.4371 acc: 1.0\n",
      "epoch: 14 batch_num: 25 loss: 0.6961 acc: 1.0\n",
      "epoch: 14 batch_num: 26 loss: 0.3868 acc: 1.0\n",
      "epoch: 14 batch_num: 27 loss: 0.6514 acc: 1.0\n",
      "epoch: 14 batch_num: 28 loss: 4.16 acc: 0.96875\n",
      "epoch: 14 batch_num: 29 loss: 0.451 acc: 1.0\n",
      "epoch: 14 batch_num: 30 loss: 2.2528 acc: 0.984375\n",
      "epoch: 14 batch_num: 31 loss: 0.7719 acc: 1.0\n",
      "epoch: 14 batch_num: 32 loss: 1.3165 acc: 1.0\n",
      "epoch: 14 batch_num: 33 loss: 0.4437 acc: 1.0\n",
      "epoch: 14 batch_num: 34 loss: 0.5605 acc: 1.0\n",
      "epoch: 14 batch_num: 35 loss: 3.418 acc: 0.984375\n",
      "epoch: 14 batch_num: 36 loss: 0.4464 acc: 1.0\n",
      "epoch: 14 batch_num: 37 loss: 0.5888 acc: 1.0\n",
      "epoch: 14 batch_num: 38 loss: 0.5058 acc: 1.0\n",
      "epoch: 14 batch_num: 39 loss: 0.4937 acc: 1.0\n",
      "epoch: 14 batch_num: 40 loss: 0.7474 acc: 1.0\n",
      "epoch: 14 batch_num: 41 loss: 0.9546 acc: 1.0\n",
      "epoch: 14 batch_num: 42 loss: 0.4794 acc: 1.0\n",
      "epoch: 14 batch_num: 43 loss: 0.678 acc: 1.0\n",
      "epoch: 14 batch_num: 44 loss: 1.7792 acc: 0.984375\n",
      "epoch: 14 batch_num: 45 loss: 0.7665 acc: 1.0\n",
      "epoch: 14 batch_num: 46 loss: 0.6172 acc: 1.0\n",
      "epoch: 14 batch_num: 47 loss: 3.593 acc: 0.984375\n",
      "epoch: 14 batch_num: 48 loss: 1.0936 acc: 0.984375\n",
      "epoch: 14 batch_num: 49 loss: 1.2283 acc: 0.984375\n",
      "epoch: 14 batch_num: 50 loss: 0.6326 acc: 1.0\n",
      "epoch: 14 batch_num: 51 loss: 0.5606 acc: 1.0\n",
      "epoch: 14 batch_num: 52 loss: 1.3141 acc: 0.984375\n",
      "epoch: 14 batch_num: 53 loss: 0.489 acc: 1.0\n",
      "epoch: 14 batch_num: 54 loss: 0.4095 acc: 1.0\n",
      "epoch: 14 batch_num: 55 loss: 0.5971 acc: 1.0\n",
      "epoch: 14 batch_num: 56 loss: 0.4309 acc: 1.0\n",
      "epoch: 14 batch_num: 57 loss: 0.4937 acc: 1.0\n",
      "epoch: 14 batch_num: 58 loss: 0.9216 acc: 1.0\n",
      "epoch: 14 batch_num: 59 loss: 0.5731 acc: 1.0\n",
      "epoch: 14 batch_num: 60 loss: 0.3088 acc: 1.0\n",
      "epoch: 14 batch_num: 61 loss: 0.4443 acc: 1.0\n",
      "epoch: 14 batch_num: 62 loss: 0.2723 acc: 1.0\n",
      "epoch: 14 batch_num: 63 loss: 0.2591 acc: 1.0\n",
      "epoch: 14 batch_num: 64 loss: 1.0007 acc: 1.0\n",
      "epoch: 14 batch_num: 65 loss: 1.537 acc: 0.984375\n",
      "epoch: 14 batch_num: 66 loss: 1.5908 acc: 0.984375\n",
      "epoch: 14 batch_num: 67 loss: 0.7741 acc: 1.0\n",
      "epoch: 14 batch_num: 68 loss: 0.308 acc: 1.0\n",
      "epoch: 14 batch_num: 69 loss: 0.7355 acc: 1.0\n",
      "epoch: 14 batch_num: 70 loss: 0.6904 acc: 1.0\n",
      "epoch: 14 batch_num: 71 loss: 6.5625 acc: 0.984375\n",
      "epoch: 14 batch_num: 72 loss: 1.9091 acc: 0.984375\n",
      "epoch: 14 batch_num: 73 loss: 0.4512 acc: 1.0\n",
      "epoch: 14 batch_num: 74 loss: 0.5472 acc: 1.0\n",
      "epoch: 14 batch_num: 75 loss: 0.5365 acc: 1.0\n",
      "epoch: 14 batch_num: 76 loss: 10.0008 acc: 0.984375\n",
      "epoch: 14 batch_num: 77 loss: 4.7119 acc: 0.96875\n",
      "epoch: 14 batch_num: 78 loss: 0.3793 acc: 1.0\n",
      "epoch: 14 batch_num: 79 loss: 0.5135 acc: 1.0\n",
      "epoch: 14 batch_num: 80 loss: 0.6737 acc: 1.0\n",
      "epoch: 14 batch_num: 81 loss: 0.5114 acc: 1.0\n",
      "epoch: 14 batch_num: 82 loss: 0.7365 acc: 1.0\n",
      "epoch: 14 batch_num: 83 loss: 0.5414 acc: 1.0\n",
      "epoch: 14 batch_num: 84 loss: 0.3607 acc: 1.0\n",
      "epoch: 14 batch_num: 85 loss: 0.722 acc: 1.0\n",
      "epoch: 14 batch_num: 86 loss: 0.9615 acc: 1.0\n",
      "epoch: 14 batch_num: 87 loss: 0.5084 acc: 1.0\n",
      "epoch: 14 batch_num: 88 loss: 0.4096 acc: 1.0\n",
      "epoch: 14 batch_num: 89 loss: 0.6756 acc: 1.0\n",
      "epoch: 14 batch_num: 90 loss: 1.22 acc: 0.984375\n",
      "epoch: 14 batch_num: 91 loss: 0.8154 acc: 1.0\n",
      "epoch: 14 batch_num: 92 loss: 2.5769 acc: 0.984375\n",
      "epoch: 14 batch_num: 93 loss: 0.8586 acc: 1.0\n",
      "epoch: 14 batch_num: 94 loss: 1.2656 acc: 1.0\n",
      "epoch: 14 batch_num: 95 loss: 0.649 acc: 1.0\n",
      "epoch: 14 batch_num: 96 loss: 0.3541 acc: 1.0\n",
      "epoch: 14 batch_num: 97 loss: 0.3185 acc: 1.0\n",
      "epoch: 14 batch_num: 98 loss: 1.1319 acc: 0.984375\n",
      "epoch: 14 batch_num: 99 loss: 0.6868 acc: 1.0\n",
      "epoch: 14 batch_num: 100 loss: 2.0594 acc: 0.984375\n",
      "epoch: 14 batch_num: 101 loss: 2.2316 acc: 0.984375\n",
      "epoch: 14 batch_num: 102 loss: 1.2296 acc: 0.984375\n",
      "epoch: 14 batch_num: 103 loss: 0.7297 acc: 1.0\n",
      "epoch: 14 batch_num: 104 loss: 0.6339 acc: 1.0\n",
      "epoch: 14 batch_num: 105 loss: 1.3594 acc: 1.0\n",
      "epoch: 14 batch_num: 106 loss: 1.8979 acc: 1.0\n",
      "epoch: 14 batch_num: 107 loss: 1.9332 acc: 1.0\n",
      "epoch: 14 batch_num: 108 loss: 14.2188 acc: 0.9375\n",
      "epoch: 14 batch_num: 109 loss: 1.5196 acc: 1.0\n",
      "epoch: 14 batch_num: 110 loss: 1.0943 acc: 1.0\n",
      "epoch: 14 batch_num: 111 loss: 1.3038 acc: 1.0\n",
      "epoch: 14 batch_num: 112 loss: 0.335 acc: 1.0\n",
      "epoch: 14 batch_num: 113 loss: 1.4478 acc: 1.0\n",
      "epoch: 14 batch_num: 114 loss: 6.2208 acc: 0.984375\n",
      "epoch: 14 batch_num: 115 loss: 1.5043 acc: 1.0\n",
      "epoch: 14 batch_num: 116 loss: 1.035 acc: 1.0\n",
      "epoch: 15 batch_num: 1 loss: 2.3754 acc: 0.984375\n",
      "epoch: 15 batch_num: 2 loss: 1.5852 acc: 1.0\n",
      "epoch: 15 batch_num: 3 loss: 1.6438 acc: 1.0\n",
      "epoch: 15 batch_num: 4 loss: 1.869 acc: 1.0\n",
      "epoch: 15 batch_num: 5 loss: 2.1126 acc: 1.0\n",
      "epoch: 15 batch_num: 6 loss: 0.8447 acc: 1.0\n",
      "epoch: 15 batch_num: 7 loss: 0.5078 acc: 1.0\n",
      "epoch: 15 batch_num: 8 loss: 0.6163 acc: 1.0\n",
      "epoch: 15 batch_num: 9 loss: 0.5055 acc: 1.0\n",
      "epoch: 15 batch_num: 10 loss: 0.395 acc: 1.0\n",
      "epoch: 15 batch_num: 11 loss: 0.265 acc: 1.0\n",
      "epoch: 15 batch_num: 12 loss: 0.5648 acc: 1.0\n",
      "epoch: 15 batch_num: 13 loss: 1.0787 acc: 1.0\n",
      "epoch: 15 batch_num: 14 loss: 1.0888 acc: 1.0\n",
      "epoch: 15 batch_num: 15 loss: 0.5188 acc: 1.0\n",
      "epoch: 15 batch_num: 16 loss: 0.4682 acc: 1.0\n",
      "epoch: 15 batch_num: 17 loss: 0.8107 acc: 1.0\n",
      "epoch: 15 batch_num: 18 loss: 2.5042 acc: 0.984375\n",
      "epoch: 15 batch_num: 19 loss: 0.6001 acc: 1.0\n",
      "epoch: 15 batch_num: 20 loss: 0.9918 acc: 1.0\n",
      "epoch: 15 batch_num: 21 loss: 2.0323 acc: 0.984375\n",
      "epoch: 15 batch_num: 22 loss: 0.5857 acc: 1.0\n",
      "epoch: 15 batch_num: 23 loss: 0.4319 acc: 1.0\n",
      "epoch: 15 batch_num: 24 loss: 7.958 acc: 0.984375\n",
      "epoch: 15 batch_num: 25 loss: 1.3069 acc: 1.0\n",
      "epoch: 15 batch_num: 26 loss: 0.5326 acc: 1.0\n",
      "epoch: 15 batch_num: 27 loss: 0.4963 acc: 1.0\n",
      "epoch: 15 batch_num: 28 loss: 0.3744 acc: 1.0\n",
      "epoch: 15 batch_num: 29 loss: 0.616 acc: 1.0\n",
      "epoch: 15 batch_num: 30 loss: 1.1781 acc: 1.0\n",
      "epoch: 15 batch_num: 31 loss: 0.4569 acc: 1.0\n",
      "epoch: 15 batch_num: 32 loss: 3.5936 acc: 0.984375\n",
      "epoch: 15 batch_num: 33 loss: 0.4041 acc: 1.0\n",
      "epoch: 15 batch_num: 34 loss: 0.3771 acc: 1.0\n",
      "epoch: 15 batch_num: 35 loss: 0.4613 acc: 1.0\n",
      "epoch: 15 batch_num: 36 loss: 0.5463 acc: 1.0\n",
      "epoch: 15 batch_num: 37 loss: 0.9806 acc: 1.0\n",
      "epoch: 15 batch_num: 38 loss: 0.5358 acc: 1.0\n",
      "epoch: 15 batch_num: 39 loss: 5.4451 acc: 0.984375\n",
      "epoch: 15 batch_num: 40 loss: 1.0068 acc: 1.0\n",
      "epoch: 15 batch_num: 41 loss: 0.9874 acc: 1.0\n",
      "epoch: 15 batch_num: 42 loss: 0.5897 acc: 1.0\n",
      "epoch: 15 batch_num: 43 loss: 2.9188 acc: 0.984375\n",
      "epoch: 15 batch_num: 44 loss: 0.6031 acc: 1.0\n",
      "epoch: 15 batch_num: 45 loss: 0.437 acc: 1.0\n",
      "epoch: 15 batch_num: 46 loss: 0.5624 acc: 1.0\n",
      "epoch: 15 batch_num: 47 loss: 1.051 acc: 1.0\n",
      "epoch: 15 batch_num: 48 loss: 0.9296 acc: 1.0\n",
      "epoch: 15 batch_num: 49 loss: 0.4213 acc: 1.0\n",
      "epoch: 15 batch_num: 50 loss: 0.5125 acc: 1.0\n",
      "epoch: 15 batch_num: 51 loss: 0.3618 acc: 1.0\n",
      "epoch: 15 batch_num: 52 loss: 1.1804 acc: 0.984375\n",
      "epoch: 15 batch_num: 53 loss: 0.9901 acc: 1.0\n",
      "epoch: 15 batch_num: 54 loss: 0.3264 acc: 1.0\n",
      "epoch: 15 batch_num: 55 loss: 0.6314 acc: 1.0\n",
      "epoch: 15 batch_num: 56 loss: 0.4797 acc: 1.0\n",
      "epoch: 15 batch_num: 57 loss: 0.6901 acc: 1.0\n",
      "epoch: 15 batch_num: 58 loss: 1.475 acc: 0.984375\n",
      "epoch: 15 batch_num: 59 loss: 0.3667 acc: 1.0\n",
      "epoch: 15 batch_num: 60 loss: 0.7441 acc: 1.0\n",
      "epoch: 15 batch_num: 61 loss: 0.3521 acc: 1.0\n",
      "epoch: 15 batch_num: 62 loss: 7.2903 acc: 0.984375\n",
      "epoch: 15 batch_num: 63 loss: 2.2996 acc: 0.984375\n",
      "epoch: 15 batch_num: 64 loss: 0.6312 acc: 1.0\n",
      "epoch: 15 batch_num: 65 loss: 1.8741 acc: 0.984375\n",
      "epoch: 15 batch_num: 66 loss: 0.5844 acc: 1.0\n",
      "epoch: 15 batch_num: 67 loss: 0.6606 acc: 1.0\n",
      "epoch: 15 batch_num: 68 loss: 0.8998 acc: 1.0\n",
      "epoch: 15 batch_num: 69 loss: 0.9122 acc: 1.0\n",
      "epoch: 15 batch_num: 70 loss: 0.5424 acc: 1.0\n",
      "epoch: 15 batch_num: 71 loss: 1.0013 acc: 1.0\n",
      "epoch: 15 batch_num: 72 loss: 0.4742 acc: 1.0\n",
      "epoch: 15 batch_num: 73 loss: 0.3401 acc: 1.0\n",
      "epoch: 15 batch_num: 74 loss: 0.5028 acc: 1.0\n",
      "epoch: 15 batch_num: 75 loss: 1.8071 acc: 0.984375\n",
      "epoch: 15 batch_num: 76 loss: 0.6404 acc: 1.0\n",
      "epoch: 15 batch_num: 77 loss: 0.7474 acc: 1.0\n",
      "epoch: 15 batch_num: 78 loss: 0.5392 acc: 1.0\n",
      "epoch: 15 batch_num: 79 loss: 0.4968 acc: 1.0\n",
      "epoch: 15 batch_num: 80 loss: 0.6478 acc: 1.0\n",
      "epoch: 15 batch_num: 81 loss: 0.5027 acc: 1.0\n",
      "epoch: 15 batch_num: 82 loss: 0.9332 acc: 1.0\n",
      "epoch: 15 batch_num: 83 loss: 2.4296 acc: 0.984375\n",
      "epoch: 15 batch_num: 84 loss: 0.5243 acc: 1.0\n",
      "epoch: 15 batch_num: 85 loss: 0.767 acc: 1.0\n",
      "epoch: 15 batch_num: 86 loss: 0.5562 acc: 1.0\n",
      "epoch: 15 batch_num: 87 loss: 0.6244 acc: 1.0\n",
      "epoch: 15 batch_num: 88 loss: 0.706 acc: 1.0\n",
      "epoch: 15 batch_num: 89 loss: 1.5181 acc: 0.984375\n",
      "epoch: 15 batch_num: 90 loss: 3.4722 acc: 0.96875\n",
      "epoch: 15 batch_num: 91 loss: 0.3902 acc: 1.0\n",
      "epoch: 15 batch_num: 92 loss: 0.5709 acc: 1.0\n",
      "epoch: 15 batch_num: 93 loss: 2.54 acc: 0.984375\n",
      "epoch: 15 batch_num: 94 loss: 0.5807 acc: 1.0\n",
      "epoch: 15 batch_num: 95 loss: 0.4789 acc: 1.0\n",
      "epoch: 15 batch_num: 96 loss: 2.7506 acc: 0.984375\n",
      "epoch: 15 batch_num: 97 loss: 2.8069 acc: 0.984375\n",
      "epoch: 15 batch_num: 98 loss: 0.5137 acc: 1.0\n",
      "epoch: 15 batch_num: 99 loss: 1.1096 acc: 0.984375\n",
      "epoch: 15 batch_num: 100 loss: 0.4413 acc: 1.0\n",
      "epoch: 15 batch_num: 101 loss: 3.3533 acc: 0.984375\n",
      "epoch: 15 batch_num: 102 loss: 0.2636 acc: 1.0\n",
      "epoch: 15 batch_num: 103 loss: 1.1319 acc: 1.0\n",
      "epoch: 15 batch_num: 104 loss: 0.8226 acc: 1.0\n",
      "epoch: 15 batch_num: 105 loss: 0.8758 acc: 1.0\n",
      "epoch: 15 batch_num: 106 loss: 1.5143 acc: 0.984375\n",
      "epoch: 15 batch_num: 107 loss: 0.7523 acc: 1.0\n",
      "epoch: 15 batch_num: 108 loss: 0.5034 acc: 1.0\n",
      "epoch: 15 batch_num: 109 loss: 0.7164 acc: 1.0\n",
      "epoch: 15 batch_num: 110 loss: 0.5054 acc: 1.0\n",
      "epoch: 15 batch_num: 111 loss: 1.0335 acc: 1.0\n",
      "epoch: 15 batch_num: 112 loss: 2.9877 acc: 0.984375\n",
      "epoch: 15 batch_num: 113 loss: 0.5058 acc: 1.0\n",
      "epoch: 15 batch_num: 114 loss: 0.3333 acc: 1.0\n",
      "epoch: 15 batch_num: 115 loss: 2.0317 acc: 0.984375\n",
      "epoch: 15 batch_num: 116 loss: 0.2597 acc: 1.0\n",
      "epoch: 16 batch_num: 1 loss: 0.3505 acc: 1.0\n",
      "epoch: 16 batch_num: 2 loss: 0.6207 acc: 1.0\n",
      "epoch: 16 batch_num: 3 loss: 0.4484 acc: 1.0\n",
      "epoch: 16 batch_num: 4 loss: 0.496 acc: 1.0\n",
      "epoch: 16 batch_num: 5 loss: 0.5171 acc: 1.0\n",
      "epoch: 16 batch_num: 6 loss: 0.3546 acc: 1.0\n",
      "epoch: 16 batch_num: 7 loss: 0.478 acc: 1.0\n",
      "epoch: 16 batch_num: 8 loss: 0.4905 acc: 1.0\n",
      "epoch: 16 batch_num: 9 loss: 0.6533 acc: 1.0\n",
      "epoch: 16 batch_num: 10 loss: 0.4628 acc: 1.0\n",
      "epoch: 16 batch_num: 11 loss: 0.3813 acc: 1.0\n",
      "epoch: 16 batch_num: 12 loss: 0.3291 acc: 1.0\n",
      "epoch: 16 batch_num: 13 loss: 1.1966 acc: 0.984375\n",
      "epoch: 16 batch_num: 14 loss: 0.3688 acc: 1.0\n",
      "epoch: 16 batch_num: 15 loss: 0.7273 acc: 1.0\n",
      "epoch: 16 batch_num: 16 loss: 0.8732 acc: 1.0\n",
      "epoch: 16 batch_num: 17 loss: 0.2661 acc: 1.0\n",
      "epoch: 16 batch_num: 18 loss: 0.3716 acc: 1.0\n",
      "epoch: 16 batch_num: 19 loss: 0.4693 acc: 1.0\n",
      "epoch: 16 batch_num: 20 loss: 0.5104 acc: 1.0\n",
      "epoch: 16 batch_num: 21 loss: 4.4983 acc: 0.984375\n",
      "epoch: 16 batch_num: 22 loss: 0.8407 acc: 1.0\n",
      "epoch: 16 batch_num: 23 loss: 0.9625 acc: 1.0\n",
      "epoch: 16 batch_num: 24 loss: 0.4035 acc: 1.0\n",
      "epoch: 16 batch_num: 25 loss: 0.6617 acc: 1.0\n",
      "epoch: 16 batch_num: 26 loss: 0.625 acc: 1.0\n",
      "epoch: 16 batch_num: 27 loss: 0.294 acc: 1.0\n",
      "epoch: 16 batch_num: 28 loss: 2.9809 acc: 0.984375\n",
      "epoch: 16 batch_num: 29 loss: 0.51 acc: 1.0\n",
      "epoch: 16 batch_num: 30 loss: 1.2 acc: 1.0\n",
      "epoch: 16 batch_num: 31 loss: 3.2976 acc: 0.984375\n",
      "epoch: 16 batch_num: 32 loss: 0.2986 acc: 1.0\n",
      "epoch: 16 batch_num: 33 loss: 0.399 acc: 1.0\n",
      "epoch: 16 batch_num: 34 loss: 3.1225 acc: 0.984375\n",
      "epoch: 16 batch_num: 35 loss: 0.3121 acc: 1.0\n",
      "epoch: 16 batch_num: 36 loss: 1.0781 acc: 0.984375\n",
      "epoch: 16 batch_num: 37 loss: 0.2888 acc: 1.0\n",
      "epoch: 16 batch_num: 38 loss: 0.7196 acc: 1.0\n",
      "epoch: 16 batch_num: 39 loss: 0.3063 acc: 1.0\n",
      "epoch: 16 batch_num: 40 loss: 0.2156 acc: 1.0\n",
      "epoch: 16 batch_num: 41 loss: 0.3871 acc: 1.0\n",
      "epoch: 16 batch_num: 42 loss: 0.5682 acc: 1.0\n",
      "epoch: 16 batch_num: 43 loss: 1.4643 acc: 1.0\n",
      "epoch: 16 batch_num: 44 loss: 0.592 acc: 1.0\n",
      "epoch: 16 batch_num: 45 loss: 0.4328 acc: 1.0\n",
      "epoch: 16 batch_num: 46 loss: 1.2609 acc: 0.984375\n",
      "epoch: 16 batch_num: 47 loss: 0.3638 acc: 1.0\n",
      "epoch: 16 batch_num: 48 loss: 0.3368 acc: 1.0\n",
      "epoch: 16 batch_num: 49 loss: 1.1968 acc: 1.0\n",
      "epoch: 16 batch_num: 50 loss: 0.444 acc: 1.0\n",
      "epoch: 16 batch_num: 51 loss: 0.5793 acc: 1.0\n",
      "epoch: 16 batch_num: 52 loss: 0.8531 acc: 1.0\n",
      "epoch: 16 batch_num: 53 loss: 1.0716 acc: 1.0\n",
      "epoch: 16 batch_num: 54 loss: 0.3057 acc: 1.0\n",
      "epoch: 16 batch_num: 55 loss: 0.3979 acc: 1.0\n",
      "epoch: 16 batch_num: 56 loss: 0.3348 acc: 1.0\n",
      "epoch: 16 batch_num: 57 loss: 0.898 acc: 1.0\n",
      "epoch: 16 batch_num: 58 loss: 0.5952 acc: 1.0\n",
      "epoch: 16 batch_num: 59 loss: 1.5554 acc: 0.984375\n",
      "epoch: 16 batch_num: 60 loss: 0.3584 acc: 1.0\n",
      "epoch: 16 batch_num: 61 loss: 0.2571 acc: 1.0\n",
      "epoch: 16 batch_num: 62 loss: 1.4142 acc: 0.984375\n",
      "epoch: 16 batch_num: 63 loss: 0.4442 acc: 1.0\n",
      "epoch: 16 batch_num: 64 loss: 2.6784 acc: 0.984375\n",
      "epoch: 16 batch_num: 65 loss: 0.3264 acc: 1.0\n",
      "epoch: 16 batch_num: 66 loss: 0.8474 acc: 1.0\n",
      "epoch: 16 batch_num: 67 loss: 1.0145 acc: 1.0\n",
      "epoch: 16 batch_num: 68 loss: 1.2178 acc: 0.984375\n",
      "epoch: 16 batch_num: 69 loss: 0.3057 acc: 1.0\n",
      "epoch: 16 batch_num: 70 loss: 2.2077 acc: 0.984375\n",
      "epoch: 16 batch_num: 71 loss: 0.4882 acc: 1.0\n",
      "epoch: 16 batch_num: 72 loss: 0.4401 acc: 1.0\n",
      "epoch: 16 batch_num: 73 loss: 0.6103 acc: 1.0\n",
      "epoch: 16 batch_num: 74 loss: 0.6112 acc: 1.0\n",
      "epoch: 16 batch_num: 75 loss: 1.2324 acc: 1.0\n",
      "epoch: 16 batch_num: 76 loss: 1.1496 acc: 1.0\n",
      "epoch: 16 batch_num: 77 loss: 0.2876 acc: 1.0\n",
      "epoch: 16 batch_num: 78 loss: 0.4183 acc: 1.0\n",
      "epoch: 16 batch_num: 79 loss: 0.2513 acc: 1.0\n",
      "epoch: 16 batch_num: 80 loss: 0.6738 acc: 1.0\n",
      "epoch: 16 batch_num: 81 loss: 0.2934 acc: 1.0\n",
      "epoch: 16 batch_num: 82 loss: 0.2447 acc: 1.0\n",
      "epoch: 16 batch_num: 83 loss: 0.4734 acc: 1.0\n",
      "epoch: 16 batch_num: 84 loss: 0.4469 acc: 1.0\n",
      "epoch: 16 batch_num: 85 loss: 1.7394 acc: 0.984375\n",
      "epoch: 16 batch_num: 86 loss: 0.4831 acc: 1.0\n",
      "epoch: 16 batch_num: 87 loss: 1.4771 acc: 0.984375\n",
      "epoch: 16 batch_num: 88 loss: 0.9604 acc: 1.0\n",
      "epoch: 16 batch_num: 89 loss: 1.7579 acc: 0.984375\n",
      "epoch: 16 batch_num: 90 loss: 0.9396 acc: 1.0\n",
      "epoch: 16 batch_num: 91 loss: 0.3625 acc: 1.0\n",
      "epoch: 16 batch_num: 92 loss: 0.3995 acc: 1.0\n",
      "epoch: 16 batch_num: 93 loss: 0.4821 acc: 1.0\n",
      "epoch: 16 batch_num: 94 loss: 1.0811 acc: 1.0\n",
      "epoch: 16 batch_num: 95 loss: 0.7566 acc: 1.0\n",
      "epoch: 16 batch_num: 96 loss: 2.5454 acc: 0.984375\n",
      "epoch: 16 batch_num: 97 loss: 0.3577 acc: 1.0\n",
      "epoch: 16 batch_num: 98 loss: 0.5359 acc: 1.0\n",
      "epoch: 16 batch_num: 99 loss: 4.3972 acc: 0.984375\n",
      "epoch: 16 batch_num: 100 loss: 0.2961 acc: 1.0\n",
      "epoch: 16 batch_num: 101 loss: 0.3323 acc: 1.0\n",
      "epoch: 16 batch_num: 102 loss: 0.2108 acc: 1.0\n",
      "epoch: 16 batch_num: 103 loss: 0.3018 acc: 1.0\n",
      "epoch: 16 batch_num: 104 loss: 0.3568 acc: 1.0\n",
      "epoch: 16 batch_num: 105 loss: 0.32 acc: 1.0\n",
      "epoch: 16 batch_num: 106 loss: 0.3333 acc: 1.0\n",
      "epoch: 16 batch_num: 107 loss: 0.8058 acc: 1.0\n",
      "epoch: 16 batch_num: 108 loss: 1.7676 acc: 0.984375\n",
      "epoch: 16 batch_num: 109 loss: 1.3183 acc: 0.984375\n",
      "epoch: 16 batch_num: 110 loss: 0.3888 acc: 1.0\n",
      "epoch: 16 batch_num: 111 loss: 0.3868 acc: 1.0\n",
      "epoch: 16 batch_num: 112 loss: 0.79 acc: 1.0\n",
      "epoch: 16 batch_num: 113 loss: 1.4736 acc: 0.984375\n",
      "epoch: 16 batch_num: 114 loss: 1.5902 acc: 0.984375\n",
      "epoch: 16 batch_num: 115 loss: 0.3276 acc: 1.0\n",
      "epoch: 16 batch_num: 116 loss: 0.2251 acc: 1.0\n",
      "epoch: 17 batch_num: 1 loss: 0.7533 acc: 1.0\n",
      "epoch: 17 batch_num: 2 loss: 0.8347 acc: 1.0\n",
      "epoch: 17 batch_num: 3 loss: 0.32 acc: 1.0\n",
      "epoch: 17 batch_num: 4 loss: 0.4039 acc: 1.0\n",
      "epoch: 17 batch_num: 5 loss: 0.263 acc: 1.0\n",
      "epoch: 17 batch_num: 6 loss: 0.3451 acc: 1.0\n",
      "epoch: 17 batch_num: 7 loss: 0.2986 acc: 1.0\n",
      "epoch: 17 batch_num: 8 loss: 1.081 acc: 0.984375\n",
      "epoch: 17 batch_num: 9 loss: 0.3013 acc: 1.0\n",
      "epoch: 17 batch_num: 10 loss: 0.2947 acc: 1.0\n",
      "epoch: 17 batch_num: 11 loss: 0.2536 acc: 1.0\n",
      "epoch: 17 batch_num: 12 loss: 0.3111 acc: 1.0\n",
      "epoch: 17 batch_num: 13 loss: 0.2561 acc: 1.0\n",
      "epoch: 17 batch_num: 14 loss: 0.3939 acc: 1.0\n",
      "epoch: 17 batch_num: 15 loss: 0.4778 acc: 1.0\n",
      "epoch: 17 batch_num: 16 loss: 0.579 acc: 1.0\n",
      "epoch: 17 batch_num: 17 loss: 0.497 acc: 1.0\n",
      "epoch: 17 batch_num: 18 loss: 0.8078 acc: 1.0\n",
      "epoch: 17 batch_num: 19 loss: 0.8548 acc: 1.0\n",
      "epoch: 17 batch_num: 20 loss: 0.4939 acc: 1.0\n",
      "epoch: 17 batch_num: 21 loss: 0.4096 acc: 1.0\n",
      "epoch: 17 batch_num: 22 loss: 0.328 acc: 1.0\n",
      "epoch: 17 batch_num: 23 loss: 0.418 acc: 1.0\n",
      "epoch: 17 batch_num: 24 loss: 0.1834 acc: 1.0\n",
      "epoch: 17 batch_num: 25 loss: 0.3916 acc: 1.0\n",
      "epoch: 17 batch_num: 26 loss: 0.2359 acc: 1.0\n",
      "epoch: 17 batch_num: 27 loss: 2.9388 acc: 0.984375\n",
      "epoch: 17 batch_num: 28 loss: 1.092 acc: 1.0\n",
      "epoch: 17 batch_num: 29 loss: 2.66 acc: 0.984375\n",
      "epoch: 17 batch_num: 30 loss: 1.7517 acc: 0.984375\n",
      "epoch: 17 batch_num: 31 loss: 0.3403 acc: 1.0\n",
      "epoch: 17 batch_num: 32 loss: 0.4796 acc: 1.0\n",
      "epoch: 17 batch_num: 33 loss: 0.5384 acc: 1.0\n",
      "epoch: 17 batch_num: 34 loss: 0.1748 acc: 1.0\n",
      "epoch: 17 batch_num: 35 loss: 0.2375 acc: 1.0\n",
      "epoch: 17 batch_num: 36 loss: 0.3576 acc: 1.0\n",
      "epoch: 17 batch_num: 37 loss: 0.4135 acc: 1.0\n",
      "epoch: 17 batch_num: 38 loss: 0.5784 acc: 1.0\n",
      "epoch: 17 batch_num: 39 loss: 0.9852 acc: 1.0\n",
      "epoch: 17 batch_num: 40 loss: 0.7397 acc: 1.0\n",
      "epoch: 17 batch_num: 41 loss: 0.6827 acc: 1.0\n",
      "epoch: 17 batch_num: 42 loss: 0.3098 acc: 1.0\n",
      "epoch: 17 batch_num: 43 loss: 0.7976 acc: 1.0\n",
      "epoch: 17 batch_num: 44 loss: 0.3809 acc: 1.0\n",
      "epoch: 17 batch_num: 45 loss: 0.5195 acc: 1.0\n",
      "epoch: 17 batch_num: 46 loss: 0.7617 acc: 1.0\n",
      "epoch: 17 batch_num: 47 loss: 0.3502 acc: 1.0\n",
      "epoch: 17 batch_num: 48 loss: 0.3148 acc: 1.0\n",
      "epoch: 17 batch_num: 49 loss: 0.6293 acc: 1.0\n",
      "epoch: 17 batch_num: 50 loss: 1.2393 acc: 1.0\n",
      "epoch: 17 batch_num: 51 loss: 0.7924 acc: 1.0\n",
      "epoch: 17 batch_num: 52 loss: 0.2739 acc: 1.0\n",
      "epoch: 17 batch_num: 53 loss: 0.3423 acc: 1.0\n",
      "epoch: 17 batch_num: 54 loss: 1.5055 acc: 0.984375\n",
      "epoch: 17 batch_num: 55 loss: 0.3307 acc: 1.0\n",
      "epoch: 17 batch_num: 56 loss: 0.1502 acc: 1.0\n",
      "epoch: 17 batch_num: 57 loss: 1.0936 acc: 1.0\n",
      "epoch: 17 batch_num: 58 loss: 0.9329 acc: 1.0\n",
      "epoch: 17 batch_num: 59 loss: 1.3804 acc: 0.984375\n",
      "epoch: 17 batch_num: 60 loss: 0.2199 acc: 1.0\n",
      "epoch: 17 batch_num: 61 loss: 3.3113 acc: 0.984375\n",
      "epoch: 17 batch_num: 62 loss: 1.222 acc: 0.984375\n",
      "epoch: 17 batch_num: 63 loss: 0.2684 acc: 1.0\n",
      "epoch: 17 batch_num: 64 loss: 0.3952 acc: 1.0\n",
      "epoch: 17 batch_num: 65 loss: 0.3018 acc: 1.0\n",
      "epoch: 17 batch_num: 66 loss: 0.4358 acc: 1.0\n",
      "epoch: 17 batch_num: 67 loss: 0.4009 acc: 1.0\n",
      "epoch: 17 batch_num: 68 loss: 2.7395 acc: 0.984375\n",
      "epoch: 17 batch_num: 69 loss: 0.7747 acc: 1.0\n",
      "epoch: 17 batch_num: 70 loss: 0.3777 acc: 1.0\n",
      "epoch: 17 batch_num: 71 loss: 0.3089 acc: 1.0\n",
      "epoch: 17 batch_num: 72 loss: 2.7287 acc: 0.984375\n",
      "epoch: 17 batch_num: 73 loss: 0.6053 acc: 1.0\n",
      "epoch: 17 batch_num: 74 loss: 1.2517 acc: 0.984375\n",
      "epoch: 17 batch_num: 75 loss: 0.2628 acc: 1.0\n",
      "epoch: 17 batch_num: 76 loss: 0.3606 acc: 1.0\n",
      "epoch: 17 batch_num: 77 loss: 0.2887 acc: 1.0\n",
      "epoch: 17 batch_num: 78 loss: 0.4034 acc: 1.0\n",
      "epoch: 17 batch_num: 79 loss: 0.2432 acc: 1.0\n",
      "epoch: 17 batch_num: 80 loss: 0.4615 acc: 1.0\n",
      "epoch: 17 batch_num: 81 loss: 0.5269 acc: 1.0\n",
      "epoch: 17 batch_num: 82 loss: 0.4373 acc: 1.0\n",
      "epoch: 17 batch_num: 83 loss: 0.6071 acc: 1.0\n",
      "epoch: 17 batch_num: 84 loss: 0.4274 acc: 1.0\n",
      "epoch: 17 batch_num: 85 loss: 0.3389 acc: 1.0\n",
      "epoch: 17 batch_num: 86 loss: 0.3735 acc: 1.0\n",
      "epoch: 17 batch_num: 87 loss: 0.6816 acc: 1.0\n",
      "epoch: 17 batch_num: 88 loss: 0.4063 acc: 1.0\n",
      "epoch: 17 batch_num: 89 loss: 0.6233 acc: 1.0\n",
      "epoch: 17 batch_num: 90 loss: 1.1359 acc: 1.0\n",
      "epoch: 17 batch_num: 91 loss: 0.6217 acc: 1.0\n",
      "epoch: 17 batch_num: 92 loss: 0.2496 acc: 1.0\n",
      "epoch: 17 batch_num: 93 loss: 0.3373 acc: 1.0\n",
      "epoch: 17 batch_num: 94 loss: 0.5411 acc: 1.0\n",
      "epoch: 17 batch_num: 95 loss: 0.4855 acc: 1.0\n",
      "epoch: 17 batch_num: 96 loss: 1.0962 acc: 0.984375\n",
      "epoch: 17 batch_num: 97 loss: 1.6462 acc: 0.984375\n",
      "epoch: 17 batch_num: 98 loss: 0.2764 acc: 1.0\n",
      "epoch: 17 batch_num: 99 loss: 0.4705 acc: 1.0\n",
      "epoch: 17 batch_num: 100 loss: 0.438 acc: 1.0\n",
      "epoch: 17 batch_num: 101 loss: 0.7747 acc: 1.0\n",
      "epoch: 17 batch_num: 102 loss: 0.2508 acc: 1.0\n",
      "epoch: 17 batch_num: 103 loss: 0.2628 acc: 1.0\n",
      "epoch: 17 batch_num: 104 loss: 3.2719 acc: 0.984375\n",
      "epoch: 17 batch_num: 105 loss: 0.4685 acc: 1.0\n",
      "epoch: 17 batch_num: 106 loss: 0.2028 acc: 1.0\n",
      "epoch: 17 batch_num: 107 loss: 0.4747 acc: 1.0\n",
      "epoch: 17 batch_num: 108 loss: 0.405 acc: 1.0\n",
      "epoch: 17 batch_num: 109 loss: 0.2153 acc: 1.0\n",
      "epoch: 17 batch_num: 110 loss: 0.3564 acc: 1.0\n",
      "epoch: 17 batch_num: 111 loss: 1.5914 acc: 0.984375\n",
      "epoch: 17 batch_num: 112 loss: 0.5132 acc: 1.0\n",
      "epoch: 17 batch_num: 113 loss: 4.4759 acc: 0.984375\n",
      "epoch: 17 batch_num: 114 loss: 0.5015 acc: 1.0\n",
      "epoch: 17 batch_num: 115 loss: 0.3068 acc: 1.0\n",
      "epoch: 17 batch_num: 116 loss: 0.1297 acc: 1.0\n",
      "epoch: 18 batch_num: 1 loss: 0.3369 acc: 1.0\n",
      "epoch: 18 batch_num: 2 loss: 0.3 acc: 1.0\n",
      "epoch: 18 batch_num: 3 loss: 0.9148 acc: 0.984375\n",
      "epoch: 18 batch_num: 4 loss: 0.3653 acc: 1.0\n",
      "epoch: 18 batch_num: 5 loss: 0.2544 acc: 1.0\n",
      "epoch: 18 batch_num: 6 loss: 0.438 acc: 1.0\n",
      "epoch: 18 batch_num: 7 loss: 0.3076 acc: 1.0\n",
      "epoch: 18 batch_num: 8 loss: 0.2395 acc: 1.0\n",
      "epoch: 18 batch_num: 9 loss: 0.3125 acc: 1.0\n",
      "epoch: 18 batch_num: 10 loss: 0.6527 acc: 1.0\n",
      "epoch: 18 batch_num: 11 loss: 0.5668 acc: 1.0\n",
      "epoch: 18 batch_num: 12 loss: 2.7056 acc: 0.984375\n",
      "epoch: 18 batch_num: 13 loss: 0.5546 acc: 1.0\n",
      "epoch: 18 batch_num: 14 loss: 0.1906 acc: 1.0\n",
      "epoch: 18 batch_num: 15 loss: 1.6159 acc: 0.984375\n",
      "epoch: 18 batch_num: 16 loss: 0.2735 acc: 1.0\n",
      "epoch: 18 batch_num: 17 loss: 0.8333 acc: 1.0\n",
      "epoch: 18 batch_num: 18 loss: 1.9229 acc: 0.984375\n",
      "epoch: 18 batch_num: 19 loss: 0.7193 acc: 1.0\n",
      "epoch: 18 batch_num: 20 loss: 0.4794 acc: 1.0\n",
      "epoch: 18 batch_num: 21 loss: 0.6908 acc: 1.0\n",
      "epoch: 18 batch_num: 22 loss: 0.2426 acc: 1.0\n",
      "epoch: 18 batch_num: 23 loss: 0.2094 acc: 1.0\n",
      "epoch: 18 batch_num: 24 loss: 0.2727 acc: 1.0\n",
      "epoch: 18 batch_num: 25 loss: 1.0336 acc: 0.984375\n",
      "epoch: 18 batch_num: 26 loss: 0.1715 acc: 1.0\n",
      "epoch: 18 batch_num: 27 loss: 0.2672 acc: 1.0\n",
      "epoch: 18 batch_num: 28 loss: 0.2256 acc: 1.0\n",
      "epoch: 18 batch_num: 29 loss: 0.3909 acc: 1.0\n",
      "epoch: 18 batch_num: 30 loss: 1.4096 acc: 0.984375\n",
      "epoch: 18 batch_num: 31 loss: 0.2775 acc: 1.0\n",
      "epoch: 18 batch_num: 32 loss: 0.3452 acc: 1.0\n",
      "epoch: 18 batch_num: 33 loss: 1.7281 acc: 0.984375\n",
      "epoch: 18 batch_num: 34 loss: 0.3278 acc: 1.0\n",
      "epoch: 18 batch_num: 35 loss: 0.8521 acc: 1.0\n",
      "epoch: 18 batch_num: 36 loss: 0.4056 acc: 1.0\n",
      "epoch: 18 batch_num: 37 loss: 0.5216 acc: 1.0\n",
      "epoch: 18 batch_num: 38 loss: 1.4233 acc: 0.984375\n",
      "epoch: 18 batch_num: 39 loss: 0.2456 acc: 1.0\n",
      "epoch: 18 batch_num: 40 loss: 0.2947 acc: 1.0\n",
      "epoch: 18 batch_num: 41 loss: 0.3861 acc: 1.0\n",
      "epoch: 18 batch_num: 42 loss: 1.2975 acc: 0.984375\n",
      "epoch: 18 batch_num: 43 loss: 0.21 acc: 1.0\n",
      "epoch: 18 batch_num: 44 loss: 2.0506 acc: 0.984375\n",
      "epoch: 18 batch_num: 45 loss: 0.7285 acc: 1.0\n",
      "epoch: 18 batch_num: 46 loss: 0.2267 acc: 1.0\n",
      "epoch: 18 batch_num: 47 loss: 0.5773 acc: 1.0\n",
      "epoch: 18 batch_num: 48 loss: 0.3675 acc: 1.0\n",
      "epoch: 18 batch_num: 49 loss: 0.3316 acc: 1.0\n",
      "epoch: 18 batch_num: 50 loss: 0.5005 acc: 1.0\n",
      "epoch: 18 batch_num: 51 loss: 0.2965 acc: 1.0\n",
      "epoch: 18 batch_num: 52 loss: 0.307 acc: 1.0\n",
      "epoch: 18 batch_num: 53 loss: 0.3354 acc: 1.0\n",
      "epoch: 18 batch_num: 54 loss: 0.1998 acc: 1.0\n",
      "epoch: 18 batch_num: 55 loss: 0.8854 acc: 1.0\n",
      "epoch: 18 batch_num: 56 loss: 0.2793 acc: 1.0\n",
      "epoch: 18 batch_num: 57 loss: 0.34 acc: 1.0\n",
      "epoch: 18 batch_num: 58 loss: 0.3286 acc: 1.0\n",
      "epoch: 18 batch_num: 59 loss: 0.1517 acc: 1.0\n",
      "epoch: 18 batch_num: 60 loss: 0.6231 acc: 1.0\n",
      "epoch: 18 batch_num: 61 loss: 0.5145 acc: 1.0\n",
      "epoch: 18 batch_num: 62 loss: 0.2429 acc: 1.0\n",
      "epoch: 18 batch_num: 63 loss: 0.3331 acc: 1.0\n",
      "epoch: 18 batch_num: 64 loss: 0.4697 acc: 1.0\n",
      "epoch: 18 batch_num: 65 loss: 2.4754 acc: 0.984375\n",
      "epoch: 18 batch_num: 66 loss: 0.3668 acc: 1.0\n",
      "epoch: 18 batch_num: 67 loss: 0.911 acc: 1.0\n",
      "epoch: 18 batch_num: 68 loss: 0.4573 acc: 1.0\n",
      "epoch: 18 batch_num: 69 loss: 1.8031 acc: 0.984375\n",
      "epoch: 18 batch_num: 70 loss: 0.3355 acc: 1.0\n",
      "epoch: 18 batch_num: 71 loss: 2.0129 acc: 0.984375\n",
      "epoch: 18 batch_num: 72 loss: 0.4073 acc: 1.0\n",
      "epoch: 18 batch_num: 73 loss: 0.2428 acc: 1.0\n",
      "epoch: 18 batch_num: 74 loss: 0.3696 acc: 1.0\n",
      "epoch: 18 batch_num: 75 loss: 0.266 acc: 1.0\n",
      "epoch: 18 batch_num: 76 loss: 0.6413 acc: 1.0\n",
      "epoch: 18 batch_num: 77 loss: 1.0038 acc: 1.0\n",
      "epoch: 18 batch_num: 78 loss: 0.4199 acc: 1.0\n",
      "epoch: 18 batch_num: 79 loss: 0.7173 acc: 1.0\n",
      "epoch: 18 batch_num: 80 loss: 0.2336 acc: 1.0\n",
      "epoch: 18 batch_num: 81 loss: 0.2017 acc: 1.0\n",
      "epoch: 18 batch_num: 82 loss: 0.2734 acc: 1.0\n",
      "epoch: 18 batch_num: 83 loss: 0.2876 acc: 1.0\n",
      "epoch: 18 batch_num: 84 loss: 0.3863 acc: 1.0\n",
      "epoch: 18 batch_num: 85 loss: 0.4981 acc: 1.0\n",
      "epoch: 18 batch_num: 86 loss: 0.3008 acc: 1.0\n",
      "epoch: 18 batch_num: 87 loss: 0.2913 acc: 1.0\n",
      "epoch: 18 batch_num: 88 loss: 0.6163 acc: 1.0\n",
      "epoch: 18 batch_num: 89 loss: 0.9839 acc: 1.0\n",
      "epoch: 18 batch_num: 90 loss: 0.2484 acc: 1.0\n",
      "epoch: 18 batch_num: 91 loss: 0.2734 acc: 1.0\n",
      "epoch: 18 batch_num: 92 loss: 0.2698 acc: 1.0\n",
      "epoch: 18 batch_num: 93 loss: 0.1319 acc: 1.0\n",
      "epoch: 18 batch_num: 94 loss: 0.5508 acc: 1.0\n",
      "epoch: 18 batch_num: 95 loss: 0.3843 acc: 1.0\n",
      "epoch: 18 batch_num: 96 loss: 0.4109 acc: 1.0\n",
      "epoch: 18 batch_num: 97 loss: 1.1638 acc: 0.984375\n",
      "epoch: 18 batch_num: 98 loss: 0.3037 acc: 1.0\n",
      "epoch: 18 batch_num: 99 loss: 0.2957 acc: 1.0\n",
      "epoch: 18 batch_num: 100 loss: 0.2936 acc: 1.0\n",
      "epoch: 18 batch_num: 101 loss: 0.3276 acc: 1.0\n",
      "epoch: 18 batch_num: 102 loss: 0.5363 acc: 1.0\n",
      "epoch: 18 batch_num: 103 loss: 0.3685 acc: 1.0\n",
      "epoch: 18 batch_num: 104 loss: 0.27 acc: 1.0\n",
      "epoch: 18 batch_num: 105 loss: 0.2905 acc: 1.0\n",
      "epoch: 18 batch_num: 106 loss: 3.5271 acc: 0.984375\n",
      "epoch: 18 batch_num: 107 loss: 0.7458 acc: 1.0\n",
      "epoch: 18 batch_num: 108 loss: 0.331 acc: 1.0\n",
      "epoch: 18 batch_num: 109 loss: 0.5028 acc: 1.0\n",
      "epoch: 18 batch_num: 110 loss: 0.4339 acc: 1.0\n",
      "epoch: 18 batch_num: 111 loss: 0.1532 acc: 1.0\n",
      "epoch: 18 batch_num: 112 loss: 0.3265 acc: 1.0\n",
      "epoch: 18 batch_num: 113 loss: 0.2561 acc: 1.0\n",
      "epoch: 18 batch_num: 114 loss: 0.7569 acc: 1.0\n",
      "epoch: 18 batch_num: 115 loss: 0.3208 acc: 1.0\n",
      "epoch: 18 batch_num: 116 loss: 0.1159 acc: 1.0\n",
      "epoch: 19 batch_num: 1 loss: 0.2854 acc: 1.0\n",
      "epoch: 19 batch_num: 2 loss: 0.3607 acc: 1.0\n",
      "epoch: 19 batch_num: 3 loss: 0.9377 acc: 0.984375\n",
      "epoch: 19 batch_num: 4 loss: 0.7686 acc: 1.0\n",
      "epoch: 19 batch_num: 5 loss: 0.3834 acc: 1.0\n",
      "epoch: 19 batch_num: 6 loss: 0.256 acc: 1.0\n",
      "epoch: 19 batch_num: 7 loss: 0.2038 acc: 1.0\n",
      "epoch: 19 batch_num: 8 loss: 0.269 acc: 1.0\n",
      "epoch: 19 batch_num: 9 loss: 1.2116 acc: 1.0\n",
      "epoch: 19 batch_num: 10 loss: 0.3447 acc: 1.0\n",
      "epoch: 19 batch_num: 11 loss: 0.2609 acc: 1.0\n",
      "epoch: 19 batch_num: 12 loss: 0.4572 acc: 1.0\n",
      "epoch: 19 batch_num: 13 loss: 0.264 acc: 1.0\n",
      "epoch: 19 batch_num: 14 loss: 0.1944 acc: 1.0\n",
      "epoch: 19 batch_num: 15 loss: 0.2997 acc: 1.0\n",
      "epoch: 19 batch_num: 16 loss: 0.2198 acc: 1.0\n",
      "epoch: 19 batch_num: 17 loss: 0.5323 acc: 1.0\n",
      "epoch: 19 batch_num: 18 loss: 0.4443 acc: 1.0\n",
      "epoch: 19 batch_num: 19 loss: 0.2317 acc: 1.0\n",
      "epoch: 19 batch_num: 20 loss: 0.2826 acc: 1.0\n",
      "epoch: 19 batch_num: 21 loss: 0.7048 acc: 1.0\n",
      "epoch: 19 batch_num: 22 loss: 0.1946 acc: 1.0\n",
      "epoch: 19 batch_num: 23 loss: 0.239 acc: 1.0\n",
      "epoch: 19 batch_num: 24 loss: 0.1845 acc: 1.0\n",
      "epoch: 19 batch_num: 25 loss: 0.7203 acc: 1.0\n",
      "epoch: 19 batch_num: 26 loss: 0.3101 acc: 1.0\n",
      "epoch: 19 batch_num: 27 loss: 2.3514 acc: 0.984375\n",
      "epoch: 19 batch_num: 28 loss: 0.2406 acc: 1.0\n",
      "epoch: 19 batch_num: 29 loss: 0.2233 acc: 1.0\n",
      "epoch: 19 batch_num: 30 loss: 0.2722 acc: 1.0\n",
      "epoch: 19 batch_num: 31 loss: 0.2239 acc: 1.0\n",
      "epoch: 19 batch_num: 32 loss: 0.204 acc: 1.0\n",
      "epoch: 19 batch_num: 33 loss: 0.3124 acc: 1.0\n",
      "epoch: 19 batch_num: 34 loss: 0.4325 acc: 1.0\n",
      "epoch: 19 batch_num: 35 loss: 0.6674 acc: 1.0\n",
      "epoch: 19 batch_num: 36 loss: 2.0201 acc: 0.984375\n",
      "epoch: 19 batch_num: 37 loss: 0.2679 acc: 1.0\n",
      "epoch: 19 batch_num: 38 loss: 0.3272 acc: 1.0\n",
      "epoch: 19 batch_num: 39 loss: 0.1995 acc: 1.0\n",
      "epoch: 19 batch_num: 40 loss: 1.0263 acc: 1.0\n",
      "epoch: 19 batch_num: 41 loss: 0.3688 acc: 1.0\n",
      "epoch: 19 batch_num: 42 loss: 0.6136 acc: 1.0\n",
      "epoch: 19 batch_num: 43 loss: 0.2715 acc: 1.0\n",
      "epoch: 19 batch_num: 44 loss: 1.1312 acc: 1.0\n",
      "epoch: 19 batch_num: 45 loss: 0.4134 acc: 1.0\n",
      "epoch: 19 batch_num: 46 loss: 0.266 acc: 1.0\n",
      "epoch: 19 batch_num: 47 loss: 0.4506 acc: 1.0\n",
      "epoch: 19 batch_num: 48 loss: 0.2129 acc: 1.0\n",
      "epoch: 19 batch_num: 49 loss: 0.1598 acc: 1.0\n",
      "epoch: 19 batch_num: 50 loss: 0.4123 acc: 1.0\n",
      "epoch: 19 batch_num: 51 loss: 0.2676 acc: 1.0\n",
      "epoch: 19 batch_num: 52 loss: 0.5893 acc: 1.0\n",
      "epoch: 19 batch_num: 53 loss: 0.977 acc: 0.984375\n",
      "epoch: 19 batch_num: 54 loss: 0.2967 acc: 1.0\n",
      "epoch: 19 batch_num: 55 loss: 0.7118 acc: 1.0\n",
      "epoch: 19 batch_num: 56 loss: 0.3863 acc: 1.0\n",
      "epoch: 19 batch_num: 57 loss: 0.256 acc: 1.0\n",
      "epoch: 19 batch_num: 58 loss: 0.333 acc: 1.0\n",
      "epoch: 19 batch_num: 59 loss: 0.2393 acc: 1.0\n",
      "epoch: 19 batch_num: 60 loss: 0.1163 acc: 1.0\n",
      "epoch: 19 batch_num: 61 loss: 0.2594 acc: 1.0\n",
      "epoch: 19 batch_num: 62 loss: 0.3226 acc: 1.0\n",
      "epoch: 19 batch_num: 63 loss: 0.328 acc: 1.0\n",
      "epoch: 19 batch_num: 64 loss: 0.3058 acc: 1.0\n",
      "epoch: 19 batch_num: 65 loss: 0.3498 acc: 1.0\n",
      "epoch: 19 batch_num: 66 loss: 0.607 acc: 1.0\n",
      "epoch: 19 batch_num: 67 loss: 0.3765 acc: 1.0\n",
      "epoch: 19 batch_num: 68 loss: 0.423 acc: 1.0\n",
      "epoch: 19 batch_num: 69 loss: 0.6271 acc: 1.0\n",
      "epoch: 19 batch_num: 70 loss: 0.4844 acc: 1.0\n",
      "epoch: 19 batch_num: 71 loss: 0.5629 acc: 1.0\n",
      "epoch: 19 batch_num: 72 loss: 0.2651 acc: 1.0\n",
      "epoch: 19 batch_num: 73 loss: 0.2768 acc: 1.0\n",
      "epoch: 19 batch_num: 74 loss: 0.1545 acc: 1.0\n",
      "epoch: 19 batch_num: 75 loss: 0.3349 acc: 1.0\n",
      "epoch: 19 batch_num: 76 loss: 0.3353 acc: 1.0\n",
      "epoch: 19 batch_num: 77 loss: 0.4484 acc: 1.0\n",
      "epoch: 19 batch_num: 78 loss: 0.3184 acc: 1.0\n",
      "epoch: 19 batch_num: 79 loss: 0.4256 acc: 1.0\n",
      "epoch: 19 batch_num: 80 loss: 0.6607 acc: 1.0\n",
      "epoch: 19 batch_num: 81 loss: 0.248 acc: 1.0\n",
      "epoch: 19 batch_num: 82 loss: 0.2865 acc: 1.0\n",
      "epoch: 19 batch_num: 83 loss: 0.1373 acc: 1.0\n",
      "epoch: 19 batch_num: 84 loss: 3.6063 acc: 0.984375\n",
      "epoch: 19 batch_num: 85 loss: 0.1813 acc: 1.0\n",
      "epoch: 19 batch_num: 86 loss: 0.1752 acc: 1.0\n",
      "epoch: 19 batch_num: 87 loss: 0.3106 acc: 1.0\n",
      "epoch: 19 batch_num: 88 loss: 0.2138 acc: 1.0\n",
      "epoch: 19 batch_num: 89 loss: 3.1412 acc: 0.984375\n",
      "epoch: 19 batch_num: 90 loss: 0.2783 acc: 1.0\n",
      "epoch: 19 batch_num: 91 loss: 0.1931 acc: 1.0\n",
      "epoch: 19 batch_num: 92 loss: 5.0797 acc: 0.984375\n",
      "epoch: 19 batch_num: 93 loss: 0.4112 acc: 1.0\n",
      "epoch: 19 batch_num: 94 loss: 0.4992 acc: 1.0\n",
      "epoch: 19 batch_num: 95 loss: 0.3467 acc: 1.0\n",
      "epoch: 19 batch_num: 96 loss: 1.0866 acc: 0.984375\n",
      "epoch: 19 batch_num: 97 loss: 0.468 acc: 1.0\n",
      "epoch: 19 batch_num: 98 loss: 0.503 acc: 1.0\n",
      "epoch: 19 batch_num: 99 loss: 0.4455 acc: 1.0\n",
      "epoch: 19 batch_num: 100 loss: 0.187 acc: 1.0\n",
      "epoch: 19 batch_num: 101 loss: 0.3176 acc: 1.0\n",
      "epoch: 19 batch_num: 102 loss: 0.225 acc: 1.0\n",
      "epoch: 19 batch_num: 103 loss: 0.1792 acc: 1.0\n",
      "epoch: 19 batch_num: 104 loss: 0.314 acc: 1.0\n",
      "epoch: 19 batch_num: 105 loss: 0.5121 acc: 1.0\n",
      "epoch: 19 batch_num: 106 loss: 0.2232 acc: 1.0\n",
      "epoch: 19 batch_num: 107 loss: 0.3001 acc: 1.0\n",
      "epoch: 19 batch_num: 108 loss: 0.3184 acc: 1.0\n",
      "epoch: 19 batch_num: 109 loss: 0.2603 acc: 1.0\n",
      "epoch: 19 batch_num: 110 loss: 0.8232 acc: 1.0\n",
      "epoch: 19 batch_num: 111 loss: 0.3325 acc: 1.0\n",
      "epoch: 19 batch_num: 112 loss: 0.3248 acc: 1.0\n",
      "epoch: 19 batch_num: 113 loss: 4.8512 acc: 0.984375\n",
      "epoch: 19 batch_num: 114 loss: 0.4241 acc: 1.0\n",
      "epoch: 19 batch_num: 115 loss: 2.2196 acc: 0.984375\n",
      "epoch: 19 batch_num: 116 loss: 0.192 acc: 1.0\n",
      "epoch: 20 batch_num: 1 loss: 0.1526 acc: 1.0\n",
      "epoch: 20 batch_num: 2 loss: 0.2576 acc: 1.0\n",
      "epoch: 20 batch_num: 3 loss: 1.8918 acc: 0.984375\n",
      "epoch: 20 batch_num: 4 loss: 0.244 acc: 1.0\n",
      "epoch: 20 batch_num: 5 loss: 0.6596 acc: 1.0\n",
      "epoch: 20 batch_num: 6 loss: 0.1828 acc: 1.0\n",
      "epoch: 20 batch_num: 7 loss: 0.7379 acc: 1.0\n",
      "epoch: 20 batch_num: 8 loss: 0.2303 acc: 1.0\n",
      "epoch: 20 batch_num: 9 loss: 0.3217 acc: 1.0\n",
      "epoch: 20 batch_num: 10 loss: 0.5548 acc: 1.0\n",
      "epoch: 20 batch_num: 11 loss: 0.2444 acc: 1.0\n",
      "epoch: 20 batch_num: 12 loss: 0.3423 acc: 1.0\n",
      "epoch: 20 batch_num: 13 loss: 0.4214 acc: 1.0\n",
      "epoch: 20 batch_num: 14 loss: 0.2835 acc: 1.0\n",
      "epoch: 20 batch_num: 15 loss: 0.1953 acc: 1.0\n",
      "epoch: 20 batch_num: 16 loss: 0.2344 acc: 1.0\n",
      "epoch: 20 batch_num: 17 loss: 0.9171 acc: 1.0\n",
      "epoch: 20 batch_num: 18 loss: 0.3572 acc: 1.0\n",
      "epoch: 20 batch_num: 19 loss: 0.2395 acc: 1.0\n",
      "epoch: 20 batch_num: 20 loss: 0.2861 acc: 1.0\n",
      "epoch: 20 batch_num: 21 loss: 0.2624 acc: 1.0\n",
      "epoch: 20 batch_num: 22 loss: 0.3263 acc: 1.0\n",
      "epoch: 20 batch_num: 23 loss: 0.315 acc: 1.0\n",
      "epoch: 20 batch_num: 24 loss: 0.1619 acc: 1.0\n",
      "epoch: 20 batch_num: 25 loss: 0.2003 acc: 1.0\n",
      "epoch: 20 batch_num: 26 loss: 0.1395 acc: 1.0\n",
      "epoch: 20 batch_num: 27 loss: 0.6029 acc: 1.0\n",
      "epoch: 20 batch_num: 28 loss: 0.2877 acc: 1.0\n",
      "epoch: 20 batch_num: 29 loss: 0.3586 acc: 1.0\n",
      "epoch: 20 batch_num: 30 loss: 0.0865 acc: 1.0\n",
      "epoch: 20 batch_num: 31 loss: 3.3109 acc: 0.984375\n",
      "epoch: 20 batch_num: 32 loss: 0.9872 acc: 0.984375\n",
      "epoch: 20 batch_num: 33 loss: 0.1865 acc: 1.0\n",
      "epoch: 20 batch_num: 34 loss: 1.4982 acc: 0.984375\n",
      "epoch: 20 batch_num: 35 loss: 0.5685 acc: 1.0\n",
      "epoch: 20 batch_num: 36 loss: 0.7043 acc: 1.0\n",
      "epoch: 20 batch_num: 37 loss: 0.2437 acc: 1.0\n",
      "epoch: 20 batch_num: 38 loss: 0.1683 acc: 1.0\n",
      "epoch: 20 batch_num: 39 loss: 0.2566 acc: 1.0\n",
      "epoch: 20 batch_num: 40 loss: 0.2182 acc: 1.0\n",
      "epoch: 20 batch_num: 41 loss: 0.373 acc: 1.0\n",
      "epoch: 20 batch_num: 42 loss: 0.2926 acc: 1.0\n",
      "epoch: 20 batch_num: 43 loss: 0.1576 acc: 1.0\n",
      "epoch: 20 batch_num: 44 loss: 1.1207 acc: 1.0\n",
      "epoch: 20 batch_num: 45 loss: 0.2299 acc: 1.0\n",
      "epoch: 20 batch_num: 46 loss: 0.282 acc: 1.0\n",
      "epoch: 20 batch_num: 47 loss: 0.2901 acc: 1.0\n",
      "epoch: 20 batch_num: 48 loss: 0.335 acc: 1.0\n",
      "epoch: 20 batch_num: 49 loss: 0.3137 acc: 1.0\n",
      "epoch: 20 batch_num: 50 loss: 0.6305 acc: 1.0\n",
      "epoch: 20 batch_num: 51 loss: 3.7227 acc: 0.984375\n",
      "epoch: 20 batch_num: 52 loss: 0.2493 acc: 1.0\n",
      "epoch: 20 batch_num: 53 loss: 0.4947 acc: 1.0\n",
      "epoch: 20 batch_num: 54 loss: 0.2052 acc: 1.0\n",
      "epoch: 20 batch_num: 55 loss: 0.2299 acc: 1.0\n",
      "epoch: 20 batch_num: 56 loss: 0.3244 acc: 1.0\n",
      "epoch: 20 batch_num: 57 loss: 0.3897 acc: 1.0\n",
      "epoch: 20 batch_num: 58 loss: 0.2626 acc: 1.0\n",
      "epoch: 20 batch_num: 59 loss: 2.2198 acc: 0.984375\n",
      "epoch: 20 batch_num: 60 loss: 0.2021 acc: 1.0\n",
      "epoch: 20 batch_num: 61 loss: 1.0595 acc: 1.0\n",
      "epoch: 20 batch_num: 62 loss: 0.1822 acc: 1.0\n",
      "epoch: 20 batch_num: 63 loss: 0.165 acc: 1.0\n",
      "epoch: 20 batch_num: 64 loss: 0.5131 acc: 1.0\n",
      "epoch: 20 batch_num: 65 loss: 0.3701 acc: 1.0\n",
      "epoch: 20 batch_num: 66 loss: 0.3574 acc: 1.0\n",
      "epoch: 20 batch_num: 67 loss: 0.2354 acc: 1.0\n",
      "epoch: 20 batch_num: 68 loss: 0.354 acc: 1.0\n",
      "epoch: 20 batch_num: 69 loss: 0.2892 acc: 1.0\n",
      "epoch: 20 batch_num: 70 loss: 2.7793 acc: 0.96875\n",
      "epoch: 20 batch_num: 71 loss: 0.1862 acc: 1.0\n",
      "epoch: 20 batch_num: 72 loss: 0.3967 acc: 1.0\n",
      "epoch: 20 batch_num: 73 loss: 0.8815 acc: 0.984375\n",
      "epoch: 20 batch_num: 74 loss: 0.3457 acc: 1.0\n",
      "epoch: 20 batch_num: 75 loss: 0.8017 acc: 1.0\n",
      "epoch: 20 batch_num: 76 loss: 0.808 acc: 1.0\n",
      "epoch: 20 batch_num: 77 loss: 2.1219 acc: 0.984375\n",
      "epoch: 20 batch_num: 78 loss: 0.2667 acc: 1.0\n",
      "epoch: 20 batch_num: 79 loss: 1.1173 acc: 0.984375\n",
      "epoch: 20 batch_num: 80 loss: 0.4131 acc: 1.0\n",
      "epoch: 20 batch_num: 81 loss: 0.56 acc: 1.0\n",
      "epoch: 20 batch_num: 82 loss: 0.2076 acc: 1.0\n",
      "epoch: 20 batch_num: 83 loss: 0.2527 acc: 1.0\n",
      "epoch: 20 batch_num: 84 loss: 0.3556 acc: 1.0\n",
      "epoch: 20 batch_num: 85 loss: 0.8232 acc: 1.0\n",
      "epoch: 20 batch_num: 86 loss: 0.2212 acc: 1.0\n",
      "epoch: 20 batch_num: 87 loss: 0.1884 acc: 1.0\n",
      "epoch: 20 batch_num: 88 loss: 0.1976 acc: 1.0\n",
      "epoch: 20 batch_num: 89 loss: 0.3437 acc: 1.0\n",
      "epoch: 20 batch_num: 90 loss: 0.7729 acc: 1.0\n",
      "epoch: 20 batch_num: 91 loss: 0.3565 acc: 1.0\n",
      "epoch: 20 batch_num: 92 loss: 0.9825 acc: 0.984375\n",
      "epoch: 20 batch_num: 93 loss: 1.0279 acc: 1.0\n",
      "epoch: 20 batch_num: 94 loss: 0.2754 acc: 1.0\n",
      "epoch: 20 batch_num: 95 loss: 0.25 acc: 1.0\n",
      "epoch: 20 batch_num: 96 loss: 0.8986 acc: 1.0\n",
      "epoch: 20 batch_num: 97 loss: 0.1706 acc: 1.0\n",
      "epoch: 20 batch_num: 98 loss: 0.2111 acc: 1.0\n",
      "epoch: 20 batch_num: 99 loss: 0.4862 acc: 1.0\n",
      "epoch: 20 batch_num: 100 loss: 0.2863 acc: 1.0\n",
      "epoch: 20 batch_num: 101 loss: 0.1724 acc: 1.0\n",
      "epoch: 20 batch_num: 102 loss: 0.1559 acc: 1.0\n",
      "epoch: 20 batch_num: 103 loss: 0.8433 acc: 1.0\n",
      "epoch: 20 batch_num: 104 loss: 0.3252 acc: 1.0\n",
      "epoch: 20 batch_num: 105 loss: 3.1043 acc: 0.984375\n",
      "epoch: 20 batch_num: 106 loss: 0.253 acc: 1.0\n",
      "epoch: 20 batch_num: 107 loss: 0.413 acc: 1.0\n",
      "epoch: 20 batch_num: 108 loss: 2.0228 acc: 0.984375\n",
      "epoch: 20 batch_num: 109 loss: 0.2223 acc: 1.0\n",
      "epoch: 20 batch_num: 110 loss: 0.6161 acc: 1.0\n",
      "epoch: 20 batch_num: 111 loss: 0.1898 acc: 1.0\n",
      "epoch: 20 batch_num: 112 loss: 0.3279 acc: 1.0\n",
      "epoch: 20 batch_num: 113 loss: 0.3659 acc: 1.0\n",
      "epoch: 20 batch_num: 114 loss: 0.2046 acc: 1.0\n",
      "epoch: 20 batch_num: 115 loss: 0.5124 acc: 1.0\n",
      "epoch: 20 batch_num: 116 loss: 0.0626 acc: 1.0\n",
      "max_acc: 1.0\n",
      "test acc: 0.6424242424242425\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # loss function\n",
    "english_model = AnswerableClassifier(vocab_size=20004, num_labels=2, num_hidden=100).to('cuda')\n",
    "optimizer = torch.optim.Adam(english_model.parameters(), lr = lr, amsgrad=True)\n",
    "\n",
    "max_acc = train_features_model(model = english_model, train_loader=train_features_model_loader,\n",
    "                               criterion= criterion, optimizer=optimizer, model_file_name=\"english_model.pth\",\n",
    "                               epochs = 20)\n",
    "print(\"max_acc:\", max_acc)\n",
    "english_model.load_state_dict(torch.load(\"english_model.pth\"))\n",
    "english_model.eval()\n",
    "predict_label = english_model(val_features)\n",
    "pred = predict_label.max(-1, keepdim=True)[1]\n",
    "label = val_label\n",
    "test_acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "\n",
    "print(\"test acc:\", test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # loss function\n",
    "japanese_model = AnswerableClassifier(vocab_size=20004, num_labels=2, num_hidden=100).to('cuda')\n",
    "optimizer = torch.optim.Adam(japanese_model.parameters(), lr = lr, amsgrad=True)\n",
    "\n",
    "max_acc = train_features_model(model = japanese_model, train_loader=train_features_model_loader,\n",
    "                               criterion= criterion, optimizer=optimizer, model_file_name=\"japanese_model.pth\")\n",
    "print(\"max_acc:\", max_acc)\n",
    "japanese_model.load_state_dict(torch.load(\"japanese_model.pth\"))\n",
    "japanese_model.eval()\n",
    "predict_label = japanese_model(val_features)\n",
    "pred = predict_label.max(-1, keepdim=True)[1]\n",
    "label = val_label\n",
    "test_acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "\n",
    "print(\"test acc:\", test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
