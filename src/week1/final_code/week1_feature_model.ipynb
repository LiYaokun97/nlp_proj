{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "'''\n",
    "@File    : data_processing.py\n",
    "@IDE     : PyCharm\n",
    "@Author  : Yaokun Li\n",
    "@Date    : 2022/10/18 20:30\n",
    "@Description :\n",
    "'''\n",
    "\n",
    "import gensim\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import trigrams\n",
    "\n",
    "\n",
    "def getLanguageDataSet(data, language):\n",
    "    return data.filter(lambda x: x['language'] == language)\n",
    "\n",
    "\n",
    "def getJapaneseDataSet(data):\n",
    "    return getLanguageDataSet(data, \"japanese\")\n",
    "\n",
    "\n",
    "def getEnglishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"english\")\n",
    "\n",
    "\n",
    "def getFinnishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"finnish\")\n",
    "\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "UNK, PAD = '<UNK>', '<PAD>'\n",
    "\n",
    "\n",
    "def build_vocab(sent_list, max_size, min_freq, tokenizer):\n",
    "    vocab_dic = {}\n",
    "    for sent in sent_list:\n",
    "        for word in tokenizer(sent):\n",
    "            vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "    print(len(vocab_dic))\n",
    "    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[\n",
    "                 :max_size]\n",
    "    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n",
    "    print(\"final voc length:\", len(vocab_dic))\n",
    "    return vocab_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataSet():\n",
    "    English_vocab = {}\n",
    "    Japanese_vocab = {}\n",
    "    Finnish_vocab = {}\n",
    "    def __init__(self, tokenizer, dataset,language = \"english\", vocab_size = 10000):\n",
    "        self.vocabulary = None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.question = []\n",
    "        self.answer_text = []\n",
    "        self.answer_start = []\n",
    "        self.document = []\n",
    "        self.tokenized_question = []\n",
    "        self.tokenized_answer_text = []\n",
    "        self.tokenized_document = []\n",
    "        self.answer_label = []\n",
    "        self.vocab_size = vocab_size\n",
    "        self.language=language\n",
    "\n",
    "        for element in dataset:\n",
    "            self.question.append(element[\"question_text\"].lower())\n",
    "            self.answer_text.append(element[\"annotations\"][\"answer_text\"][0])\n",
    "            self.answer_start.append(element[\"annotations\"][\"answer_start\"])\n",
    "            self.document.append(element[\"document_plaintext\"].lower())\n",
    "            if (element[\"annotations\"][\"answer_start\"] == [-1]):\n",
    "                self.answer_label.append(torch.tensor([0], dtype=torch.int64).cuda())\n",
    "            else:\n",
    "                self.answer_label.append(torch.tensor([1], dtype=torch.int64).cuda())\n",
    "\n",
    "\n",
    "        for s in self.answer_text:\n",
    "            self.tokenized_answer_text.append(self.__tokenize(s))\n",
    "\n",
    "        for s in self.question:\n",
    "            self.tokenized_question.append(self.__tokenize(s))\n",
    "\n",
    "        for s in self.document:\n",
    "            self.tokenized_document.append(self.__tokenize(s))\n",
    "\n",
    "        self.get_vocab(language)\n",
    "        self.document_num = []\n",
    "        self.question_num = []\n",
    "        for sent in self.tokenized_document:\n",
    "            self.document_num.append([self.vocabulary.get(word, self.vocab_size) for word in sent])\n",
    "        for sent in self.tokenized_question:\n",
    "            self.question_num.append([self.vocabulary.get(word, self.vocab_size) for word in sent])\n",
    "\n",
    "    def get_vocab(self, language):\n",
    "        if language == \"english\":\n",
    "            if len(QADataSet.English_vocab) != 0:\n",
    "                self.vocabulary = QADataSet.English_vocab\n",
    "            else:\n",
    "                QADataSet.English_vocab = build_vocab(self.question + self.document, self.vocab_size, 2, self.tokenizer)\n",
    "                self.vocabulary = QADataSet.English_vocab\n",
    "        elif language == \"japanese\":\n",
    "            if len(QADataSet.Japanese_vocab) != 0:\n",
    "                self.vocabulary = QADataSet.Japanese_vocab\n",
    "            else:\n",
    "                QADataSet.Japanese_vocab = build_vocab(self.question + self.document, self.vocab_size, 2, self.tokenizer)\n",
    "                self.vocabulary = QADataSet.Japanese_vocab\n",
    "        elif language == \"finnish\":\n",
    "            if len(QADataSet.Finnish_vocab) != 0:\n",
    "                self.vocabulary = QADataSet.Finnish_vocab\n",
    "            else:\n",
    "                QADataSet.Finnish_vocab = build_vocab(self.question + self.document, self.vocab_size, 2, self.tokenizer)\n",
    "                self.vocabulary = QADataSet.Finnish_vocab\n",
    "        return self.vocabulary\n",
    "\n",
    "    def __tokenize(self, l, with_stop_word=True):\n",
    "        return self.tokenizer(l)\n",
    "\n",
    "    def get_overlaps_words_num(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.question_num, self.document_num):\n",
    "            count = 0\n",
    "            for word in question:\n",
    "                if word in document:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_document_length(self):\n",
    "        return [len(document) for document in self.document_num]\n",
    "\n",
    "    def get_question_length(self):\n",
    "        return [len(question) for question in self.question_num]\n",
    "\n",
    "    def get_overlaps_2_gram(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.tokenized_question, self.tokenized_document):\n",
    "            count = 0\n",
    "            doc_bigrams = list(bigrams(document))\n",
    "            for word in bigrams(question):\n",
    "                if word in doc_bigrams:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_overlaps_3_gram(self):\n",
    "        overlaps_words_num = []\n",
    "        for question, document in zip(self.tokenized_question, self.tokenized_document):\n",
    "            count = 0\n",
    "            doc_bigrams = list(trigrams(document))\n",
    "            for word in trigrams(question):\n",
    "                if word in doc_bigrams:\n",
    "                    count = count + 1\n",
    "            overlaps_words_num.append(count)\n",
    "        return overlaps_words_num\n",
    "\n",
    "    def get_label(self):\n",
    "        return torch.cat(self.answer_label, dim=0)\n",
    "\n",
    "    def get_question_bow(self, vocab_size):\n",
    "        data = []\n",
    "        for ques in self.question_num:\n",
    "            bow = [0]*vocab_size\n",
    "            for word in ques:\n",
    "                bow[word] += 1\n",
    "            data.append(bow)\n",
    "        return data\n",
    "\n",
    "    def get_doc_bow(self, vocab_size):\n",
    "        data = []\n",
    "        for ques in self.document_num:\n",
    "            bow = [0] * vocab_size\n",
    "            for word in ques:\n",
    "                bow[word] += 1\n",
    "            data.append(bow)\n",
    "        return data\n",
    "\n",
    "    def get_features(self):\n",
    "        feature1 = self.get_overlaps_words_num()\n",
    "        feature2 = self.get_overlaps_2_gram()\n",
    "        feature5 = self.get_overlaps_3_gram()\n",
    "        feature3 = self.get_document_length()\n",
    "        feature4 = self.get_question_length()\n",
    "        feature_ques_bow = torch.Tensor(self.get_question_bow(self.vocab_size + 1)).cuda()\n",
    "        feature_doc_bow = torch.Tensor(self.get_doc_bow(self.vocab_size + 1)).cuda()\n",
    "        X = torch.Tensor([feature1,feature2, feature3, feature4,feature5]).t().cuda()\n",
    "        return torch.cat([feature_ques_bow,feature_doc_bow, X], dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023061275482177734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016a20f461e94851812cab6c40ec4d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AnswerableClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels = 2, num_hidden = 20):\n",
    "        super(AnswerableClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(num_hidden, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return self.final(self.nonlinear(self.dropout(self.linear(bow_vec))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_features_model( model, train_loader, criterion, optimizer, model_file_name, epochs):\n",
    "    max_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_num = 0\n",
    "\n",
    "        for features, label in train_loader:\n",
    "            predict_label = model(features)\n",
    "            loss = criterion(predict_label, label)\n",
    "\n",
    "            pred = predict_label.max(-1, keepdim=True)[1]\n",
    "            acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            if (acc > max_acc):\n",
    "                max_acc = acc\n",
    "                torch.save(model.state_dict(), model_file_name)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_num += 1\n",
    "            print(\"epoch:\", epoch + 1, \"batch_num:\", batch_num, \"loss:\", round(loss.item(), 4), \"acc:\", acc)\n",
    "    return max_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "torch_tokenizer = get_tokenizer('basic_english', language=\"en\")\n",
    "English_vocab_size = 30000\n",
    "\n",
    "english_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "def new_english_tokenizer(sent):\n",
    "    return [token.text for token in english_tokenizer(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-1103e6c04ff44af3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61385\n",
      "final voc length: 27211\n"
     ]
    }
   ],
   "source": [
    "train_english_qa_dataset = QADataSet(new_english_tokenizer, getEnglishDataSet(train_set), language = \"english\", vocab_size = English_vocab_size)\n",
    "train_features = train_english_qa_dataset.get_features()\n",
    "train_label = train_english_qa_dataset.get_label()\n",
    "train_features_model_dataset = Data.TensorDataset(train_features, train_label)\n",
    "train_features_model_loader = Data.DataLoader(dataset=train_features_model_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-dabfcfd450c9224c.arrow\n"
     ]
    }
   ],
   "source": [
    "val_english_qa_dataset = QADataSet(new_english_tokenizer, getEnglishDataSet(validation_set),  vocab_size = English_vocab_size)\n",
    "val_features = val_english_qa_dataset.get_features()\n",
    "val_label = val_english_qa_dataset.get_label()\n",
    "val_features_model_dataset = Data.TensorDataset(val_features, val_label)\n",
    "val_features_model_loader = Data.DataLoader(dataset=val_features_model_dataset,\n",
    "                                            batch_size= batch_size,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 1 loss: 44.8093 acc: 0.453125\n",
      "epoch: 1 batch_num: 2 loss: 44.7177 acc: 0.515625\n",
      "epoch: 1 batch_num: 3 loss: 42.5308 acc: 0.625\n",
      "epoch: 1 batch_num: 4 loss: 44.5481 acc: 0.484375\n",
      "epoch: 1 batch_num: 5 loss: 43.1645 acc: 0.546875\n",
      "epoch: 1 batch_num: 6 loss: 42.9111 acc: 0.609375\n",
      "epoch: 1 batch_num: 7 loss: 45.1337 acc: 0.484375\n",
      "epoch: 1 batch_num: 8 loss: 46.6303 acc: 0.46875\n",
      "epoch: 1 batch_num: 9 loss: 44.0609 acc: 0.484375\n",
      "epoch: 1 batch_num: 10 loss: 44.0736 acc: 0.453125\n",
      "epoch: 1 batch_num: 11 loss: 40.3666 acc: 0.703125\n",
      "epoch: 1 batch_num: 12 loss: 42.5462 acc: 0.625\n",
      "epoch: 1 batch_num: 13 loss: 42.5296 acc: 0.5625\n",
      "epoch: 1 batch_num: 14 loss: 42.3705 acc: 0.671875\n",
      "epoch: 1 batch_num: 15 loss: 43.3426 acc: 0.625\n",
      "epoch: 1 batch_num: 16 loss: 42.3368 acc: 0.671875\n",
      "epoch: 1 batch_num: 17 loss: 40.8403 acc: 0.65625\n",
      "epoch: 1 batch_num: 18 loss: 41.3647 acc: 0.6875\n",
      "epoch: 1 batch_num: 19 loss: 41.5203 acc: 0.609375\n",
      "epoch: 1 batch_num: 20 loss: 42.884 acc: 0.65625\n",
      "epoch: 1 batch_num: 21 loss: 40.1187 acc: 0.765625\n",
      "epoch: 1 batch_num: 22 loss: 40.4341 acc: 0.71875\n",
      "epoch: 1 batch_num: 23 loss: 41.7357 acc: 0.609375\n",
      "epoch: 1 batch_num: 24 loss: 42.4004 acc: 0.609375\n",
      "epoch: 1 batch_num: 25 loss: 42.3591 acc: 0.65625\n",
      "epoch: 1 batch_num: 26 loss: 40.4585 acc: 0.6875\n",
      "epoch: 1 batch_num: 27 loss: 40.4185 acc: 0.703125\n",
      "epoch: 1 batch_num: 28 loss: 41.0779 acc: 0.671875\n",
      "epoch: 1 batch_num: 29 loss: 41.8802 acc: 0.65625\n",
      "epoch: 1 batch_num: 30 loss: 40.516 acc: 0.609375\n",
      "epoch: 1 batch_num: 31 loss: 41.0311 acc: 0.609375\n",
      "epoch: 1 batch_num: 32 loss: 40.0403 acc: 0.640625\n",
      "epoch: 1 batch_num: 33 loss: 41.1066 acc: 0.765625\n",
      "epoch: 1 batch_num: 34 loss: 41.403 acc: 0.734375\n",
      "epoch: 1 batch_num: 35 loss: 42.4932 acc: 0.703125\n",
      "epoch: 1 batch_num: 36 loss: 40.5081 acc: 0.6875\n",
      "epoch: 1 batch_num: 37 loss: 35.562 acc: 0.765625\n",
      "epoch: 1 batch_num: 38 loss: 39.1067 acc: 0.734375\n",
      "epoch: 1 batch_num: 39 loss: 38.7616 acc: 0.734375\n",
      "epoch: 1 batch_num: 40 loss: 37.6569 acc: 0.8125\n",
      "epoch: 1 batch_num: 41 loss: 40.3903 acc: 0.75\n",
      "epoch: 1 batch_num: 42 loss: 39.7804 acc: 0.6875\n",
      "epoch: 1 batch_num: 43 loss: 39.2508 acc: 0.671875\n",
      "epoch: 1 batch_num: 44 loss: 44.3676 acc: 0.703125\n",
      "epoch: 1 batch_num: 45 loss: 38.263 acc: 0.71875\n",
      "epoch: 1 batch_num: 46 loss: 38.9774 acc: 0.671875\n",
      "epoch: 1 batch_num: 47 loss: 40.3705 acc: 0.671875\n",
      "epoch: 1 batch_num: 48 loss: 40.9844 acc: 0.65625\n",
      "epoch: 1 batch_num: 49 loss: 38.101 acc: 0.703125\n",
      "epoch: 1 batch_num: 50 loss: 37.5738 acc: 0.6875\n",
      "epoch: 1 batch_num: 51 loss: 38.8279 acc: 0.703125\n",
      "epoch: 1 batch_num: 52 loss: 40.3798 acc: 0.71875\n",
      "epoch: 1 batch_num: 53 loss: 36.58 acc: 0.796875\n",
      "epoch: 1 batch_num: 54 loss: 37.12 acc: 0.78125\n",
      "epoch: 1 batch_num: 55 loss: 37.8787 acc: 0.765625\n",
      "epoch: 1 batch_num: 56 loss: 39.5168 acc: 0.703125\n",
      "epoch: 1 batch_num: 57 loss: 36.1025 acc: 0.765625\n",
      "epoch: 1 batch_num: 58 loss: 42.2611 acc: 0.640625\n",
      "epoch: 1 batch_num: 59 loss: 33.5046 acc: 0.859375\n",
      "epoch: 1 batch_num: 60 loss: 36.9878 acc: 0.71875\n",
      "epoch: 1 batch_num: 61 loss: 35.9116 acc: 0.828125\n",
      "epoch: 1 batch_num: 62 loss: 36.8989 acc: 0.78125\n",
      "epoch: 1 batch_num: 63 loss: 36.9733 acc: 0.84375\n",
      "epoch: 1 batch_num: 64 loss: 33.779 acc: 0.75\n",
      "epoch: 1 batch_num: 65 loss: 36.5432 acc: 0.75\n",
      "epoch: 1 batch_num: 66 loss: 32.4873 acc: 0.875\n",
      "epoch: 1 batch_num: 67 loss: 32.6749 acc: 0.796875\n",
      "epoch: 1 batch_num: 68 loss: 38.1739 acc: 0.625\n",
      "epoch: 1 batch_num: 69 loss: 39.6305 acc: 0.75\n",
      "epoch: 1 batch_num: 70 loss: 35.6873 acc: 0.71875\n",
      "epoch: 1 batch_num: 71 loss: 34.8427 acc: 0.78125\n",
      "epoch: 1 batch_num: 72 loss: 34.7965 acc: 0.75\n",
      "epoch: 1 batch_num: 73 loss: 35.8451 acc: 0.796875\n",
      "epoch: 1 batch_num: 74 loss: 39.1037 acc: 0.6875\n",
      "epoch: 1 batch_num: 75 loss: 37.3882 acc: 0.75\n",
      "epoch: 1 batch_num: 76 loss: 37.0347 acc: 0.6875\n",
      "epoch: 1 batch_num: 77 loss: 35.6366 acc: 0.765625\n",
      "epoch: 1 batch_num: 78 loss: 34.3659 acc: 0.8125\n",
      "epoch: 1 batch_num: 79 loss: 32.347 acc: 0.84375\n",
      "epoch: 1 batch_num: 80 loss: 33.5271 acc: 0.765625\n",
      "epoch: 1 batch_num: 81 loss: 34.4405 acc: 0.765625\n",
      "epoch: 1 batch_num: 82 loss: 40.9709 acc: 0.71875\n",
      "epoch: 1 batch_num: 83 loss: 34.2673 acc: 0.796875\n",
      "epoch: 1 batch_num: 84 loss: 34.3497 acc: 0.75\n",
      "epoch: 1 batch_num: 85 loss: 32.1064 acc: 0.828125\n",
      "epoch: 1 batch_num: 86 loss: 33.4585 acc: 0.765625\n",
      "epoch: 1 batch_num: 87 loss: 38.7816 acc: 0.703125\n",
      "epoch: 1 batch_num: 88 loss: 35.2377 acc: 0.796875\n",
      "epoch: 1 batch_num: 89 loss: 30.9369 acc: 0.828125\n",
      "epoch: 1 batch_num: 90 loss: 33.3485 acc: 0.765625\n",
      "epoch: 1 batch_num: 91 loss: 34.128 acc: 0.78125\n",
      "epoch: 1 batch_num: 92 loss: 35.6942 acc: 0.671875\n",
      "epoch: 1 batch_num: 93 loss: 31.0878 acc: 0.8125\n",
      "epoch: 1 batch_num: 94 loss: 30.767 acc: 0.859375\n",
      "epoch: 1 batch_num: 95 loss: 31.6255 acc: 0.78125\n",
      "epoch: 1 batch_num: 96 loss: 35.6645 acc: 0.71875\n",
      "epoch: 1 batch_num: 97 loss: 36.0972 acc: 0.734375\n",
      "epoch: 1 batch_num: 98 loss: 35.9868 acc: 0.71875\n",
      "epoch: 1 batch_num: 99 loss: 31.6692 acc: 0.78125\n",
      "epoch: 1 batch_num: 100 loss: 31.3132 acc: 0.796875\n",
      "epoch: 1 batch_num: 101 loss: 29.0933 acc: 0.859375\n",
      "epoch: 1 batch_num: 102 loss: 32.7559 acc: 0.734375\n",
      "epoch: 1 batch_num: 103 loss: 36.9502 acc: 0.828125\n",
      "epoch: 1 batch_num: 104 loss: 32.6775 acc: 0.796875\n",
      "epoch: 1 batch_num: 105 loss: 31.9196 acc: 0.84375\n",
      "epoch: 1 batch_num: 106 loss: 34.1189 acc: 0.75\n",
      "epoch: 1 batch_num: 107 loss: 29.1057 acc: 0.8125\n",
      "epoch: 1 batch_num: 108 loss: 30.5942 acc: 0.796875\n",
      "epoch: 1 batch_num: 109 loss: 32.244 acc: 0.765625\n",
      "epoch: 1 batch_num: 110 loss: 31.6645 acc: 0.75\n",
      "epoch: 1 batch_num: 111 loss: 33.4789 acc: 0.71875\n",
      "epoch: 1 batch_num: 112 loss: 27.9794 acc: 0.84375\n",
      "epoch: 1 batch_num: 113 loss: 29.1162 acc: 0.859375\n",
      "epoch: 1 batch_num: 114 loss: 34.8439 acc: 0.78125\n",
      "epoch: 1 batch_num: 115 loss: 33.8088 acc: 0.734375\n",
      "epoch: 1 batch_num: 116 loss: 42.164 acc: 0.765625\n",
      "epoch: 1 batch_num: 117 loss: 28.2575 acc: 0.859375\n",
      "epoch: 1 batch_num: 118 loss: 30.5237 acc: 0.765625\n",
      "epoch: 1 batch_num: 119 loss: 34.1329 acc: 0.78125\n",
      "epoch: 1 batch_num: 120 loss: 31.1173 acc: 0.8125\n",
      "epoch: 1 batch_num: 121 loss: 44.1597 acc: 0.78125\n",
      "epoch: 1 batch_num: 122 loss: 37.1108 acc: 0.75\n",
      "epoch: 1 batch_num: 123 loss: 34.0044 acc: 0.703125\n",
      "epoch: 1 batch_num: 124 loss: 34.8757 acc: 0.8125\n",
      "epoch: 1 batch_num: 125 loss: 40.3319 acc: 0.796875\n",
      "epoch: 1 batch_num: 126 loss: 31.2275 acc: 0.8125\n",
      "epoch: 1 batch_num: 127 loss: 29.5335 acc: 0.8125\n",
      "epoch: 1 batch_num: 128 loss: 29.3898 acc: 0.828125\n",
      "epoch: 1 batch_num: 129 loss: 34.5666 acc: 0.796875\n",
      "epoch: 1 batch_num: 130 loss: 29.4179 acc: 0.828125\n",
      "epoch: 1 batch_num: 131 loss: 30.8524 acc: 0.75\n",
      "epoch: 1 batch_num: 132 loss: 35.0567 acc: 0.75\n",
      "epoch: 1 batch_num: 133 loss: 35.9625 acc: 0.75\n",
      "epoch: 1 batch_num: 134 loss: 29.5973 acc: 0.8125\n",
      "epoch: 1 batch_num: 135 loss: 30.445 acc: 0.78125\n",
      "epoch: 1 batch_num: 136 loss: 34.541 acc: 0.71875\n",
      "epoch: 1 batch_num: 137 loss: 35.5518 acc: 0.703125\n",
      "epoch: 1 batch_num: 138 loss: 34.9846 acc: 0.75\n",
      "epoch: 1 batch_num: 139 loss: 30.4893 acc: 0.78125\n",
      "epoch: 1 batch_num: 140 loss: 34.2492 acc: 0.71875\n",
      "epoch: 1 batch_num: 141 loss: 28.2887 acc: 0.84375\n",
      "epoch: 1 batch_num: 142 loss: 26.8972 acc: 0.859375\n",
      "epoch: 1 batch_num: 143 loss: 29.2937 acc: 0.8125\n",
      "epoch: 1 batch_num: 144 loss: 30.0057 acc: 0.78125\n",
      "epoch: 1 batch_num: 145 loss: 27.2578 acc: 0.875\n",
      "epoch: 1 batch_num: 146 loss: 28.3591 acc: 0.796875\n",
      "epoch: 1 batch_num: 147 loss: 28.8593 acc: 0.78125\n",
      "epoch: 1 batch_num: 148 loss: 34.5965 acc: 0.765625\n",
      "epoch: 1 batch_num: 149 loss: 29.3541 acc: 0.78125\n",
      "epoch: 1 batch_num: 150 loss: 31.7464 acc: 0.796875\n",
      "epoch: 1 batch_num: 151 loss: 29.9755 acc: 0.8125\n",
      "epoch: 1 batch_num: 152 loss: 30.8255 acc: 0.796875\n",
      "epoch: 1 batch_num: 153 loss: 26.5711 acc: 0.828125\n",
      "epoch: 1 batch_num: 154 loss: 35.3513 acc: 0.6875\n",
      "epoch: 1 batch_num: 155 loss: 64.6984 acc: 0.796875\n",
      "epoch: 1 batch_num: 156 loss: 30.5585 acc: 0.75\n",
      "epoch: 1 batch_num: 157 loss: 28.8564 acc: 0.765625\n",
      "epoch: 1 batch_num: 158 loss: 27.0599 acc: 0.8125\n",
      "epoch: 1 batch_num: 159 loss: 32.11 acc: 0.6875\n",
      "epoch: 1 batch_num: 160 loss: 38.7186 acc: 0.6875\n",
      "epoch: 1 batch_num: 161 loss: 25.6791 acc: 0.828125\n",
      "epoch: 1 batch_num: 162 loss: 29.0411 acc: 0.75\n",
      "epoch: 1 batch_num: 163 loss: 30.9845 acc: 0.75\n",
      "epoch: 1 batch_num: 164 loss: 56.9571 acc: 0.703125\n",
      "epoch: 1 batch_num: 165 loss: 48.7236 acc: 0.765625\n",
      "epoch: 1 batch_num: 166 loss: 34.6921 acc: 0.75\n",
      "epoch: 1 batch_num: 167 loss: 36.1872 acc: 0.765625\n",
      "epoch: 1 batch_num: 168 loss: 27.9431 acc: 0.8125\n",
      "epoch: 1 batch_num: 169 loss: 30.6529 acc: 0.796875\n",
      "epoch: 1 batch_num: 170 loss: 30.6422 acc: 0.8125\n",
      "epoch: 1 batch_num: 171 loss: 28.7319 acc: 0.796875\n",
      "epoch: 1 batch_num: 172 loss: 29.4339 acc: 0.796875\n",
      "epoch: 1 batch_num: 173 loss: 37.2289 acc: 0.6875\n",
      "epoch: 1 batch_num: 174 loss: 30.2674 acc: 0.75\n",
      "epoch: 1 batch_num: 175 loss: 30.033 acc: 0.828125\n",
      "epoch: 1 batch_num: 176 loss: 35.9492 acc: 0.734375\n",
      "epoch: 1 batch_num: 177 loss: 26.1046 acc: 0.890625\n",
      "epoch: 1 batch_num: 178 loss: 31.6389 acc: 0.828125\n",
      "epoch: 1 batch_num: 179 loss: 29.1528 acc: 0.765625\n",
      "epoch: 1 batch_num: 180 loss: 36.1597 acc: 0.703125\n",
      "epoch: 1 batch_num: 181 loss: 38.3887 acc: 0.6875\n",
      "epoch: 1 batch_num: 182 loss: 28.369 acc: 0.859375\n",
      "epoch: 1 batch_num: 183 loss: 33.2671 acc: 0.75\n",
      "epoch: 1 batch_num: 184 loss: 26.0592 acc: 0.84375\n",
      "epoch: 1 batch_num: 185 loss: 34.2129 acc: 0.703125\n",
      "epoch: 1 batch_num: 186 loss: 33.5791 acc: 0.78125\n",
      "epoch: 1 batch_num: 187 loss: 33.4835 acc: 0.703125\n",
      "epoch: 1 batch_num: 188 loss: 36.9324 acc: 0.734375\n",
      "epoch: 1 batch_num: 189 loss: 28.8714 acc: 0.84375\n",
      "epoch: 1 batch_num: 190 loss: 27.6478 acc: 0.796875\n",
      "epoch: 1 batch_num: 191 loss: 28.8475 acc: 0.84375\n",
      "epoch: 1 batch_num: 192 loss: 25.2889 acc: 0.90625\n",
      "epoch: 1 batch_num: 193 loss: 31.7501 acc: 0.75\n",
      "epoch: 1 batch_num: 194 loss: 29.5521 acc: 0.78125\n",
      "epoch: 1 batch_num: 195 loss: 28.3537 acc: 0.78125\n",
      "epoch: 1 batch_num: 196 loss: 31.9653 acc: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 197 loss: 31.4266 acc: 0.734375\n",
      "epoch: 1 batch_num: 198 loss: 32.1646 acc: 0.765625\n",
      "epoch: 1 batch_num: 199 loss: 25.0558 acc: 0.828125\n",
      "epoch: 1 batch_num: 200 loss: 34.6397 acc: 0.78125\n",
      "epoch: 1 batch_num: 201 loss: 27.5995 acc: 0.875\n",
      "epoch: 1 batch_num: 202 loss: 22.9048 acc: 0.859375\n",
      "epoch: 1 batch_num: 203 loss: 30.2297 acc: 0.84375\n",
      "epoch: 1 batch_num: 204 loss: 26.7127 acc: 0.8125\n",
      "epoch: 1 batch_num: 205 loss: 32.5531 acc: 0.765625\n",
      "epoch: 1 batch_num: 206 loss: 27.6195 acc: 0.859375\n",
      "epoch: 1 batch_num: 207 loss: 31.2814 acc: 0.78125\n",
      "epoch: 1 batch_num: 208 loss: 29.2752 acc: 0.84375\n",
      "epoch: 1 batch_num: 209 loss: 26.5395 acc: 0.8125\n",
      "epoch: 1 batch_num: 210 loss: 27.9996 acc: 0.84375\n",
      "epoch: 1 batch_num: 211 loss: 36.965 acc: 0.734375\n",
      "epoch: 1 batch_num: 212 loss: 30.2268 acc: 0.828125\n",
      "epoch: 1 batch_num: 213 loss: 27.08 acc: 0.828125\n",
      "epoch: 1 batch_num: 214 loss: 26.8855 acc: 0.84375\n",
      "epoch: 1 batch_num: 215 loss: 2.891 acc: 0.4\n",
      "epoch: 2 batch_num: 1 loss: 21.3574 acc: 0.90625\n",
      "epoch: 2 batch_num: 2 loss: 25.1619 acc: 0.8125\n",
      "epoch: 2 batch_num: 3 loss: 19.9365 acc: 0.9375\n",
      "epoch: 2 batch_num: 4 loss: 24.3275 acc: 0.828125\n",
      "epoch: 2 batch_num: 5 loss: 21.6358 acc: 0.90625\n",
      "epoch: 2 batch_num: 6 loss: 21.174 acc: 0.859375\n",
      "epoch: 2 batch_num: 7 loss: 21.3029 acc: 0.890625\n",
      "epoch: 2 batch_num: 8 loss: 25.5152 acc: 0.8125\n",
      "epoch: 2 batch_num: 9 loss: 22.0616 acc: 0.875\n",
      "epoch: 2 batch_num: 10 loss: 29.2349 acc: 0.734375\n",
      "epoch: 2 batch_num: 11 loss: 22.1181 acc: 0.828125\n",
      "epoch: 2 batch_num: 12 loss: 25.6973 acc: 0.828125\n",
      "epoch: 2 batch_num: 13 loss: 24.1976 acc: 0.859375\n",
      "epoch: 2 batch_num: 14 loss: 26.8417 acc: 0.828125\n",
      "epoch: 2 batch_num: 15 loss: 25.652 acc: 0.796875\n",
      "epoch: 2 batch_num: 16 loss: 20.8597 acc: 0.875\n",
      "epoch: 2 batch_num: 17 loss: 24.1648 acc: 0.90625\n",
      "epoch: 2 batch_num: 18 loss: 22.443 acc: 0.859375\n",
      "epoch: 2 batch_num: 19 loss: 24.0986 acc: 0.84375\n",
      "epoch: 2 batch_num: 20 loss: 29.3409 acc: 0.75\n",
      "epoch: 2 batch_num: 21 loss: 22.2487 acc: 0.859375\n",
      "epoch: 2 batch_num: 22 loss: 27.3616 acc: 0.84375\n",
      "epoch: 2 batch_num: 23 loss: 28.3469 acc: 0.8125\n",
      "epoch: 2 batch_num: 24 loss: 36.057 acc: 0.75\n",
      "epoch: 2 batch_num: 25 loss: 27.5789 acc: 0.8125\n",
      "epoch: 2 batch_num: 26 loss: 37.4492 acc: 0.734375\n",
      "epoch: 2 batch_num: 27 loss: 21.4668 acc: 0.875\n",
      "epoch: 2 batch_num: 28 loss: 22.7787 acc: 0.890625\n",
      "epoch: 2 batch_num: 29 loss: 22.9626 acc: 0.875\n",
      "epoch: 2 batch_num: 30 loss: 25.5885 acc: 0.828125\n",
      "epoch: 2 batch_num: 31 loss: 25.2436 acc: 0.796875\n",
      "epoch: 2 batch_num: 32 loss: 19.6067 acc: 0.875\n",
      "epoch: 2 batch_num: 33 loss: 21.1489 acc: 0.921875\n",
      "epoch: 2 batch_num: 34 loss: 25.2281 acc: 0.8125\n",
      "epoch: 2 batch_num: 35 loss: 33.4561 acc: 0.78125\n",
      "epoch: 2 batch_num: 36 loss: 26.7681 acc: 0.859375\n",
      "epoch: 2 batch_num: 37 loss: 24.3832 acc: 0.828125\n",
      "epoch: 2 batch_num: 38 loss: 26.9568 acc: 0.828125\n",
      "epoch: 2 batch_num: 39 loss: 23.3638 acc: 0.859375\n",
      "epoch: 2 batch_num: 40 loss: 24.5661 acc: 0.828125\n",
      "epoch: 2 batch_num: 41 loss: 24.5479 acc: 0.875\n",
      "epoch: 2 batch_num: 42 loss: 24.4264 acc: 0.890625\n",
      "epoch: 2 batch_num: 43 loss: 35.1207 acc: 0.765625\n",
      "epoch: 2 batch_num: 44 loss: 24.5536 acc: 0.8125\n",
      "epoch: 2 batch_num: 45 loss: 20.8441 acc: 0.859375\n",
      "epoch: 2 batch_num: 46 loss: 31.4014 acc: 0.828125\n",
      "epoch: 2 batch_num: 47 loss: 25.488 acc: 0.8125\n",
      "epoch: 2 batch_num: 48 loss: 22.0085 acc: 0.875\n",
      "epoch: 2 batch_num: 49 loss: 25.0583 acc: 0.84375\n",
      "epoch: 2 batch_num: 50 loss: 35.9397 acc: 0.765625\n",
      "epoch: 2 batch_num: 51 loss: 25.6135 acc: 0.796875\n",
      "epoch: 2 batch_num: 52 loss: 24.5912 acc: 0.875\n",
      "epoch: 2 batch_num: 53 loss: 20.4192 acc: 0.90625\n",
      "epoch: 2 batch_num: 54 loss: 23.3402 acc: 0.90625\n",
      "epoch: 2 batch_num: 55 loss: 23.9934 acc: 0.828125\n",
      "epoch: 2 batch_num: 56 loss: 36.9209 acc: 0.6875\n",
      "epoch: 2 batch_num: 57 loss: 29.2243 acc: 0.78125\n",
      "epoch: 2 batch_num: 58 loss: 27.9696 acc: 0.8125\n",
      "epoch: 2 batch_num: 59 loss: 26.3284 acc: 0.796875\n",
      "epoch: 2 batch_num: 60 loss: 22.437 acc: 0.890625\n",
      "epoch: 2 batch_num: 61 loss: 22.0161 acc: 0.890625\n",
      "epoch: 2 batch_num: 62 loss: 24.6542 acc: 0.828125\n",
      "epoch: 2 batch_num: 63 loss: 19.3914 acc: 0.890625\n",
      "epoch: 2 batch_num: 64 loss: 20.7479 acc: 0.859375\n",
      "epoch: 2 batch_num: 65 loss: 18.2296 acc: 0.90625\n",
      "epoch: 2 batch_num: 66 loss: 31.559 acc: 0.78125\n",
      "epoch: 2 batch_num: 67 loss: 24.5387 acc: 0.890625\n",
      "epoch: 2 batch_num: 68 loss: 19.287 acc: 0.9375\n",
      "epoch: 2 batch_num: 69 loss: 21.3927 acc: 0.84375\n",
      "epoch: 2 batch_num: 70 loss: 19.9911 acc: 0.890625\n",
      "epoch: 2 batch_num: 71 loss: 25.5982 acc: 0.8125\n",
      "epoch: 2 batch_num: 72 loss: 24.8107 acc: 0.859375\n",
      "epoch: 2 batch_num: 73 loss: 25.8042 acc: 0.84375\n",
      "epoch: 2 batch_num: 74 loss: 25.2732 acc: 0.8125\n",
      "epoch: 2 batch_num: 75 loss: 25.7005 acc: 0.84375\n",
      "epoch: 2 batch_num: 76 loss: 27.8474 acc: 0.828125\n",
      "epoch: 2 batch_num: 77 loss: 27.5591 acc: 0.78125\n",
      "epoch: 2 batch_num: 78 loss: 17.544 acc: 0.875\n",
      "epoch: 2 batch_num: 79 loss: 31.1597 acc: 0.703125\n",
      "epoch: 2 batch_num: 80 loss: 17.1625 acc: 0.9375\n",
      "epoch: 2 batch_num: 81 loss: 21.3796 acc: 0.875\n",
      "epoch: 2 batch_num: 82 loss: 22.9919 acc: 0.8125\n",
      "epoch: 2 batch_num: 83 loss: 21.2766 acc: 0.875\n",
      "epoch: 2 batch_num: 84 loss: 21.4511 acc: 0.890625\n",
      "epoch: 2 batch_num: 85 loss: 29.5023 acc: 0.78125\n",
      "epoch: 2 batch_num: 86 loss: 24.6241 acc: 0.828125\n",
      "epoch: 2 batch_num: 87 loss: 18.6024 acc: 0.890625\n",
      "epoch: 2 batch_num: 88 loss: 20.2973 acc: 0.890625\n",
      "epoch: 2 batch_num: 89 loss: 21.6847 acc: 0.90625\n",
      "epoch: 2 batch_num: 90 loss: 20.7026 acc: 0.875\n",
      "epoch: 2 batch_num: 91 loss: 19.5268 acc: 0.9375\n",
      "epoch: 2 batch_num: 92 loss: 25.2521 acc: 0.859375\n",
      "epoch: 2 batch_num: 93 loss: 19.6626 acc: 0.84375\n",
      "epoch: 2 batch_num: 94 loss: 19.6342 acc: 0.9375\n",
      "epoch: 2 batch_num: 95 loss: 21.3472 acc: 0.859375\n",
      "epoch: 2 batch_num: 96 loss: 21.895 acc: 0.84375\n",
      "epoch: 2 batch_num: 97 loss: 23.7474 acc: 0.84375\n",
      "epoch: 2 batch_num: 98 loss: 21.1155 acc: 0.859375\n",
      "epoch: 2 batch_num: 99 loss: 23.0561 acc: 0.875\n",
      "epoch: 2 batch_num: 100 loss: 23.8689 acc: 0.859375\n",
      "epoch: 2 batch_num: 101 loss: 22.4463 acc: 0.875\n",
      "epoch: 2 batch_num: 102 loss: 26.3964 acc: 0.859375\n",
      "epoch: 2 batch_num: 103 loss: 18.6306 acc: 0.875\n",
      "epoch: 2 batch_num: 104 loss: 22.8018 acc: 0.84375\n",
      "epoch: 2 batch_num: 105 loss: 23.167 acc: 0.875\n",
      "epoch: 2 batch_num: 106 loss: 23.1066 acc: 0.84375\n",
      "epoch: 2 batch_num: 107 loss: 25.17 acc: 0.84375\n",
      "epoch: 2 batch_num: 108 loss: 24.966 acc: 0.828125\n",
      "epoch: 2 batch_num: 109 loss: 18.032 acc: 0.90625\n",
      "epoch: 2 batch_num: 110 loss: 25.5381 acc: 0.84375\n",
      "epoch: 2 batch_num: 111 loss: 19.3523 acc: 0.90625\n",
      "epoch: 2 batch_num: 112 loss: 21.5086 acc: 0.890625\n",
      "epoch: 2 batch_num: 113 loss: 28.8107 acc: 0.78125\n",
      "epoch: 2 batch_num: 114 loss: 24.734 acc: 0.84375\n",
      "epoch: 2 batch_num: 115 loss: 21.8393 acc: 0.796875\n",
      "epoch: 2 batch_num: 116 loss: 18.4002 acc: 0.875\n",
      "epoch: 2 batch_num: 117 loss: 24.4808 acc: 0.875\n",
      "epoch: 2 batch_num: 118 loss: 27.7079 acc: 0.765625\n",
      "epoch: 2 batch_num: 119 loss: 24.5152 acc: 0.796875\n",
      "epoch: 2 batch_num: 120 loss: 22.6292 acc: 0.859375\n",
      "epoch: 2 batch_num: 121 loss: 21.2589 acc: 0.890625\n",
      "epoch: 2 batch_num: 122 loss: 19.2435 acc: 0.921875\n",
      "epoch: 2 batch_num: 123 loss: 21.6514 acc: 0.859375\n",
      "epoch: 2 batch_num: 124 loss: 26.1117 acc: 0.8125\n",
      "epoch: 2 batch_num: 125 loss: 22.2014 acc: 0.84375\n",
      "epoch: 2 batch_num: 126 loss: 25.6254 acc: 0.828125\n",
      "epoch: 2 batch_num: 127 loss: 24.1135 acc: 0.859375\n",
      "epoch: 2 batch_num: 128 loss: 24.2029 acc: 0.8125\n",
      "epoch: 2 batch_num: 129 loss: 15.0065 acc: 0.921875\n",
      "epoch: 2 batch_num: 130 loss: 23.4361 acc: 0.828125\n",
      "epoch: 2 batch_num: 131 loss: 20.8687 acc: 0.90625\n",
      "epoch: 2 batch_num: 132 loss: 18.8255 acc: 0.875\n",
      "epoch: 2 batch_num: 133 loss: 52.2767 acc: 0.84375\n",
      "epoch: 2 batch_num: 134 loss: 19.9966 acc: 0.90625\n",
      "epoch: 2 batch_num: 135 loss: 26.9635 acc: 0.84375\n",
      "epoch: 2 batch_num: 136 loss: 21.833 acc: 0.890625\n",
      "epoch: 2 batch_num: 137 loss: 12.9483 acc: 0.953125\n",
      "epoch: 2 batch_num: 138 loss: 24.4394 acc: 0.8125\n",
      "epoch: 2 batch_num: 139 loss: 29.2753 acc: 0.796875\n",
      "epoch: 2 batch_num: 140 loss: 28.2407 acc: 0.796875\n",
      "epoch: 2 batch_num: 141 loss: 12.8219 acc: 0.953125\n",
      "epoch: 2 batch_num: 142 loss: 21.4621 acc: 0.859375\n",
      "epoch: 2 batch_num: 143 loss: 22.9633 acc: 0.84375\n",
      "epoch: 2 batch_num: 144 loss: 24.4265 acc: 0.84375\n",
      "epoch: 2 batch_num: 145 loss: 20.7724 acc: 0.90625\n",
      "epoch: 2 batch_num: 146 loss: 22.5271 acc: 0.875\n",
      "epoch: 2 batch_num: 147 loss: 27.2867 acc: 0.78125\n",
      "epoch: 2 batch_num: 148 loss: 13.9844 acc: 0.96875\n",
      "epoch: 2 batch_num: 149 loss: 26.3035 acc: 0.8125\n",
      "epoch: 2 batch_num: 150 loss: 19.2564 acc: 0.921875\n",
      "epoch: 2 batch_num: 151 loss: 18.0001 acc: 0.9375\n",
      "epoch: 2 batch_num: 152 loss: 26.1958 acc: 0.8125\n",
      "epoch: 2 batch_num: 153 loss: 20.3096 acc: 0.828125\n",
      "epoch: 2 batch_num: 154 loss: 20.4784 acc: 0.875\n",
      "epoch: 2 batch_num: 155 loss: 27.6919 acc: 0.78125\n",
      "epoch: 2 batch_num: 156 loss: 32.8037 acc: 0.765625\n",
      "epoch: 2 batch_num: 157 loss: 38.9726 acc: 0.703125\n",
      "epoch: 2 batch_num: 158 loss: 18.3455 acc: 0.90625\n",
      "epoch: 2 batch_num: 159 loss: 30.9147 acc: 0.765625\n",
      "epoch: 2 batch_num: 160 loss: 24.0072 acc: 0.875\n",
      "epoch: 2 batch_num: 161 loss: 16.8463 acc: 0.875\n",
      "epoch: 2 batch_num: 162 loss: 28.2816 acc: 0.8125\n",
      "epoch: 2 batch_num: 163 loss: 33.4761 acc: 0.875\n",
      "epoch: 2 batch_num: 164 loss: 19.5028 acc: 0.859375\n",
      "epoch: 2 batch_num: 165 loss: 27.2439 acc: 0.84375\n",
      "epoch: 2 batch_num: 166 loss: 24.3034 acc: 0.84375\n",
      "epoch: 2 batch_num: 167 loss: 14.4032 acc: 0.953125\n",
      "epoch: 2 batch_num: 168 loss: 41.8244 acc: 0.734375\n",
      "epoch: 2 batch_num: 169 loss: 19.3132 acc: 0.90625\n",
      "epoch: 2 batch_num: 170 loss: 19.8295 acc: 0.921875\n",
      "epoch: 2 batch_num: 171 loss: 21.7165 acc: 0.796875\n",
      "epoch: 2 batch_num: 172 loss: 23.4257 acc: 0.859375\n",
      "epoch: 2 batch_num: 173 loss: 23.7598 acc: 0.796875\n",
      "epoch: 2 batch_num: 174 loss: 22.8582 acc: 0.875\n",
      "epoch: 2 batch_num: 175 loss: 24.9935 acc: 0.84375\n",
      "epoch: 2 batch_num: 176 loss: 27.5063 acc: 0.78125\n",
      "epoch: 2 batch_num: 177 loss: 30.7234 acc: 0.765625\n",
      "epoch: 2 batch_num: 178 loss: 18.833 acc: 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch_num: 179 loss: 28.1441 acc: 0.796875\n",
      "epoch: 2 batch_num: 180 loss: 26.1352 acc: 0.875\n",
      "epoch: 2 batch_num: 181 loss: 22.1133 acc: 0.875\n",
      "epoch: 2 batch_num: 182 loss: 27.6349 acc: 0.765625\n",
      "epoch: 2 batch_num: 183 loss: 23.6873 acc: 0.84375\n",
      "epoch: 2 batch_num: 184 loss: 22.9334 acc: 0.875\n",
      "epoch: 2 batch_num: 185 loss: 21.4117 acc: 0.859375\n",
      "epoch: 2 batch_num: 186 loss: 18.7234 acc: 0.890625\n",
      "epoch: 2 batch_num: 187 loss: 23.0184 acc: 0.859375\n",
      "epoch: 2 batch_num: 188 loss: 15.7587 acc: 0.9375\n",
      "epoch: 2 batch_num: 189 loss: 21.0098 acc: 0.875\n",
      "epoch: 2 batch_num: 190 loss: 19.7606 acc: 0.90625\n",
      "epoch: 2 batch_num: 191 loss: 22.6585 acc: 0.859375\n",
      "epoch: 2 batch_num: 192 loss: 23.1133 acc: 0.859375\n",
      "epoch: 2 batch_num: 193 loss: 19.1601 acc: 0.890625\n",
      "epoch: 2 batch_num: 194 loss: 22.3784 acc: 0.8125\n",
      "epoch: 2 batch_num: 195 loss: 24.541 acc: 0.875\n",
      "epoch: 2 batch_num: 196 loss: 26.8072 acc: 0.875\n",
      "epoch: 2 batch_num: 197 loss: 19.8098 acc: 0.890625\n",
      "epoch: 2 batch_num: 198 loss: 26.2972 acc: 0.859375\n",
      "epoch: 2 batch_num: 199 loss: 24.8905 acc: 0.8125\n",
      "epoch: 2 batch_num: 200 loss: 20.4539 acc: 0.90625\n",
      "epoch: 2 batch_num: 201 loss: 23.0963 acc: 0.828125\n",
      "epoch: 2 batch_num: 202 loss: 37.7156 acc: 0.6875\n",
      "epoch: 2 batch_num: 203 loss: 23.7298 acc: 0.84375\n",
      "epoch: 2 batch_num: 204 loss: 28.8665 acc: 0.765625\n",
      "epoch: 2 batch_num: 205 loss: 18.2292 acc: 0.890625\n",
      "epoch: 2 batch_num: 206 loss: 30.7356 acc: 0.796875\n",
      "epoch: 2 batch_num: 207 loss: 24.467 acc: 0.828125\n",
      "epoch: 2 batch_num: 208 loss: 19.2075 acc: 0.921875\n",
      "epoch: 2 batch_num: 209 loss: 25.4003 acc: 0.84375\n",
      "epoch: 2 batch_num: 210 loss: 26.5752 acc: 0.8125\n",
      "epoch: 2 batch_num: 211 loss: 24.2222 acc: 0.875\n",
      "epoch: 2 batch_num: 212 loss: 22.022 acc: 0.90625\n",
      "epoch: 2 batch_num: 213 loss: 14.257 acc: 0.953125\n",
      "epoch: 2 batch_num: 214 loss: 23.5944 acc: 0.796875\n",
      "epoch: 2 batch_num: 215 loss: 1.6251 acc: 0.8\n",
      "epoch: 3 batch_num: 1 loss: 28.8978 acc: 0.859375\n",
      "epoch: 3 batch_num: 2 loss: 23.5486 acc: 0.828125\n",
      "epoch: 3 batch_num: 3 loss: 19.9118 acc: 0.890625\n",
      "epoch: 3 batch_num: 4 loss: 12.5666 acc: 0.953125\n",
      "epoch: 3 batch_num: 5 loss: 19.9884 acc: 0.875\n",
      "epoch: 3 batch_num: 6 loss: 21.901 acc: 0.84375\n",
      "epoch: 3 batch_num: 7 loss: 18.4898 acc: 0.90625\n",
      "epoch: 3 batch_num: 8 loss: 17.8847 acc: 0.890625\n",
      "epoch: 3 batch_num: 9 loss: 15.4831 acc: 0.9375\n",
      "epoch: 3 batch_num: 10 loss: 19.3162 acc: 0.859375\n",
      "epoch: 3 batch_num: 11 loss: 16.1026 acc: 0.90625\n",
      "epoch: 3 batch_num: 12 loss: 18.3429 acc: 0.90625\n",
      "epoch: 3 batch_num: 13 loss: 15.9603 acc: 0.921875\n",
      "epoch: 3 batch_num: 14 loss: 13.594 acc: 0.953125\n",
      "epoch: 3 batch_num: 15 loss: 15.5874 acc: 0.9375\n",
      "epoch: 3 batch_num: 16 loss: 16.1496 acc: 0.921875\n",
      "epoch: 3 batch_num: 17 loss: 18.3014 acc: 0.921875\n",
      "epoch: 3 batch_num: 18 loss: 18.9087 acc: 0.890625\n",
      "epoch: 3 batch_num: 19 loss: 14.4756 acc: 0.890625\n",
      "epoch: 3 batch_num: 20 loss: 18.6637 acc: 0.890625\n",
      "epoch: 3 batch_num: 21 loss: 14.4862 acc: 0.90625\n",
      "epoch: 3 batch_num: 22 loss: 18.4399 acc: 0.875\n",
      "epoch: 3 batch_num: 23 loss: 14.8456 acc: 0.90625\n",
      "epoch: 3 batch_num: 24 loss: 11.3902 acc: 0.953125\n",
      "epoch: 3 batch_num: 25 loss: 19.578 acc: 0.890625\n",
      "epoch: 3 batch_num: 26 loss: 20.7285 acc: 0.859375\n",
      "epoch: 3 batch_num: 27 loss: 14.302 acc: 0.875\n",
      "epoch: 3 batch_num: 28 loss: 20.7334 acc: 0.890625\n",
      "epoch: 3 batch_num: 29 loss: 18.8375 acc: 0.890625\n",
      "epoch: 3 batch_num: 30 loss: 16.4938 acc: 0.875\n",
      "epoch: 3 batch_num: 31 loss: 13.5496 acc: 0.921875\n",
      "epoch: 3 batch_num: 32 loss: 19.5271 acc: 0.90625\n",
      "epoch: 3 batch_num: 33 loss: 19.4595 acc: 0.90625\n",
      "epoch: 3 batch_num: 34 loss: 25.8112 acc: 0.828125\n",
      "epoch: 3 batch_num: 35 loss: 20.0147 acc: 0.875\n",
      "epoch: 3 batch_num: 36 loss: 12.9645 acc: 0.953125\n",
      "epoch: 3 batch_num: 37 loss: 22.8013 acc: 0.875\n",
      "epoch: 3 batch_num: 38 loss: 13.4909 acc: 0.96875\n",
      "epoch: 3 batch_num: 39 loss: 19.0049 acc: 0.890625\n",
      "epoch: 3 batch_num: 40 loss: 13.5286 acc: 0.921875\n",
      "epoch: 3 batch_num: 41 loss: 13.0069 acc: 0.984375\n",
      "epoch: 3 batch_num: 42 loss: 16.1853 acc: 0.921875\n",
      "epoch: 3 batch_num: 43 loss: 15.2994 acc: 0.9375\n",
      "epoch: 3 batch_num: 44 loss: 14.8773 acc: 0.921875\n",
      "epoch: 3 batch_num: 45 loss: 15.6976 acc: 0.890625\n",
      "epoch: 3 batch_num: 46 loss: 21.6966 acc: 0.875\n",
      "epoch: 3 batch_num: 47 loss: 16.8366 acc: 0.921875\n",
      "epoch: 3 batch_num: 48 loss: 13.1862 acc: 0.953125\n",
      "epoch: 3 batch_num: 49 loss: 10.5114 acc: 0.953125\n",
      "epoch: 3 batch_num: 50 loss: 22.9342 acc: 0.84375\n",
      "epoch: 3 batch_num: 51 loss: 13.7192 acc: 0.90625\n",
      "epoch: 3 batch_num: 52 loss: 16.4199 acc: 0.90625\n",
      "epoch: 3 batch_num: 53 loss: 12.3356 acc: 0.921875\n",
      "epoch: 3 batch_num: 54 loss: 13.5017 acc: 0.9375\n",
      "epoch: 3 batch_num: 55 loss: 18.2045 acc: 0.90625\n",
      "epoch: 3 batch_num: 56 loss: 20.8141 acc: 0.859375\n",
      "epoch: 3 batch_num: 57 loss: 18.5473 acc: 0.859375\n",
      "epoch: 3 batch_num: 58 loss: 14.979 acc: 0.90625\n",
      "epoch: 3 batch_num: 59 loss: 17.2144 acc: 0.890625\n",
      "epoch: 3 batch_num: 60 loss: 13.9208 acc: 0.921875\n",
      "epoch: 3 batch_num: 61 loss: 18.9008 acc: 0.890625\n",
      "epoch: 3 batch_num: 62 loss: 13.3378 acc: 0.921875\n",
      "epoch: 3 batch_num: 63 loss: 16.1693 acc: 0.921875\n",
      "epoch: 3 batch_num: 64 loss: 20.5503 acc: 0.875\n",
      "epoch: 3 batch_num: 65 loss: 10.8544 acc: 0.96875\n",
      "epoch: 3 batch_num: 66 loss: 16.6601 acc: 0.921875\n",
      "epoch: 3 batch_num: 67 loss: 14.1034 acc: 0.90625\n",
      "epoch: 3 batch_num: 68 loss: 13.3212 acc: 0.921875\n",
      "epoch: 3 batch_num: 69 loss: 17.0612 acc: 0.921875\n",
      "epoch: 3 batch_num: 70 loss: 19.3551 acc: 0.84375\n",
      "epoch: 3 batch_num: 71 loss: 15.8695 acc: 0.90625\n",
      "epoch: 3 batch_num: 72 loss: 17.2869 acc: 0.875\n",
      "epoch: 3 batch_num: 73 loss: 14.1574 acc: 0.921875\n",
      "epoch: 3 batch_num: 74 loss: 16.822 acc: 0.890625\n",
      "epoch: 3 batch_num: 75 loss: 22.079 acc: 0.828125\n",
      "epoch: 3 batch_num: 76 loss: 14.1119 acc: 0.921875\n",
      "epoch: 3 batch_num: 77 loss: 42.1927 acc: 0.890625\n",
      "epoch: 3 batch_num: 78 loss: 19.3818 acc: 0.9375\n",
      "epoch: 3 batch_num: 79 loss: 13.4584 acc: 0.96875\n",
      "epoch: 3 batch_num: 80 loss: 15.9171 acc: 0.90625\n",
      "epoch: 3 batch_num: 81 loss: 16.0599 acc: 0.90625\n",
      "epoch: 3 batch_num: 82 loss: 12.6995 acc: 0.9375\n",
      "epoch: 3 batch_num: 83 loss: 13.8003 acc: 0.90625\n",
      "epoch: 3 batch_num: 84 loss: 19.397 acc: 0.890625\n",
      "epoch: 3 batch_num: 85 loss: 21.6166 acc: 0.84375\n",
      "epoch: 3 batch_num: 86 loss: 21.9764 acc: 0.859375\n",
      "epoch: 3 batch_num: 87 loss: 14.7986 acc: 0.890625\n",
      "epoch: 3 batch_num: 88 loss: 12.8645 acc: 0.9375\n",
      "epoch: 3 batch_num: 89 loss: 21.1847 acc: 0.828125\n",
      "epoch: 3 batch_num: 90 loss: 21.6457 acc: 0.875\n",
      "epoch: 3 batch_num: 91 loss: 12.01 acc: 0.9375\n",
      "epoch: 3 batch_num: 92 loss: 17.2477 acc: 0.9375\n",
      "epoch: 3 batch_num: 93 loss: 18.9032 acc: 0.90625\n",
      "epoch: 3 batch_num: 94 loss: 16.2896 acc: 0.890625\n",
      "epoch: 3 batch_num: 95 loss: 16.425 acc: 0.875\n",
      "epoch: 3 batch_num: 96 loss: 16.697 acc: 0.90625\n",
      "epoch: 3 batch_num: 97 loss: 15.6945 acc: 0.921875\n",
      "epoch: 3 batch_num: 98 loss: 11.3716 acc: 0.921875\n",
      "epoch: 3 batch_num: 99 loss: 17.8991 acc: 0.890625\n",
      "epoch: 3 batch_num: 100 loss: 14.1561 acc: 0.921875\n",
      "epoch: 3 batch_num: 101 loss: 17.0657 acc: 0.875\n",
      "epoch: 3 batch_num: 102 loss: 12.3748 acc: 0.921875\n",
      "epoch: 3 batch_num: 103 loss: 17.6337 acc: 0.90625\n",
      "epoch: 3 batch_num: 104 loss: 11.4773 acc: 0.9375\n",
      "epoch: 3 batch_num: 105 loss: 24.2542 acc: 0.78125\n",
      "epoch: 3 batch_num: 106 loss: 16.5056 acc: 0.890625\n",
      "epoch: 3 batch_num: 107 loss: 19.2531 acc: 0.90625\n",
      "epoch: 3 batch_num: 108 loss: 16.4867 acc: 0.90625\n",
      "epoch: 3 batch_num: 109 loss: 16.4941 acc: 0.921875\n",
      "epoch: 3 batch_num: 110 loss: 18.7679 acc: 0.875\n",
      "epoch: 3 batch_num: 111 loss: 16.0578 acc: 0.90625\n",
      "epoch: 3 batch_num: 112 loss: 19.3804 acc: 0.859375\n",
      "epoch: 3 batch_num: 113 loss: 22.0795 acc: 0.84375\n",
      "epoch: 3 batch_num: 114 loss: 14.99 acc: 0.90625\n",
      "epoch: 3 batch_num: 115 loss: 13.7093 acc: 0.921875\n",
      "epoch: 3 batch_num: 116 loss: 13.4941 acc: 0.90625\n",
      "epoch: 3 batch_num: 117 loss: 15.3836 acc: 0.921875\n",
      "epoch: 3 batch_num: 118 loss: 15.8198 acc: 0.875\n",
      "epoch: 3 batch_num: 119 loss: 22.8307 acc: 0.875\n",
      "epoch: 3 batch_num: 120 loss: 13.7094 acc: 0.953125\n",
      "epoch: 3 batch_num: 121 loss: 18.5993 acc: 0.890625\n",
      "epoch: 3 batch_num: 122 loss: 20.8994 acc: 0.875\n",
      "epoch: 3 batch_num: 123 loss: 18.4246 acc: 0.890625\n",
      "epoch: 3 batch_num: 124 loss: 17.3341 acc: 0.890625\n",
      "epoch: 3 batch_num: 125 loss: 15.4407 acc: 0.90625\n",
      "epoch: 3 batch_num: 126 loss: 18.1897 acc: 0.875\n",
      "epoch: 3 batch_num: 127 loss: 12.0975 acc: 0.921875\n",
      "epoch: 3 batch_num: 128 loss: 25.4242 acc: 0.84375\n",
      "epoch: 3 batch_num: 129 loss: 21.0822 acc: 0.828125\n",
      "epoch: 3 batch_num: 130 loss: 21.6946 acc: 0.875\n",
      "epoch: 3 batch_num: 131 loss: 14.3567 acc: 0.9375\n",
      "epoch: 3 batch_num: 132 loss: 18.5527 acc: 0.859375\n",
      "epoch: 3 batch_num: 133 loss: 17.4094 acc: 0.890625\n",
      "epoch: 3 batch_num: 134 loss: 18.0676 acc: 0.875\n",
      "epoch: 3 batch_num: 135 loss: 16.1848 acc: 0.921875\n",
      "epoch: 3 batch_num: 136 loss: 17.1363 acc: 0.890625\n",
      "epoch: 3 batch_num: 137 loss: 14.1251 acc: 0.890625\n",
      "epoch: 3 batch_num: 138 loss: 17.3757 acc: 0.875\n",
      "epoch: 3 batch_num: 139 loss: 17.9689 acc: 0.890625\n",
      "epoch: 3 batch_num: 140 loss: 15.7729 acc: 0.921875\n",
      "epoch: 3 batch_num: 141 loss: 14.076 acc: 0.9375\n",
      "epoch: 3 batch_num: 142 loss: 15.0159 acc: 0.953125\n",
      "epoch: 3 batch_num: 143 loss: 15.3047 acc: 0.921875\n",
      "epoch: 3 batch_num: 144 loss: 16.2329 acc: 0.890625\n",
      "epoch: 3 batch_num: 145 loss: 18.3195 acc: 0.90625\n",
      "epoch: 3 batch_num: 146 loss: 14.8417 acc: 0.90625\n",
      "epoch: 3 batch_num: 147 loss: 13.3319 acc: 0.9375\n",
      "epoch: 3 batch_num: 148 loss: 16.4025 acc: 0.921875\n",
      "epoch: 3 batch_num: 149 loss: 19.5488 acc: 0.875\n",
      "epoch: 3 batch_num: 150 loss: 15.9315 acc: 0.90625\n",
      "epoch: 3 batch_num: 151 loss: 14.7988 acc: 0.921875\n",
      "epoch: 3 batch_num: 152 loss: 24.4093 acc: 0.84375\n",
      "epoch: 3 batch_num: 153 loss: 15.4138 acc: 0.90625\n",
      "epoch: 3 batch_num: 154 loss: 15.3388 acc: 0.921875\n",
      "epoch: 3 batch_num: 155 loss: 22.4616 acc: 0.875\n",
      "epoch: 3 batch_num: 156 loss: 20.3792 acc: 0.890625\n",
      "epoch: 3 batch_num: 157 loss: 20.6366 acc: 0.84375\n",
      "epoch: 3 batch_num: 158 loss: 19.1817 acc: 0.8125\n",
      "epoch: 3 batch_num: 159 loss: 15.5672 acc: 0.90625\n",
      "epoch: 3 batch_num: 160 loss: 21.0834 acc: 0.875\n",
      "epoch: 3 batch_num: 161 loss: 23.1996 acc: 0.921875\n",
      "epoch: 3 batch_num: 162 loss: 11.9575 acc: 0.9375\n",
      "epoch: 3 batch_num: 163 loss: 12.4686 acc: 0.9375\n",
      "epoch: 3 batch_num: 164 loss: 9.5523 acc: 0.9375\n",
      "epoch: 3 batch_num: 165 loss: 21.5864 acc: 0.890625\n",
      "epoch: 3 batch_num: 166 loss: 18.1221 acc: 0.875\n",
      "epoch: 3 batch_num: 167 loss: 20.5757 acc: 0.859375\n",
      "epoch: 3 batch_num: 168 loss: 20.5556 acc: 0.859375\n",
      "epoch: 3 batch_num: 169 loss: 12.3669 acc: 0.921875\n",
      "epoch: 3 batch_num: 170 loss: 12.9674 acc: 0.9375\n",
      "epoch: 3 batch_num: 171 loss: 27.3527 acc: 0.8125\n",
      "epoch: 3 batch_num: 172 loss: 16.3533 acc: 0.859375\n",
      "epoch: 3 batch_num: 173 loss: 13.7337 acc: 0.921875\n",
      "epoch: 3 batch_num: 174 loss: 13.1396 acc: 0.953125\n",
      "epoch: 3 batch_num: 175 loss: 20.3011 acc: 0.875\n",
      "epoch: 3 batch_num: 176 loss: 12.8824 acc: 0.953125\n",
      "epoch: 3 batch_num: 177 loss: 23.8663 acc: 0.828125\n",
      "epoch: 3 batch_num: 178 loss: 14.6892 acc: 0.921875\n",
      "epoch: 3 batch_num: 179 loss: 22.4925 acc: 0.8125\n",
      "epoch: 3 batch_num: 180 loss: 20.6122 acc: 0.84375\n",
      "epoch: 3 batch_num: 181 loss: 16.319 acc: 0.890625\n",
      "epoch: 3 batch_num: 182 loss: 15.9011 acc: 0.890625\n",
      "epoch: 3 batch_num: 183 loss: 10.6709 acc: 0.96875\n",
      "epoch: 3 batch_num: 184 loss: 19.9347 acc: 0.84375\n",
      "epoch: 3 batch_num: 185 loss: 20.4686 acc: 0.890625\n",
      "epoch: 3 batch_num: 186 loss: 14.5428 acc: 0.921875\n",
      "epoch: 3 batch_num: 187 loss: 29.1811 acc: 0.84375\n",
      "epoch: 3 batch_num: 188 loss: 16.8711 acc: 0.9375\n",
      "epoch: 3 batch_num: 189 loss: 19.4747 acc: 0.84375\n",
      "epoch: 3 batch_num: 190 loss: 30.5013 acc: 0.765625\n",
      "epoch: 3 batch_num: 191 loss: 20.9827 acc: 0.84375\n",
      "epoch: 3 batch_num: 192 loss: 16.3227 acc: 0.90625\n",
      "epoch: 3 batch_num: 193 loss: 15.4594 acc: 0.921875\n",
      "epoch: 3 batch_num: 194 loss: 11.3063 acc: 0.953125\n",
      "epoch: 3 batch_num: 195 loss: 14.1906 acc: 0.921875\n",
      "epoch: 3 batch_num: 196 loss: 23.5536 acc: 0.890625\n",
      "epoch: 3 batch_num: 197 loss: 15.6479 acc: 0.890625\n",
      "epoch: 3 batch_num: 198 loss: 23.7498 acc: 0.765625\n",
      "epoch: 3 batch_num: 199 loss: 20.352 acc: 0.875\n",
      "epoch: 3 batch_num: 200 loss: 13.3138 acc: 0.921875\n",
      "epoch: 3 batch_num: 201 loss: 17.9828 acc: 0.875\n",
      "epoch: 3 batch_num: 202 loss: 13.4089 acc: 0.953125\n",
      "epoch: 3 batch_num: 203 loss: 17.2631 acc: 0.875\n",
      "epoch: 3 batch_num: 204 loss: 17.4249 acc: 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 batch_num: 205 loss: 26.9121 acc: 0.796875\n",
      "epoch: 3 batch_num: 206 loss: 17.5091 acc: 0.90625\n",
      "epoch: 3 batch_num: 207 loss: 15.856 acc: 0.890625\n",
      "epoch: 3 batch_num: 208 loss: 16.4125 acc: 0.875\n",
      "epoch: 3 batch_num: 209 loss: 20.1004 acc: 0.859375\n",
      "epoch: 3 batch_num: 210 loss: 20.4378 acc: 0.875\n",
      "epoch: 3 batch_num: 211 loss: 11.8911 acc: 0.953125\n",
      "epoch: 3 batch_num: 212 loss: 20.3502 acc: 0.859375\n",
      "epoch: 3 batch_num: 213 loss: 19.9837 acc: 0.84375\n",
      "epoch: 3 batch_num: 214 loss: 17.1052 acc: 0.921875\n",
      "epoch: 3 batch_num: 215 loss: 0.2911 acc: 1.0\n",
      "max_acc: 1.0\n",
      "test acc: 0.7882562277580071\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # loss function\n",
    "english_model = AnswerableClassifier(vocab_size= 2*English_vocab_size + 7, num_labels=2, num_hidden=100).to('cuda')\n",
    "optimizer = torch.optim.Adam(english_model.parameters(), lr = 0.0005, amsgrad=True)\n",
    "\n",
    "max_acc = train_features_model(model = english_model, train_loader=train_features_model_loader,\n",
    "                               criterion= criterion, optimizer=optimizer, model_file_name=\"english_model.pth\",\n",
    "                               epochs = 3)\n",
    "print(\"max_acc:\", max_acc)\n",
    "english_model.load_state_dict(torch.load(\"english_model.pth\"))\n",
    "english_model.eval()\n",
    "predict_label = english_model(val_features)\n",
    "pred = predict_label.max(-1, keepdim=True)[1]\n",
    "label = val_label\n",
    "test_acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "\n",
    "print(\"test acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix : \n",
      " [[670 173]\n",
      " [184 659]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.78454333, 0.79206731]),\n",
       " array([0.79478055, 0.78173191]),\n",
       " array([0.78962876, 0.78686567]),\n",
       " array([843, 843]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"confusion matrix : \\n\", confusion_matrix(label.cpu(),  pred.cpu()))\n",
    "precision_recall_fscore_support(label.cpu(), pred.cpu(), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7883053166096199, 0.7882562277580072, 0.7882472141355685, None)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(label.cpu(), pred.cpu(), average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'アップルがイギリスの新興企業を１０億ドルで購入を検討'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_tokenizer = spacy.load(\"ja_core_news_sm\")\n",
    "japanese_vocab_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_japanese_tokenizer(sent):\n",
    "    return [token.text for token in japanese_tokenizer(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['アップル', 'が', 'イギリス', 'の', '新興', '企業', 'を', '１０億', 'ドル', 'で', '購入', 'を', '検討']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_japanese_tokenizer(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-dc9fb4fd79187984.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64303\n",
      "final voc length: 30000\n"
     ]
    }
   ],
   "source": [
    "train_japanese_qa_dataset = QADataSet(new_japanese_tokenizer, getJapaneseDataSet(train_set), language=\"japanese\", vocab_size = japanese_vocab_size)\n",
    "train_features = train_japanese_qa_dataset.get_features()\n",
    "train_label = train_japanese_qa_dataset.get_label()\n",
    "train_features_model_dataset = Data.TensorDataset(train_features, train_label)\n",
    "train_features_model_loader = Data.DataLoader(dataset=train_features_model_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(QADataSet.Japanese_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cee1771b5f371cc3.arrow\n"
     ]
    }
   ],
   "source": [
    "val_japanese_qa_dataset = QADataSet(new_japanese_tokenizer, getJapaneseDataSet(validation_set), language=\"japanese\", vocab_size = japanese_vocab_size)\n",
    "val_features = val_japanese_qa_dataset.get_features()\n",
    "val_label = val_japanese_qa_dataset.get_label()\n",
    "val_features_model_dataset = Data.TensorDataset(val_features, val_label)\n",
    "val_features_model_loader = Data.DataLoader(dataset=val_features_model_dataset,\n",
    "                                            batch_size= batch_size,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 1 loss: 44.9062 acc: 0.515625\n",
      "epoch: 1 batch_num: 2 loss: 44.2825 acc: 0.5\n",
      "epoch: 1 batch_num: 3 loss: 43.7293 acc: 0.59375\n",
      "epoch: 1 batch_num: 4 loss: 43.9112 acc: 0.5\n",
      "epoch: 1 batch_num: 5 loss: 43.9055 acc: 0.640625\n",
      "epoch: 1 batch_num: 6 loss: 43.0701 acc: 0.59375\n",
      "epoch: 1 batch_num: 7 loss: 42.6467 acc: 0.625\n",
      "epoch: 1 batch_num: 8 loss: 42.8872 acc: 0.546875\n",
      "epoch: 1 batch_num: 9 loss: 41.3268 acc: 0.671875\n",
      "epoch: 1 batch_num: 10 loss: 41.6627 acc: 0.625\n",
      "epoch: 1 batch_num: 11 loss: 41.6158 acc: 0.546875\n",
      "epoch: 1 batch_num: 12 loss: 40.2094 acc: 0.5625\n",
      "epoch: 1 batch_num: 13 loss: 40.8105 acc: 0.625\n",
      "epoch: 1 batch_num: 14 loss: 41.2021 acc: 0.5625\n",
      "epoch: 1 batch_num: 15 loss: 39.1377 acc: 0.6875\n",
      "epoch: 1 batch_num: 16 loss: 37.8692 acc: 0.703125\n",
      "epoch: 1 batch_num: 17 loss: 40.9677 acc: 0.71875\n",
      "epoch: 1 batch_num: 18 loss: 41.3198 acc: 0.625\n",
      "epoch: 1 batch_num: 19 loss: 39.8226 acc: 0.6875\n",
      "epoch: 1 batch_num: 20 loss: 40.0426 acc: 0.703125\n",
      "epoch: 1 batch_num: 21 loss: 39.6816 acc: 0.640625\n",
      "epoch: 1 batch_num: 22 loss: 34.6532 acc: 0.78125\n",
      "epoch: 1 batch_num: 23 loss: 40.3233 acc: 0.703125\n",
      "epoch: 1 batch_num: 24 loss: 34.9112 acc: 0.8125\n",
      "epoch: 1 batch_num: 25 loss: 47.7517 acc: 0.6875\n",
      "epoch: 1 batch_num: 26 loss: 36.5808 acc: 0.78125\n",
      "epoch: 1 batch_num: 27 loss: 38.3143 acc: 0.75\n",
      "epoch: 1 batch_num: 28 loss: 36.3565 acc: 0.734375\n",
      "epoch: 1 batch_num: 29 loss: 36.5198 acc: 0.734375\n",
      "epoch: 1 batch_num: 30 loss: 37.1993 acc: 0.703125\n",
      "epoch: 1 batch_num: 31 loss: 41.8723 acc: 0.625\n",
      "epoch: 1 batch_num: 32 loss: 37.6698 acc: 0.75\n",
      "epoch: 1 batch_num: 33 loss: 36.1465 acc: 0.71875\n",
      "epoch: 1 batch_num: 34 loss: 40.3826 acc: 0.703125\n",
      "epoch: 1 batch_num: 35 loss: 40.7807 acc: 0.71875\n",
      "epoch: 1 batch_num: 36 loss: 36.1616 acc: 0.703125\n",
      "epoch: 1 batch_num: 37 loss: 33.7435 acc: 0.78125\n",
      "epoch: 1 batch_num: 38 loss: 32.711 acc: 0.765625\n",
      "epoch: 1 batch_num: 39 loss: 40.2365 acc: 0.71875\n",
      "epoch: 1 batch_num: 40 loss: 36.0379 acc: 0.71875\n",
      "epoch: 1 batch_num: 41 loss: 33.6849 acc: 0.796875\n",
      "epoch: 1 batch_num: 42 loss: 37.1801 acc: 0.71875\n",
      "epoch: 1 batch_num: 43 loss: 35.1756 acc: 0.78125\n",
      "epoch: 1 batch_num: 44 loss: 35.6571 acc: 0.8125\n",
      "epoch: 1 batch_num: 45 loss: 32.8543 acc: 0.828125\n",
      "epoch: 1 batch_num: 46 loss: 38.3839 acc: 0.765625\n",
      "epoch: 1 batch_num: 47 loss: 36.6299 acc: 0.8125\n",
      "epoch: 1 batch_num: 48 loss: 36.5093 acc: 0.75\n",
      "epoch: 1 batch_num: 49 loss: 37.24 acc: 0.734375\n",
      "epoch: 1 batch_num: 50 loss: 45.8535 acc: 0.78125\n",
      "epoch: 1 batch_num: 51 loss: 30.8387 acc: 0.796875\n",
      "epoch: 1 batch_num: 52 loss: 33.9464 acc: 0.765625\n",
      "epoch: 1 batch_num: 53 loss: 39.3697 acc: 0.6875\n",
      "epoch: 1 batch_num: 54 loss: 32.3107 acc: 0.8125\n",
      "epoch: 1 batch_num: 55 loss: 32.131 acc: 0.8125\n",
      "epoch: 1 batch_num: 56 loss: 36.4581 acc: 0.75\n",
      "epoch: 1 batch_num: 57 loss: 29.1897 acc: 0.8125\n",
      "epoch: 1 batch_num: 58 loss: 29.9241 acc: 0.78125\n",
      "epoch: 1 batch_num: 59 loss: 33.6407 acc: 0.78125\n",
      "epoch: 1 batch_num: 60 loss: 31.2659 acc: 0.78125\n",
      "epoch: 1 batch_num: 61 loss: 29.115 acc: 0.84375\n",
      "epoch: 1 batch_num: 62 loss: 30.9907 acc: 0.8125\n",
      "epoch: 1 batch_num: 63 loss: 36.187 acc: 0.75\n",
      "epoch: 1 batch_num: 64 loss: 30.7395 acc: 0.78125\n",
      "epoch: 1 batch_num: 65 loss: 31.2157 acc: 0.84375\n",
      "epoch: 1 batch_num: 66 loss: 34.6336 acc: 0.734375\n",
      "epoch: 1 batch_num: 67 loss: 33.0761 acc: 0.734375\n",
      "epoch: 1 batch_num: 68 loss: 31.61 acc: 0.796875\n",
      "epoch: 1 batch_num: 69 loss: 24.5298 acc: 0.828125\n",
      "epoch: 1 batch_num: 70 loss: 29.5068 acc: 0.8125\n",
      "epoch: 1 batch_num: 71 loss: 28.8639 acc: 0.78125\n",
      "epoch: 1 batch_num: 72 loss: 33.9007 acc: 0.828125\n",
      "epoch: 1 batch_num: 73 loss: 29.3253 acc: 0.828125\n",
      "epoch: 1 batch_num: 74 loss: 33.449 acc: 0.765625\n",
      "epoch: 1 batch_num: 75 loss: 36.512 acc: 0.6875\n",
      "epoch: 1 batch_num: 76 loss: 36.3783 acc: 0.734375\n",
      "epoch: 1 batch_num: 77 loss: 26.9196 acc: 0.828125\n",
      "epoch: 1 batch_num: 78 loss: 36.279 acc: 0.671875\n",
      "epoch: 1 batch_num: 79 loss: 36.2354 acc: 0.703125\n",
      "epoch: 1 batch_num: 80 loss: 38.2184 acc: 0.734375\n",
      "epoch: 1 batch_num: 81 loss: 31.2232 acc: 0.8125\n",
      "epoch: 1 batch_num: 82 loss: 29.686 acc: 0.78125\n",
      "epoch: 1 batch_num: 83 loss: 34.4226 acc: 0.75\n",
      "epoch: 1 batch_num: 84 loss: 33.8697 acc: 0.75\n",
      "epoch: 1 batch_num: 85 loss: 29.7305 acc: 0.765625\n",
      "epoch: 1 batch_num: 86 loss: 26.3566 acc: 0.828125\n",
      "epoch: 1 batch_num: 87 loss: 31.8151 acc: 0.71875\n",
      "epoch: 1 batch_num: 88 loss: 28.547 acc: 0.859375\n",
      "epoch: 1 batch_num: 89 loss: 26.2613 acc: 0.796875\n",
      "epoch: 1 batch_num: 90 loss: 32.1631 acc: 0.8125\n",
      "epoch: 1 batch_num: 91 loss: 39.63 acc: 0.65625\n",
      "epoch: 1 batch_num: 92 loss: 31.7291 acc: 0.828125\n",
      "epoch: 1 batch_num: 93 loss: 34.3825 acc: 0.765625\n",
      "epoch: 1 batch_num: 94 loss: 28.2862 acc: 0.796875\n",
      "epoch: 1 batch_num: 95 loss: 27.3759 acc: 0.859375\n",
      "epoch: 1 batch_num: 96 loss: 29.9074 acc: 0.796875\n",
      "epoch: 1 batch_num: 97 loss: 33.8847 acc: 0.796875\n",
      "epoch: 1 batch_num: 98 loss: 31.7702 acc: 0.765625\n",
      "epoch: 1 batch_num: 99 loss: 29.8744 acc: 0.84375\n",
      "epoch: 1 batch_num: 100 loss: 31.2872 acc: 0.75\n",
      "epoch: 1 batch_num: 101 loss: 42.9918 acc: 0.75\n",
      "epoch: 1 batch_num: 102 loss: 38.8114 acc: 0.6875\n",
      "epoch: 1 batch_num: 103 loss: 36.7164 acc: 0.703125\n",
      "epoch: 1 batch_num: 104 loss: 35.5008 acc: 0.671875\n",
      "epoch: 1 batch_num: 105 loss: 35.4139 acc: 0.765625\n",
      "epoch: 1 batch_num: 106 loss: 39.5119 acc: 0.796875\n",
      "epoch: 1 batch_num: 107 loss: 40.0754 acc: 0.6875\n",
      "epoch: 1 batch_num: 108 loss: 32.3237 acc: 0.796875\n",
      "epoch: 1 batch_num: 109 loss: 28.793 acc: 0.78125\n",
      "epoch: 1 batch_num: 110 loss: 32.0122 acc: 0.78125\n",
      "epoch: 1 batch_num: 111 loss: 31.2908 acc: 0.71875\n",
      "epoch: 1 batch_num: 112 loss: 27.28 acc: 0.84375\n",
      "epoch: 1 batch_num: 113 loss: 29.8091 acc: 0.765625\n",
      "epoch: 1 batch_num: 114 loss: 29.0888 acc: 0.8125\n",
      "epoch: 1 batch_num: 115 loss: 27.2853 acc: 0.828125\n",
      "epoch: 1 batch_num: 116 loss: 31.5811 acc: 0.734375\n",
      "epoch: 1 batch_num: 117 loss: 35.05 acc: 0.734375\n",
      "epoch: 1 batch_num: 118 loss: 30.7482 acc: 0.828125\n",
      "epoch: 1 batch_num: 119 loss: 30.0724 acc: 0.8125\n",
      "epoch: 1 batch_num: 120 loss: 34.4625 acc: 0.734375\n",
      "epoch: 1 batch_num: 121 loss: 28.3475 acc: 0.84375\n",
      "epoch: 1 batch_num: 122 loss: 33.5733 acc: 0.734375\n",
      "epoch: 1 batch_num: 123 loss: 35.8996 acc: 0.71875\n",
      "epoch: 1 batch_num: 124 loss: 27.6623 acc: 0.8125\n",
      "epoch: 1 batch_num: 125 loss: 29.3239 acc: 0.78125\n",
      "epoch: 1 batch_num: 126 loss: 36.9795 acc: 0.71875\n",
      "epoch: 1 batch_num: 127 loss: 32.7013 acc: 0.796875\n",
      "epoch: 1 batch_num: 128 loss: 28.9764 acc: 0.796875\n",
      "epoch: 1 batch_num: 129 loss: 32.0967 acc: 0.765625\n",
      "epoch: 1 batch_num: 130 loss: 29.3882 acc: 0.84375\n",
      "epoch: 1 batch_num: 131 loss: 28.1874 acc: 0.78125\n",
      "epoch: 1 batch_num: 132 loss: 36.5957 acc: 0.734375\n",
      "epoch: 1 batch_num: 133 loss: 30.4162 acc: 0.78125\n",
      "epoch: 1 batch_num: 134 loss: 33.107 acc: 0.796875\n",
      "epoch: 1 batch_num: 135 loss: 34.6711 acc: 0.796875\n",
      "epoch: 1 batch_num: 136 loss: 30.1032 acc: 0.75\n",
      "epoch: 1 batch_num: 137 loss: 25.7841 acc: 0.859375\n",
      "epoch: 1 batch_num: 138 loss: 25.355 acc: 0.796875\n",
      "epoch: 1 batch_num: 139 loss: 26.1685 acc: 0.859375\n",
      "epoch: 1 batch_num: 140 loss: 33.1038 acc: 0.78125\n",
      "epoch: 1 batch_num: 141 loss: 32.4369 acc: 0.75\n",
      "epoch: 1 batch_num: 142 loss: 46.5071 acc: 0.71875\n",
      "epoch: 1 batch_num: 143 loss: 30.3643 acc: 0.734375\n",
      "epoch: 1 batch_num: 144 loss: 41.5974 acc: 0.703125\n",
      "epoch: 1 batch_num: 145 loss: 25.7636 acc: 0.8125\n",
      "epoch: 1 batch_num: 146 loss: 27.6678 acc: 0.796875\n",
      "epoch: 1 batch_num: 147 loss: 28.7043 acc: 0.78125\n",
      "epoch: 1 batch_num: 148 loss: 39.7562 acc: 0.75\n",
      "epoch: 1 batch_num: 149 loss: 27.4759 acc: 0.765625\n",
      "epoch: 1 batch_num: 150 loss: 29.8707 acc: 0.75\n",
      "epoch: 1 batch_num: 151 loss: 31.4953 acc: 0.75\n",
      "epoch: 1 batch_num: 152 loss: 27.8738 acc: 0.8125\n",
      "epoch: 1 batch_num: 153 loss: 21.3272 acc: 0.90625\n",
      "epoch: 1 batch_num: 154 loss: 25.7782 acc: 0.859375\n",
      "epoch: 1 batch_num: 155 loss: 27.3586 acc: 0.8125\n",
      "epoch: 1 batch_num: 156 loss: 30.3199 acc: 0.75\n",
      "epoch: 1 batch_num: 157 loss: 33.3764 acc: 0.671875\n",
      "epoch: 1 batch_num: 158 loss: 27.7214 acc: 0.78125\n",
      "epoch: 1 batch_num: 159 loss: 21.8757 acc: 0.859375\n",
      "epoch: 1 batch_num: 160 loss: 32.2461 acc: 0.84375\n",
      "epoch: 1 batch_num: 161 loss: 34.1302 acc: 0.765625\n",
      "epoch: 1 batch_num: 162 loss: 32.1035 acc: 0.828125\n",
      "epoch: 1 batch_num: 163 loss: 22.4393 acc: 0.859375\n",
      "epoch: 1 batch_num: 164 loss: 28.7899 acc: 0.765625\n",
      "epoch: 1 batch_num: 165 loss: 35.0568 acc: 0.71875\n",
      "epoch: 1 batch_num: 166 loss: 20.7386 acc: 0.875\n",
      "epoch: 1 batch_num: 167 loss: 39.1185 acc: 0.6875\n",
      "epoch: 1 batch_num: 168 loss: 27.1469 acc: 0.828125\n",
      "epoch: 1 batch_num: 169 loss: 22.0954 acc: 0.875\n",
      "epoch: 1 batch_num: 170 loss: 21.5154 acc: 0.859375\n",
      "epoch: 1 batch_num: 171 loss: 29.2471 acc: 0.84375\n",
      "epoch: 1 batch_num: 172 loss: 29.9213 acc: 0.828125\n",
      "epoch: 1 batch_num: 173 loss: 25.6169 acc: 0.859375\n",
      "epoch: 1 batch_num: 174 loss: 33.8798 acc: 0.75\n",
      "epoch: 1 batch_num: 175 loss: 20.2748 acc: 0.890625\n",
      "epoch: 1 batch_num: 176 loss: 25.038 acc: 0.796875\n",
      "epoch: 1 batch_num: 177 loss: 33.1812 acc: 0.734375\n",
      "epoch: 1 batch_num: 178 loss: 32.1404 acc: 0.78125\n",
      "epoch: 1 batch_num: 179 loss: 31.1519 acc: 0.796875\n",
      "epoch: 1 batch_num: 180 loss: 27.5214 acc: 0.859375\n",
      "epoch: 1 batch_num: 181 loss: 21.6203 acc: 0.890625\n",
      "epoch: 1 batch_num: 182 loss: 29.2669 acc: 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 183 loss: 25.4994 acc: 0.828125\n",
      "epoch: 1 batch_num: 184 loss: 29.5384 acc: 0.78125\n",
      "epoch: 1 batch_num: 185 loss: 29.889 acc: 0.71875\n",
      "epoch: 1 batch_num: 186 loss: 30.5355 acc: 0.84375\n",
      "epoch: 1 batch_num: 187 loss: 31.37 acc: 0.75\n",
      "epoch: 1 batch_num: 188 loss: 26.9696 acc: 0.875\n",
      "epoch: 1 batch_num: 189 loss: 30.2847 acc: 0.75\n",
      "epoch: 1 batch_num: 190 loss: 29.0013 acc: 0.8125\n",
      "epoch: 1 batch_num: 191 loss: 26.6842 acc: 0.8125\n",
      "epoch: 1 batch_num: 192 loss: 28.92 acc: 0.8125\n",
      "epoch: 1 batch_num: 193 loss: 27.7705 acc: 0.734375\n",
      "epoch: 1 batch_num: 194 loss: 33.6121 acc: 0.78125\n",
      "epoch: 1 batch_num: 195 loss: 30.6447 acc: 0.78125\n",
      "epoch: 1 batch_num: 196 loss: 27.7857 acc: 0.78125\n",
      "epoch: 1 batch_num: 197 loss: 22.0863 acc: 0.875\n",
      "epoch: 1 batch_num: 198 loss: 31.6455 acc: 0.78125\n",
      "epoch: 1 batch_num: 199 loss: 23.6372 acc: 0.90625\n",
      "epoch: 1 batch_num: 200 loss: 26.1611 acc: 0.796875\n",
      "epoch: 1 batch_num: 201 loss: 34.7989 acc: 0.796875\n",
      "epoch: 1 batch_num: 202 loss: 28.4703 acc: 0.75\n",
      "epoch: 1 batch_num: 203 loss: 37.0478 acc: 0.65625\n",
      "epoch: 1 batch_num: 204 loss: 31.1959 acc: 0.78125\n",
      "epoch: 1 batch_num: 205 loss: 18.1397 acc: 0.9375\n",
      "epoch: 1 batch_num: 206 loss: 28.0056 acc: 0.8125\n",
      "epoch: 1 batch_num: 207 loss: 23.4676 acc: 0.890625\n",
      "epoch: 1 batch_num: 208 loss: 42.5549 acc: 0.640625\n",
      "epoch: 1 batch_num: 209 loss: 26.742 acc: 0.84375\n",
      "epoch: 1 batch_num: 210 loss: 32.1905 acc: 0.765625\n",
      "epoch: 1 batch_num: 211 loss: 22.1825 acc: 0.875\n",
      "epoch: 1 batch_num: 212 loss: 28.6521 acc: 0.8125\n",
      "epoch: 1 batch_num: 213 loss: 27.7863 acc: 0.78125\n",
      "epoch: 1 batch_num: 214 loss: 25.8114 acc: 0.84375\n",
      "epoch: 1 batch_num: 215 loss: 0.8008 acc: 1.0\n",
      "epoch: 2 batch_num: 1 loss: 20.2871 acc: 0.890625\n",
      "epoch: 2 batch_num: 2 loss: 24.7104 acc: 0.828125\n",
      "epoch: 2 batch_num: 3 loss: 23.4274 acc: 0.859375\n",
      "epoch: 2 batch_num: 4 loss: 20.0775 acc: 0.921875\n",
      "epoch: 2 batch_num: 5 loss: 19.6189 acc: 0.890625\n",
      "epoch: 2 batch_num: 6 loss: 22.2261 acc: 0.890625\n",
      "epoch: 2 batch_num: 7 loss: 21.6625 acc: 0.875\n",
      "epoch: 2 batch_num: 8 loss: 23.4758 acc: 0.828125\n",
      "epoch: 2 batch_num: 9 loss: 20.3307 acc: 0.84375\n",
      "epoch: 2 batch_num: 10 loss: 15.5278 acc: 0.921875\n",
      "epoch: 2 batch_num: 11 loss: 19.9635 acc: 0.890625\n",
      "epoch: 2 batch_num: 12 loss: 14.0531 acc: 0.953125\n",
      "epoch: 2 batch_num: 13 loss: 17.4478 acc: 0.921875\n",
      "epoch: 2 batch_num: 14 loss: 14.7526 acc: 0.921875\n",
      "epoch: 2 batch_num: 15 loss: 19.4341 acc: 0.84375\n",
      "epoch: 2 batch_num: 16 loss: 20.362 acc: 0.859375\n",
      "epoch: 2 batch_num: 17 loss: 29.0938 acc: 0.8125\n",
      "epoch: 2 batch_num: 18 loss: 17.9949 acc: 0.9375\n",
      "epoch: 2 batch_num: 19 loss: 26.2035 acc: 0.796875\n",
      "epoch: 2 batch_num: 20 loss: 21.9856 acc: 0.859375\n",
      "epoch: 2 batch_num: 21 loss: 18.1423 acc: 0.859375\n",
      "epoch: 2 batch_num: 22 loss: 22.9088 acc: 0.859375\n",
      "epoch: 2 batch_num: 23 loss: 25.7465 acc: 0.84375\n",
      "epoch: 2 batch_num: 24 loss: 15.9443 acc: 0.921875\n",
      "epoch: 2 batch_num: 25 loss: 26.6765 acc: 0.828125\n",
      "epoch: 2 batch_num: 26 loss: 21.0178 acc: 0.875\n",
      "epoch: 2 batch_num: 27 loss: 17.9742 acc: 0.875\n",
      "epoch: 2 batch_num: 28 loss: 28.6958 acc: 0.796875\n",
      "epoch: 2 batch_num: 29 loss: 14.8991 acc: 0.890625\n",
      "epoch: 2 batch_num: 30 loss: 35.4188 acc: 0.796875\n",
      "epoch: 2 batch_num: 31 loss: 25.3392 acc: 0.796875\n",
      "epoch: 2 batch_num: 32 loss: 20.6301 acc: 0.828125\n",
      "epoch: 2 batch_num: 33 loss: 31.7586 acc: 0.796875\n",
      "epoch: 2 batch_num: 34 loss: 16.6217 acc: 0.875\n",
      "epoch: 2 batch_num: 35 loss: 22.2443 acc: 0.84375\n",
      "epoch: 2 batch_num: 36 loss: 18.3713 acc: 0.875\n",
      "epoch: 2 batch_num: 37 loss: 19.8602 acc: 0.84375\n",
      "epoch: 2 batch_num: 38 loss: 19.0518 acc: 0.890625\n",
      "epoch: 2 batch_num: 39 loss: 17.1115 acc: 0.875\n",
      "epoch: 2 batch_num: 40 loss: 21.6347 acc: 0.875\n",
      "epoch: 2 batch_num: 41 loss: 27.8915 acc: 0.78125\n",
      "epoch: 2 batch_num: 42 loss: 21.0268 acc: 0.84375\n",
      "epoch: 2 batch_num: 43 loss: 19.8441 acc: 0.84375\n",
      "epoch: 2 batch_num: 44 loss: 20.6214 acc: 0.890625\n",
      "epoch: 2 batch_num: 45 loss: 21.2926 acc: 0.875\n",
      "epoch: 2 batch_num: 46 loss: 16.8568 acc: 0.9375\n",
      "epoch: 2 batch_num: 47 loss: 21.3075 acc: 0.890625\n",
      "epoch: 2 batch_num: 48 loss: 18.367 acc: 0.875\n",
      "epoch: 2 batch_num: 49 loss: 20.3389 acc: 0.859375\n",
      "epoch: 2 batch_num: 50 loss: 23.5796 acc: 0.84375\n",
      "epoch: 2 batch_num: 51 loss: 22.5882 acc: 0.875\n",
      "epoch: 2 batch_num: 52 loss: 16.7976 acc: 0.890625\n",
      "epoch: 2 batch_num: 53 loss: 19.1607 acc: 0.890625\n",
      "epoch: 2 batch_num: 54 loss: 21.3066 acc: 0.828125\n",
      "epoch: 2 batch_num: 55 loss: 14.3347 acc: 0.921875\n",
      "epoch: 2 batch_num: 56 loss: 24.4171 acc: 0.828125\n",
      "epoch: 2 batch_num: 57 loss: 23.1525 acc: 0.875\n",
      "epoch: 2 batch_num: 58 loss: 24.9193 acc: 0.796875\n",
      "epoch: 2 batch_num: 59 loss: 19.1823 acc: 0.875\n",
      "epoch: 2 batch_num: 60 loss: 24.3795 acc: 0.859375\n",
      "epoch: 2 batch_num: 61 loss: 23.9441 acc: 0.8125\n",
      "epoch: 2 batch_num: 62 loss: 44.2886 acc: 0.90625\n",
      "epoch: 2 batch_num: 63 loss: 22.934 acc: 0.875\n",
      "epoch: 2 batch_num: 64 loss: 15.3139 acc: 0.90625\n",
      "epoch: 2 batch_num: 65 loss: 19.9589 acc: 0.875\n",
      "epoch: 2 batch_num: 66 loss: 19.6248 acc: 0.84375\n",
      "epoch: 2 batch_num: 67 loss: 21.7363 acc: 0.828125\n",
      "epoch: 2 batch_num: 68 loss: 20.4346 acc: 0.859375\n",
      "epoch: 2 batch_num: 69 loss: 25.5748 acc: 0.84375\n",
      "epoch: 2 batch_num: 70 loss: 25.3729 acc: 0.828125\n",
      "epoch: 2 batch_num: 71 loss: 26.2994 acc: 0.84375\n",
      "epoch: 2 batch_num: 72 loss: 18.3966 acc: 0.921875\n",
      "epoch: 2 batch_num: 73 loss: 20.6701 acc: 0.859375\n",
      "epoch: 2 batch_num: 74 loss: 19.0817 acc: 0.890625\n",
      "epoch: 2 batch_num: 75 loss: 20.0885 acc: 0.859375\n",
      "epoch: 2 batch_num: 76 loss: 20.6651 acc: 0.875\n",
      "epoch: 2 batch_num: 77 loss: 17.6168 acc: 0.875\n",
      "epoch: 2 batch_num: 78 loss: 16.8688 acc: 0.890625\n",
      "epoch: 2 batch_num: 79 loss: 21.3874 acc: 0.890625\n",
      "epoch: 2 batch_num: 80 loss: 20.0093 acc: 0.859375\n",
      "epoch: 2 batch_num: 81 loss: 22.4036 acc: 0.828125\n",
      "epoch: 2 batch_num: 82 loss: 19.019 acc: 0.890625\n",
      "epoch: 2 batch_num: 83 loss: 16.4666 acc: 0.921875\n",
      "epoch: 2 batch_num: 84 loss: 22.9255 acc: 0.84375\n",
      "epoch: 2 batch_num: 85 loss: 21.357 acc: 0.875\n",
      "epoch: 2 batch_num: 86 loss: 18.5238 acc: 0.890625\n",
      "epoch: 2 batch_num: 87 loss: 36.0522 acc: 0.84375\n",
      "epoch: 2 batch_num: 88 loss: 23.7317 acc: 0.84375\n",
      "epoch: 2 batch_num: 89 loss: 23.628 acc: 0.890625\n",
      "epoch: 2 batch_num: 90 loss: 16.2809 acc: 0.90625\n",
      "epoch: 2 batch_num: 91 loss: 20.0531 acc: 0.90625\n",
      "epoch: 2 batch_num: 92 loss: 20.2411 acc: 0.84375\n",
      "epoch: 2 batch_num: 93 loss: 20.0795 acc: 0.84375\n",
      "epoch: 2 batch_num: 94 loss: 23.8694 acc: 0.78125\n",
      "epoch: 2 batch_num: 95 loss: 15.3048 acc: 0.90625\n",
      "epoch: 2 batch_num: 96 loss: 23.5682 acc: 0.875\n",
      "epoch: 2 batch_num: 97 loss: 20.6272 acc: 0.796875\n",
      "epoch: 2 batch_num: 98 loss: 27.3683 acc: 0.828125\n",
      "epoch: 2 batch_num: 99 loss: 23.2377 acc: 0.828125\n",
      "epoch: 2 batch_num: 100 loss: 13.149 acc: 0.984375\n",
      "epoch: 2 batch_num: 101 loss: 27.8185 acc: 0.875\n",
      "epoch: 2 batch_num: 102 loss: 15.5841 acc: 0.921875\n",
      "epoch: 2 batch_num: 103 loss: 27.7497 acc: 0.859375\n",
      "epoch: 2 batch_num: 104 loss: 20.5562 acc: 0.859375\n",
      "epoch: 2 batch_num: 105 loss: 23.3526 acc: 0.890625\n",
      "epoch: 2 batch_num: 106 loss: 21.6133 acc: 0.921875\n",
      "epoch: 2 batch_num: 107 loss: 16.2074 acc: 0.9375\n",
      "epoch: 2 batch_num: 108 loss: 18.5212 acc: 0.875\n",
      "epoch: 2 batch_num: 109 loss: 25.5849 acc: 0.84375\n",
      "epoch: 2 batch_num: 110 loss: 23.7663 acc: 0.84375\n",
      "epoch: 2 batch_num: 111 loss: 21.0827 acc: 0.875\n",
      "epoch: 2 batch_num: 112 loss: 21.339 acc: 0.890625\n",
      "epoch: 2 batch_num: 113 loss: 21.7506 acc: 0.84375\n",
      "epoch: 2 batch_num: 114 loss: 19.6571 acc: 0.875\n",
      "epoch: 2 batch_num: 115 loss: 25.374 acc: 0.8125\n",
      "epoch: 2 batch_num: 116 loss: 25.0333 acc: 0.828125\n",
      "epoch: 2 batch_num: 117 loss: 17.6105 acc: 0.890625\n",
      "epoch: 2 batch_num: 118 loss: 26.4076 acc: 0.796875\n",
      "epoch: 2 batch_num: 119 loss: 23.9703 acc: 0.828125\n",
      "epoch: 2 batch_num: 120 loss: 26.9085 acc: 0.8125\n",
      "epoch: 2 batch_num: 121 loss: 15.6622 acc: 0.890625\n",
      "epoch: 2 batch_num: 122 loss: 17.6415 acc: 0.859375\n",
      "epoch: 2 batch_num: 123 loss: 22.9287 acc: 0.84375\n",
      "epoch: 2 batch_num: 124 loss: 26.8746 acc: 0.859375\n",
      "epoch: 2 batch_num: 125 loss: 25.1506 acc: 0.765625\n",
      "epoch: 2 batch_num: 126 loss: 21.6332 acc: 0.84375\n",
      "epoch: 2 batch_num: 127 loss: 18.3585 acc: 0.890625\n",
      "epoch: 2 batch_num: 128 loss: 32.8671 acc: 0.796875\n",
      "epoch: 2 batch_num: 129 loss: 19.3105 acc: 0.953125\n",
      "epoch: 2 batch_num: 130 loss: 17.6723 acc: 0.90625\n",
      "epoch: 2 batch_num: 131 loss: 24.7669 acc: 0.8125\n",
      "epoch: 2 batch_num: 132 loss: 26.4059 acc: 0.859375\n",
      "epoch: 2 batch_num: 133 loss: 24.3027 acc: 0.796875\n",
      "epoch: 2 batch_num: 134 loss: 25.4092 acc: 0.8125\n",
      "epoch: 2 batch_num: 135 loss: 23.59 acc: 0.84375\n",
      "epoch: 2 batch_num: 136 loss: 21.7761 acc: 0.84375\n",
      "epoch: 2 batch_num: 137 loss: 21.5126 acc: 0.84375\n",
      "epoch: 2 batch_num: 138 loss: 23.2048 acc: 0.859375\n",
      "epoch: 2 batch_num: 139 loss: 24.3696 acc: 0.84375\n",
      "epoch: 2 batch_num: 140 loss: 21.9561 acc: 0.890625\n",
      "epoch: 2 batch_num: 141 loss: 14.3752 acc: 0.96875\n",
      "epoch: 2 batch_num: 142 loss: 18.2206 acc: 0.875\n",
      "epoch: 2 batch_num: 143 loss: 29.2929 acc: 0.71875\n",
      "epoch: 2 batch_num: 144 loss: 19.2701 acc: 0.890625\n",
      "epoch: 2 batch_num: 145 loss: 29.7159 acc: 0.828125\n",
      "epoch: 2 batch_num: 146 loss: 16.5112 acc: 0.921875\n",
      "epoch: 2 batch_num: 147 loss: 15.8341 acc: 0.921875\n",
      "epoch: 2 batch_num: 148 loss: 25.9484 acc: 0.78125\n",
      "epoch: 2 batch_num: 149 loss: 14.7474 acc: 0.9375\n",
      "epoch: 2 batch_num: 150 loss: 20.2375 acc: 0.890625\n",
      "epoch: 2 batch_num: 151 loss: 17.5085 acc: 0.875\n",
      "epoch: 2 batch_num: 152 loss: 19.4672 acc: 0.875\n",
      "epoch: 2 batch_num: 153 loss: 20.2019 acc: 0.859375\n",
      "epoch: 2 batch_num: 154 loss: 18.0839 acc: 0.890625\n",
      "epoch: 2 batch_num: 155 loss: 22.3424 acc: 0.859375\n",
      "epoch: 2 batch_num: 156 loss: 19.7372 acc: 0.84375\n",
      "epoch: 2 batch_num: 157 loss: 19.7463 acc: 0.84375\n",
      "epoch: 2 batch_num: 158 loss: 35.3491 acc: 0.71875\n",
      "epoch: 2 batch_num: 159 loss: 21.7045 acc: 0.84375\n",
      "epoch: 2 batch_num: 160 loss: 16.6467 acc: 0.90625\n",
      "epoch: 2 batch_num: 161 loss: 20.1243 acc: 0.84375\n",
      "epoch: 2 batch_num: 162 loss: 29.21 acc: 0.78125\n",
      "epoch: 2 batch_num: 163 loss: 20.9993 acc: 0.859375\n",
      "epoch: 2 batch_num: 164 loss: 27.0025 acc: 0.796875\n",
      "epoch: 2 batch_num: 165 loss: 24.5404 acc: 0.8125\n",
      "epoch: 2 batch_num: 166 loss: 19.0189 acc: 0.84375\n",
      "epoch: 2 batch_num: 167 loss: 23.0038 acc: 0.828125\n",
      "epoch: 2 batch_num: 168 loss: 16.8135 acc: 0.90625\n",
      "epoch: 2 batch_num: 169 loss: 20.6244 acc: 0.84375\n",
      "epoch: 2 batch_num: 170 loss: 16.7165 acc: 0.890625\n",
      "epoch: 2 batch_num: 171 loss: 22.9311 acc: 0.875\n",
      "epoch: 2 batch_num: 172 loss: 19.9875 acc: 0.859375\n",
      "epoch: 2 batch_num: 173 loss: 14.7553 acc: 0.921875\n",
      "epoch: 2 batch_num: 174 loss: 18.1153 acc: 0.953125\n",
      "epoch: 2 batch_num: 175 loss: 20.9453 acc: 0.875\n",
      "epoch: 2 batch_num: 176 loss: 17.5957 acc: 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch_num: 177 loss: 20.6794 acc: 0.890625\n",
      "epoch: 2 batch_num: 178 loss: 23.4471 acc: 0.828125\n",
      "epoch: 2 batch_num: 179 loss: 25.192 acc: 0.84375\n",
      "epoch: 2 batch_num: 180 loss: 24.4108 acc: 0.859375\n",
      "epoch: 2 batch_num: 181 loss: 19.4404 acc: 0.90625\n",
      "epoch: 2 batch_num: 182 loss: 21.7478 acc: 0.875\n",
      "epoch: 2 batch_num: 183 loss: 26.465 acc: 0.828125\n",
      "epoch: 2 batch_num: 184 loss: 28.7263 acc: 0.859375\n",
      "epoch: 2 batch_num: 185 loss: 17.7278 acc: 0.90625\n",
      "epoch: 2 batch_num: 186 loss: 20.2447 acc: 0.875\n",
      "epoch: 2 batch_num: 187 loss: 17.7489 acc: 0.84375\n",
      "epoch: 2 batch_num: 188 loss: 20.5503 acc: 0.859375\n",
      "epoch: 2 batch_num: 189 loss: 19.0373 acc: 0.90625\n",
      "epoch: 2 batch_num: 190 loss: 23.497 acc: 0.8125\n",
      "epoch: 2 batch_num: 191 loss: 21.2469 acc: 0.859375\n",
      "epoch: 2 batch_num: 192 loss: 20.5419 acc: 0.890625\n",
      "epoch: 2 batch_num: 193 loss: 18.5771 acc: 0.890625\n",
      "epoch: 2 batch_num: 194 loss: 18.7909 acc: 0.890625\n",
      "epoch: 2 batch_num: 195 loss: 29.8004 acc: 0.796875\n",
      "epoch: 2 batch_num: 196 loss: 16.9901 acc: 0.875\n",
      "epoch: 2 batch_num: 197 loss: 21.2975 acc: 0.859375\n",
      "epoch: 2 batch_num: 198 loss: 21.9286 acc: 0.859375\n",
      "epoch: 2 batch_num: 199 loss: 18.9855 acc: 0.859375\n",
      "epoch: 2 batch_num: 200 loss: 30.5923 acc: 0.796875\n",
      "epoch: 2 batch_num: 201 loss: 29.2848 acc: 0.84375\n",
      "epoch: 2 batch_num: 202 loss: 13.3608 acc: 0.9375\n",
      "epoch: 2 batch_num: 203 loss: 20.2132 acc: 0.875\n",
      "epoch: 2 batch_num: 204 loss: 22.3519 acc: 0.765625\n",
      "epoch: 2 batch_num: 205 loss: 29.318 acc: 0.8125\n",
      "epoch: 2 batch_num: 206 loss: 21.2546 acc: 0.875\n",
      "epoch: 2 batch_num: 207 loss: 21.0516 acc: 0.8125\n",
      "epoch: 2 batch_num: 208 loss: 23.2294 acc: 0.84375\n",
      "epoch: 2 batch_num: 209 loss: 17.9905 acc: 0.859375\n",
      "epoch: 2 batch_num: 210 loss: 21.3564 acc: 0.84375\n",
      "epoch: 2 batch_num: 211 loss: 24.4783 acc: 0.875\n",
      "epoch: 2 batch_num: 212 loss: 18.5348 acc: 0.84375\n",
      "epoch: 2 batch_num: 213 loss: 17.7937 acc: 0.875\n",
      "epoch: 2 batch_num: 214 loss: 24.8599 acc: 0.8125\n",
      "epoch: 2 batch_num: 215 loss: 2.0007 acc: 0.8\n",
      "max_acc: 1.0\n",
      "test acc: 0.7793594306049823\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\",weight=torch.FloatTensor([1, 1]).to(device))  # loss function\n",
    "japanese_model = AnswerableClassifier(vocab_size=japanese_vocab_size * 2 + 7, num_labels=2, num_hidden=100).to('cuda')\n",
    "optimizer = torch.optim.Adam(japanese_model.parameters(), lr = 0.001, amsgrad=True)\n",
    "\n",
    "max_acc = train_features_model(model = japanese_model, train_loader=train_features_model_loader,\n",
    "                               criterion= criterion, optimizer=optimizer, model_file_name=\"japanese_model.pth\",\n",
    "                               epochs = 2)\n",
    "print(\"max_acc:\", max_acc)\n",
    "japanese_model.load_state_dict(torch.load(\"japanese_model.pth\"))\n",
    "japanese_model.eval()\n",
    "predict_label = japanese_model(val_features)\n",
    "pred = predict_label.max(-1, keepdim=True)[1]\n",
    "label = val_label\n",
    "test_acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "\n",
    "print(\"test acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix : \n",
      " [[727 116]\n",
      " [256 587]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.73957274, 0.83499289]),\n",
       " array([0.8623962 , 0.69632266]),\n",
       " array([0.79627601, 0.75937904]),\n",
       " array([843, 843]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"confusion matrix : \\n\", confusion_matrix(label.cpu(),  pred.cpu()))\n",
    "precision_recall_fscore_support(label.cpu(), pred.cpu(), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7872828120726605, 0.7793594306049823, 0.7778275279171492, None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(label.cpu(), pred.cpu(), average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finnish Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_tokenizer = spacy.load(\"fi_core_news_sm\")\n",
    "finnish_vocab_size = 30000\n",
    "def new_finnish_tokenizer(sent):\n",
    "    return [token.text for token in finnish_tokenizer(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-23b0a46089b23e95.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160505\n",
      "final voc length: 30000\n"
     ]
    }
   ],
   "source": [
    "train_finnish_qa_dataset = QADataSet(new_finnish_tokenizer, getFinnishDataSet(train_set), language=\"finnish\",vocab_size = finnish_vocab_size)\n",
    "train_features = train_finnish_qa_dataset.get_features()\n",
    "train_label = train_finnish_qa_dataset.get_label()\n",
    "train_features_model_dataset = Data.TensorDataset(train_features, train_label)\n",
    "train_features_model_loader = Data.DataLoader(dataset=train_features_model_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-9ffd3d37cf2899c6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-235768edd2fc5f73.arrow\n"
     ]
    }
   ],
   "source": [
    "val_finnish_qa_dataset = QADataSet(new_finnish_tokenizer, getFinnishDataSet(validation_set) , language=\"finnish\",vocab_size = finnish_vocab_size)\n",
    "val_features = val_finnish_qa_dataset.get_features()\n",
    "val_label = val_finnish_qa_dataset.get_label()\n",
    "val_features_model_dataset = Data.TensorDataset(val_features, val_label)\n",
    "val_features_model_loader = Data.DataLoader(dataset=val_features_model_dataset,\n",
    "                                            batch_size= batch_size,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 1 loss: 44.8371 acc: 0.546875\n",
      "epoch: 1 batch_num: 2 loss: 44.7184 acc: 0.4375\n",
      "epoch: 1 batch_num: 3 loss: 43.3586 acc: 0.734375\n",
      "epoch: 1 batch_num: 4 loss: 45.0104 acc: 0.65625\n",
      "epoch: 1 batch_num: 5 loss: 44.2091 acc: 0.578125\n",
      "epoch: 1 batch_num: 6 loss: 43.5008 acc: 0.515625\n",
      "epoch: 1 batch_num: 7 loss: 43.703 acc: 0.515625\n",
      "epoch: 1 batch_num: 8 loss: 44.0892 acc: 0.625\n",
      "epoch: 1 batch_num: 9 loss: 43.4298 acc: 0.578125\n",
      "epoch: 1 batch_num: 10 loss: 44.3167 acc: 0.609375\n",
      "epoch: 1 batch_num: 11 loss: 42.723 acc: 0.671875\n",
      "epoch: 1 batch_num: 12 loss: 43.0372 acc: 0.625\n",
      "epoch: 1 batch_num: 13 loss: 43.0186 acc: 0.640625\n",
      "epoch: 1 batch_num: 14 loss: 42.361 acc: 0.71875\n",
      "epoch: 1 batch_num: 15 loss: 43.4806 acc: 0.671875\n",
      "epoch: 1 batch_num: 16 loss: 42.2327 acc: 0.734375\n",
      "epoch: 1 batch_num: 17 loss: 42.7647 acc: 0.6875\n",
      "epoch: 1 batch_num: 18 loss: 41.3204 acc: 0.703125\n",
      "epoch: 1 batch_num: 19 loss: 42.0268 acc: 0.640625\n",
      "epoch: 1 batch_num: 20 loss: 41.8923 acc: 0.734375\n",
      "epoch: 1 batch_num: 21 loss: 42.4035 acc: 0.65625\n",
      "epoch: 1 batch_num: 22 loss: 41.9293 acc: 0.71875\n",
      "epoch: 1 batch_num: 23 loss: 42.1348 acc: 0.703125\n",
      "epoch: 1 batch_num: 24 loss: 43.3036 acc: 0.625\n",
      "epoch: 1 batch_num: 25 loss: 40.7045 acc: 0.71875\n",
      "epoch: 1 batch_num: 26 loss: 41.6257 acc: 0.71875\n",
      "epoch: 1 batch_num: 27 loss: 40.7877 acc: 0.78125\n",
      "epoch: 1 batch_num: 28 loss: 41.5033 acc: 0.734375\n",
      "epoch: 1 batch_num: 29 loss: 41.4864 acc: 0.625\n",
      "epoch: 1 batch_num: 30 loss: 40.4332 acc: 0.75\n",
      "epoch: 1 batch_num: 31 loss: 41.4863 acc: 0.75\n",
      "epoch: 1 batch_num: 32 loss: 42.1705 acc: 0.6875\n",
      "epoch: 1 batch_num: 33 loss: 40.4741 acc: 0.703125\n",
      "epoch: 1 batch_num: 34 loss: 38.5583 acc: 0.765625\n",
      "epoch: 1 batch_num: 35 loss: 41.1719 acc: 0.6875\n",
      "epoch: 1 batch_num: 36 loss: 41.6293 acc: 0.65625\n",
      "epoch: 1 batch_num: 37 loss: 37.7048 acc: 0.765625\n",
      "epoch: 1 batch_num: 38 loss: 39.9562 acc: 0.703125\n",
      "epoch: 1 batch_num: 39 loss: 41.3666 acc: 0.671875\n",
      "epoch: 1 batch_num: 40 loss: 38.7088 acc: 0.859375\n",
      "epoch: 1 batch_num: 41 loss: 40.4172 acc: 0.734375\n",
      "epoch: 1 batch_num: 42 loss: 39.5151 acc: 0.640625\n",
      "epoch: 1 batch_num: 43 loss: 39.5189 acc: 0.703125\n",
      "epoch: 1 batch_num: 44 loss: 38.9202 acc: 0.765625\n",
      "epoch: 1 batch_num: 45 loss: 38.7824 acc: 0.765625\n",
      "epoch: 1 batch_num: 46 loss: 39.8205 acc: 0.703125\n",
      "epoch: 1 batch_num: 47 loss: 38.2619 acc: 0.765625\n",
      "epoch: 1 batch_num: 48 loss: 36.8644 acc: 0.78125\n",
      "epoch: 1 batch_num: 49 loss: 38.863 acc: 0.78125\n",
      "epoch: 1 batch_num: 50 loss: 35.9732 acc: 0.796875\n",
      "epoch: 1 batch_num: 51 loss: 39.2024 acc: 0.703125\n",
      "epoch: 1 batch_num: 52 loss: 38.0465 acc: 0.796875\n",
      "epoch: 1 batch_num: 53 loss: 39.5432 acc: 0.75\n",
      "epoch: 1 batch_num: 54 loss: 36.8021 acc: 0.796875\n",
      "epoch: 1 batch_num: 55 loss: 38.8818 acc: 0.71875\n",
      "epoch: 1 batch_num: 56 loss: 38.439 acc: 0.703125\n",
      "epoch: 1 batch_num: 57 loss: 39.8966 acc: 0.640625\n",
      "epoch: 1 batch_num: 58 loss: 36.7055 acc: 0.765625\n",
      "epoch: 1 batch_num: 59 loss: 38.8992 acc: 0.640625\n",
      "epoch: 1 batch_num: 60 loss: 38.8281 acc: 0.6875\n",
      "epoch: 1 batch_num: 61 loss: 37.5977 acc: 0.671875\n",
      "epoch: 1 batch_num: 62 loss: 36.4998 acc: 0.765625\n",
      "epoch: 1 batch_num: 63 loss: 41.5484 acc: 0.65625\n",
      "epoch: 1 batch_num: 64 loss: 37.4668 acc: 0.734375\n",
      "epoch: 1 batch_num: 65 loss: 34.9432 acc: 0.765625\n",
      "epoch: 1 batch_num: 66 loss: 41.3457 acc: 0.640625\n",
      "epoch: 1 batch_num: 67 loss: 35.5772 acc: 0.796875\n",
      "epoch: 1 batch_num: 68 loss: 63.2109 acc: 0.796875\n",
      "epoch: 1 batch_num: 69 loss: 39.7081 acc: 0.671875\n",
      "epoch: 1 batch_num: 70 loss: 33.4507 acc: 0.828125\n",
      "epoch: 1 batch_num: 71 loss: 33.5372 acc: 0.859375\n",
      "epoch: 1 batch_num: 72 loss: 37.4106 acc: 0.84375\n",
      "epoch: 1 batch_num: 73 loss: 36.9695 acc: 0.703125\n",
      "epoch: 1 batch_num: 74 loss: 38.159 acc: 0.734375\n",
      "epoch: 1 batch_num: 75 loss: 36.9754 acc: 0.71875\n",
      "epoch: 1 batch_num: 76 loss: 36.7352 acc: 0.75\n",
      "epoch: 1 batch_num: 77 loss: 33.8556 acc: 0.796875\n",
      "epoch: 1 batch_num: 78 loss: 37.3012 acc: 0.8125\n",
      "epoch: 1 batch_num: 79 loss: 36.7673 acc: 0.671875\n",
      "epoch: 1 batch_num: 80 loss: 35.7571 acc: 0.734375\n",
      "epoch: 1 batch_num: 81 loss: 38.3666 acc: 0.6875\n",
      "epoch: 1 batch_num: 82 loss: 32.1655 acc: 0.8125\n",
      "epoch: 1 batch_num: 83 loss: 40.0414 acc: 0.671875\n",
      "epoch: 1 batch_num: 84 loss: 33.6679 acc: 0.78125\n",
      "epoch: 1 batch_num: 85 loss: 35.7398 acc: 0.796875\n",
      "epoch: 1 batch_num: 86 loss: 37.8441 acc: 0.6875\n",
      "epoch: 1 batch_num: 87 loss: 42.3508 acc: 0.703125\n",
      "epoch: 1 batch_num: 88 loss: 36.1237 acc: 0.75\n",
      "epoch: 1 batch_num: 89 loss: 32.9287 acc: 0.84375\n",
      "epoch: 1 batch_num: 90 loss: 37.1989 acc: 0.734375\n",
      "epoch: 1 batch_num: 91 loss: 34.2949 acc: 0.828125\n",
      "epoch: 1 batch_num: 92 loss: 35.0098 acc: 0.765625\n",
      "epoch: 1 batch_num: 93 loss: 33.6017 acc: 0.78125\n",
      "epoch: 1 batch_num: 94 loss: 33.337 acc: 0.8125\n",
      "epoch: 1 batch_num: 95 loss: 34.9462 acc: 0.765625\n",
      "epoch: 1 batch_num: 96 loss: 32.2336 acc: 0.796875\n",
      "epoch: 1 batch_num: 97 loss: 37.821 acc: 0.734375\n",
      "epoch: 1 batch_num: 98 loss: 32.1338 acc: 0.8125\n",
      "epoch: 1 batch_num: 99 loss: 35.9409 acc: 0.734375\n",
      "epoch: 1 batch_num: 100 loss: 30.4415 acc: 0.734375\n",
      "epoch: 1 batch_num: 101 loss: 40.7569 acc: 0.671875\n",
      "epoch: 1 batch_num: 102 loss: 32.3878 acc: 0.796875\n",
      "epoch: 1 batch_num: 103 loss: 31.3744 acc: 0.8125\n",
      "epoch: 1 batch_num: 104 loss: 30.5327 acc: 0.828125\n",
      "epoch: 1 batch_num: 105 loss: 29.7339 acc: 0.8125\n",
      "epoch: 1 batch_num: 106 loss: 38.4955 acc: 0.703125\n",
      "epoch: 1 batch_num: 107 loss: 38.5673 acc: 0.6875\n",
      "epoch: 1 batch_num: 108 loss: 35.8473 acc: 0.71875\n",
      "epoch: 1 batch_num: 109 loss: 30.3942 acc: 0.859375\n",
      "epoch: 1 batch_num: 110 loss: 33.0918 acc: 0.796875\n",
      "epoch: 1 batch_num: 111 loss: 33.249 acc: 0.765625\n",
      "epoch: 1 batch_num: 112 loss: 31.5822 acc: 0.8125\n",
      "epoch: 1 batch_num: 113 loss: 45.028 acc: 0.78125\n",
      "epoch: 1 batch_num: 114 loss: 28.8536 acc: 0.875\n",
      "epoch: 1 batch_num: 115 loss: 32.8794 acc: 0.734375\n",
      "epoch: 1 batch_num: 116 loss: 35.3737 acc: 0.734375\n",
      "epoch: 1 batch_num: 117 loss: 33.3998 acc: 0.75\n",
      "epoch: 1 batch_num: 118 loss: 34.5071 acc: 0.71875\n",
      "epoch: 1 batch_num: 119 loss: 33.5929 acc: 0.828125\n",
      "epoch: 1 batch_num: 120 loss: 35.5007 acc: 0.75\n",
      "epoch: 1 batch_num: 121 loss: 29.6929 acc: 0.765625\n",
      "epoch: 1 batch_num: 122 loss: 34.9136 acc: 0.78125\n",
      "epoch: 1 batch_num: 123 loss: 27.4302 acc: 0.859375\n",
      "epoch: 1 batch_num: 124 loss: 31.7032 acc: 0.84375\n",
      "epoch: 1 batch_num: 125 loss: 31.728 acc: 0.734375\n",
      "epoch: 1 batch_num: 126 loss: 33.612 acc: 0.75\n",
      "epoch: 1 batch_num: 127 loss: 38.0246 acc: 0.765625\n",
      "epoch: 1 batch_num: 128 loss: 33.9151 acc: 0.75\n",
      "epoch: 1 batch_num: 129 loss: 29.4252 acc: 0.78125\n",
      "epoch: 1 batch_num: 130 loss: 34.087 acc: 0.75\n",
      "epoch: 1 batch_num: 131 loss: 29.8626 acc: 0.8125\n",
      "epoch: 1 batch_num: 132 loss: 31.1399 acc: 0.78125\n",
      "epoch: 1 batch_num: 133 loss: 35.1037 acc: 0.8125\n",
      "epoch: 1 batch_num: 134 loss: 31.6649 acc: 0.75\n",
      "epoch: 1 batch_num: 135 loss: 41.1406 acc: 0.6875\n",
      "epoch: 1 batch_num: 136 loss: 38.7845 acc: 0.75\n",
      "epoch: 1 batch_num: 137 loss: 27.8515 acc: 0.796875\n",
      "epoch: 1 batch_num: 138 loss: 32.6628 acc: 0.6875\n",
      "epoch: 1 batch_num: 139 loss: 32.0167 acc: 0.78125\n",
      "epoch: 1 batch_num: 140 loss: 37.1176 acc: 0.75\n",
      "epoch: 1 batch_num: 141 loss: 33.1154 acc: 0.75\n",
      "epoch: 1 batch_num: 142 loss: 34.8022 acc: 0.765625\n",
      "epoch: 1 batch_num: 143 loss: 27.7763 acc: 0.828125\n",
      "epoch: 1 batch_num: 144 loss: 28.7508 acc: 0.828125\n",
      "epoch: 1 batch_num: 145 loss: 23.7739 acc: 0.890625\n",
      "epoch: 1 batch_num: 146 loss: 34.1664 acc: 0.71875\n",
      "epoch: 1 batch_num: 147 loss: 35.0893 acc: 0.734375\n",
      "epoch: 1 batch_num: 148 loss: 33.4235 acc: 0.75\n",
      "epoch: 1 batch_num: 149 loss: 32.9532 acc: 0.78125\n",
      "epoch: 1 batch_num: 150 loss: 33.5556 acc: 0.75\n",
      "epoch: 1 batch_num: 151 loss: 31.4114 acc: 0.796875\n",
      "epoch: 1 batch_num: 152 loss: 35.5762 acc: 0.734375\n",
      "epoch: 1 batch_num: 153 loss: 30.3119 acc: 0.765625\n",
      "epoch: 1 batch_num: 154 loss: 33.7032 acc: 0.734375\n",
      "epoch: 1 batch_num: 155 loss: 29.2164 acc: 0.796875\n",
      "epoch: 1 batch_num: 156 loss: 29.4889 acc: 0.765625\n",
      "epoch: 1 batch_num: 157 loss: 32.7016 acc: 0.71875\n",
      "epoch: 1 batch_num: 158 loss: 28.0256 acc: 0.8125\n",
      "epoch: 1 batch_num: 159 loss: 30.4695 acc: 0.78125\n",
      "epoch: 1 batch_num: 160 loss: 30.0587 acc: 0.734375\n",
      "epoch: 1 batch_num: 161 loss: 26.39 acc: 0.8125\n",
      "epoch: 1 batch_num: 162 loss: 27.0437 acc: 0.859375\n",
      "epoch: 1 batch_num: 163 loss: 30.4156 acc: 0.78125\n",
      "epoch: 1 batch_num: 164 loss: 36.6143 acc: 0.671875\n",
      "epoch: 1 batch_num: 165 loss: 32.5879 acc: 0.765625\n",
      "epoch: 1 batch_num: 166 loss: 28.8326 acc: 0.796875\n",
      "epoch: 1 batch_num: 167 loss: 34.8849 acc: 0.71875\n",
      "epoch: 1 batch_num: 168 loss: 29.596 acc: 0.78125\n",
      "epoch: 1 batch_num: 169 loss: 32.5926 acc: 0.75\n",
      "epoch: 1 batch_num: 170 loss: 30.3973 acc: 0.765625\n",
      "epoch: 1 batch_num: 171 loss: 33.3178 acc: 0.78125\n",
      "epoch: 1 batch_num: 172 loss: 34.0499 acc: 0.828125\n",
      "epoch: 1 batch_num: 173 loss: 31.0187 acc: 0.8125\n",
      "epoch: 1 batch_num: 174 loss: 28.1538 acc: 0.796875\n",
      "epoch: 1 batch_num: 175 loss: 32.0397 acc: 0.734375\n",
      "epoch: 1 batch_num: 176 loss: 26.9101 acc: 0.8125\n",
      "epoch: 1 batch_num: 177 loss: 28.9661 acc: 0.828125\n",
      "epoch: 1 batch_num: 178 loss: 39.4111 acc: 0.65625\n",
      "epoch: 1 batch_num: 179 loss: 30.4074 acc: 0.8125\n",
      "epoch: 1 batch_num: 180 loss: 40.9556 acc: 0.671875\n",
      "epoch: 1 batch_num: 181 loss: 32.0807 acc: 0.8125\n",
      "epoch: 1 batch_num: 182 loss: 29.665 acc: 0.75\n",
      "epoch: 1 batch_num: 183 loss: 28.1415 acc: 0.8125\n",
      "epoch: 1 batch_num: 184 loss: 26.0637 acc: 0.890625\n",
      "epoch: 1 batch_num: 185 loss: 26.3469 acc: 0.84375\n",
      "epoch: 1 batch_num: 186 loss: 30.8994 acc: 0.765625\n",
      "epoch: 1 batch_num: 187 loss: 38.5157 acc: 0.75\n",
      "epoch: 1 batch_num: 188 loss: 37.2467 acc: 0.78125\n",
      "epoch: 1 batch_num: 189 loss: 30.631 acc: 0.765625\n",
      "epoch: 1 batch_num: 190 loss: 26.0745 acc: 0.828125\n",
      "epoch: 1 batch_num: 191 loss: 32.2882 acc: 0.78125\n",
      "epoch: 1 batch_num: 192 loss: 30.0547 acc: 0.78125\n",
      "epoch: 1 batch_num: 193 loss: 26.289 acc: 0.8125\n",
      "epoch: 1 batch_num: 194 loss: 24.2161 acc: 0.84375\n",
      "epoch: 1 batch_num: 195 loss: 40.6411 acc: 0.75\n",
      "epoch: 1 batch_num: 196 loss: 28.2639 acc: 0.765625\n",
      "epoch: 1 batch_num: 197 loss: 40.4434 acc: 0.71875\n",
      "epoch: 1 batch_num: 198 loss: 29.0514 acc: 0.78125\n",
      "epoch: 1 batch_num: 199 loss: 29.957 acc: 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_num: 200 loss: 26.8201 acc: 0.8125\n",
      "epoch: 1 batch_num: 201 loss: 26.1493 acc: 0.84375\n",
      "epoch: 1 batch_num: 202 loss: 30.1103 acc: 0.8125\n",
      "epoch: 1 batch_num: 203 loss: 28.5292 acc: 0.75\n",
      "epoch: 1 batch_num: 204 loss: 25.324 acc: 0.90625\n",
      "epoch: 1 batch_num: 205 loss: 33.3368 acc: 0.734375\n",
      "epoch: 1 batch_num: 206 loss: 30.3979 acc: 0.796875\n",
      "epoch: 1 batch_num: 207 loss: 25.4685 acc: 0.84375\n",
      "epoch: 1 batch_num: 208 loss: 25.9504 acc: 0.84375\n",
      "epoch: 1 batch_num: 209 loss: 30.8551 acc: 0.765625\n",
      "epoch: 1 batch_num: 210 loss: 28.9676 acc: 0.796875\n",
      "epoch: 1 batch_num: 211 loss: 24.3642 acc: 0.828125\n",
      "epoch: 1 batch_num: 212 loss: 29.1698 acc: 0.75\n",
      "epoch: 1 batch_num: 213 loss: 31.3878 acc: 0.796875\n",
      "epoch: 1 batch_num: 214 loss: 28.9165 acc: 0.78125\n",
      "epoch: 1 batch_num: 215 loss: 1.9836 acc: 1.0\n",
      "epoch: 2 batch_num: 1 loss: 20.8061 acc: 0.921875\n",
      "epoch: 2 batch_num: 2 loss: 28.6255 acc: 0.78125\n",
      "epoch: 2 batch_num: 3 loss: 25.1824 acc: 0.828125\n",
      "epoch: 2 batch_num: 4 loss: 31.8697 acc: 0.75\n",
      "epoch: 2 batch_num: 5 loss: 30.4552 acc: 0.8125\n",
      "epoch: 2 batch_num: 6 loss: 21.1377 acc: 0.890625\n",
      "epoch: 2 batch_num: 7 loss: 21.4675 acc: 0.84375\n",
      "epoch: 2 batch_num: 8 loss: 27.204 acc: 0.8125\n",
      "epoch: 2 batch_num: 9 loss: 29.3036 acc: 0.84375\n",
      "epoch: 2 batch_num: 10 loss: 20.7916 acc: 0.875\n",
      "epoch: 2 batch_num: 11 loss: 31.1448 acc: 0.796875\n",
      "epoch: 2 batch_num: 12 loss: 23.3484 acc: 0.84375\n",
      "epoch: 2 batch_num: 13 loss: 18.8576 acc: 0.9375\n",
      "epoch: 2 batch_num: 14 loss: 21.8616 acc: 0.90625\n",
      "epoch: 2 batch_num: 15 loss: 25.8802 acc: 0.84375\n",
      "epoch: 2 batch_num: 16 loss: 17.992 acc: 0.90625\n",
      "epoch: 2 batch_num: 17 loss: 28.8151 acc: 0.796875\n",
      "epoch: 2 batch_num: 18 loss: 19.4782 acc: 0.875\n",
      "epoch: 2 batch_num: 19 loss: 26.5832 acc: 0.796875\n",
      "epoch: 2 batch_num: 20 loss: 24.3286 acc: 0.859375\n",
      "epoch: 2 batch_num: 21 loss: 20.1296 acc: 0.890625\n",
      "epoch: 2 batch_num: 22 loss: 20.9306 acc: 0.875\n",
      "epoch: 2 batch_num: 23 loss: 22.1575 acc: 0.859375\n",
      "epoch: 2 batch_num: 24 loss: 29.1872 acc: 0.8125\n",
      "epoch: 2 batch_num: 25 loss: 24.6064 acc: 0.828125\n",
      "epoch: 2 batch_num: 26 loss: 21.2092 acc: 0.859375\n",
      "epoch: 2 batch_num: 27 loss: 20.7775 acc: 0.921875\n",
      "epoch: 2 batch_num: 28 loss: 22.5667 acc: 0.84375\n",
      "epoch: 2 batch_num: 29 loss: 25.9836 acc: 0.8125\n",
      "epoch: 2 batch_num: 30 loss: 24.4157 acc: 0.84375\n",
      "epoch: 2 batch_num: 31 loss: 28.713 acc: 0.84375\n",
      "epoch: 2 batch_num: 32 loss: 30.9726 acc: 0.828125\n",
      "epoch: 2 batch_num: 33 loss: 22.6561 acc: 0.859375\n",
      "epoch: 2 batch_num: 34 loss: 24.8788 acc: 0.828125\n",
      "epoch: 2 batch_num: 35 loss: 29.2376 acc: 0.765625\n",
      "epoch: 2 batch_num: 36 loss: 20.3062 acc: 0.921875\n",
      "epoch: 2 batch_num: 37 loss: 22.4285 acc: 0.84375\n",
      "epoch: 2 batch_num: 38 loss: 33.4389 acc: 0.765625\n",
      "epoch: 2 batch_num: 39 loss: 22.0185 acc: 0.859375\n",
      "epoch: 2 batch_num: 40 loss: 30.7612 acc: 0.765625\n",
      "epoch: 2 batch_num: 41 loss: 29.2013 acc: 0.75\n",
      "epoch: 2 batch_num: 42 loss: 16.5905 acc: 0.90625\n",
      "epoch: 2 batch_num: 43 loss: 23.035 acc: 0.875\n",
      "epoch: 2 batch_num: 44 loss: 23.786 acc: 0.828125\n",
      "epoch: 2 batch_num: 45 loss: 21.8578 acc: 0.90625\n",
      "epoch: 2 batch_num: 46 loss: 19.0456 acc: 0.921875\n",
      "epoch: 2 batch_num: 47 loss: 17.9328 acc: 0.90625\n",
      "epoch: 2 batch_num: 48 loss: 28.5207 acc: 0.8125\n",
      "epoch: 2 batch_num: 49 loss: 28.8647 acc: 0.859375\n",
      "epoch: 2 batch_num: 50 loss: 28.2119 acc: 0.765625\n",
      "epoch: 2 batch_num: 51 loss: 20.447 acc: 0.890625\n",
      "epoch: 2 batch_num: 52 loss: 23.0018 acc: 0.875\n",
      "epoch: 2 batch_num: 53 loss: 17.6795 acc: 0.90625\n",
      "epoch: 2 batch_num: 54 loss: 24.6543 acc: 0.890625\n",
      "epoch: 2 batch_num: 55 loss: 21.3979 acc: 0.859375\n",
      "epoch: 2 batch_num: 56 loss: 23.1572 acc: 0.90625\n",
      "epoch: 2 batch_num: 57 loss: 22.9493 acc: 0.84375\n",
      "epoch: 2 batch_num: 58 loss: 23.7705 acc: 0.8125\n",
      "epoch: 2 batch_num: 59 loss: 23.3176 acc: 0.875\n",
      "epoch: 2 batch_num: 60 loss: 21.1631 acc: 0.875\n",
      "epoch: 2 batch_num: 61 loss: 34.1245 acc: 0.796875\n",
      "epoch: 2 batch_num: 62 loss: 28.1499 acc: 0.828125\n",
      "epoch: 2 batch_num: 63 loss: 22.5643 acc: 0.859375\n",
      "epoch: 2 batch_num: 64 loss: 22.9431 acc: 0.84375\n",
      "epoch: 2 batch_num: 65 loss: 26.8452 acc: 0.890625\n",
      "epoch: 2 batch_num: 66 loss: 17.2406 acc: 0.890625\n",
      "epoch: 2 batch_num: 67 loss: 23.1474 acc: 0.84375\n",
      "epoch: 2 batch_num: 68 loss: 25.1257 acc: 0.828125\n",
      "epoch: 2 batch_num: 69 loss: 23.8577 acc: 0.84375\n",
      "epoch: 2 batch_num: 70 loss: 25.2639 acc: 0.84375\n",
      "epoch: 2 batch_num: 71 loss: 23.6678 acc: 0.859375\n",
      "epoch: 2 batch_num: 72 loss: 21.4861 acc: 0.859375\n",
      "epoch: 2 batch_num: 73 loss: 22.9366 acc: 0.90625\n",
      "epoch: 2 batch_num: 74 loss: 30.1779 acc: 0.828125\n",
      "epoch: 2 batch_num: 75 loss: 24.1578 acc: 0.828125\n",
      "epoch: 2 batch_num: 76 loss: 21.7127 acc: 0.890625\n",
      "epoch: 2 batch_num: 77 loss: 21.7826 acc: 0.84375\n",
      "epoch: 2 batch_num: 78 loss: 19.0035 acc: 0.84375\n",
      "epoch: 2 batch_num: 79 loss: 27.6544 acc: 0.8125\n",
      "epoch: 2 batch_num: 80 loss: 28.1258 acc: 0.84375\n",
      "epoch: 2 batch_num: 81 loss: 25.4724 acc: 0.796875\n",
      "epoch: 2 batch_num: 82 loss: 19.9296 acc: 0.859375\n",
      "epoch: 2 batch_num: 83 loss: 32.744 acc: 0.796875\n",
      "epoch: 2 batch_num: 84 loss: 25.9274 acc: 0.875\n",
      "epoch: 2 batch_num: 85 loss: 23.7778 acc: 0.828125\n",
      "epoch: 2 batch_num: 86 loss: 24.2413 acc: 0.859375\n",
      "epoch: 2 batch_num: 87 loss: 26.0951 acc: 0.828125\n",
      "epoch: 2 batch_num: 88 loss: 23.1933 acc: 0.828125\n",
      "epoch: 2 batch_num: 89 loss: 26.0024 acc: 0.796875\n",
      "epoch: 2 batch_num: 90 loss: 29.1147 acc: 0.828125\n",
      "epoch: 2 batch_num: 91 loss: 48.2754 acc: 0.78125\n",
      "epoch: 2 batch_num: 92 loss: 23.9857 acc: 0.828125\n",
      "epoch: 2 batch_num: 93 loss: 28.1018 acc: 0.78125\n",
      "epoch: 2 batch_num: 94 loss: 22.9979 acc: 0.890625\n",
      "epoch: 2 batch_num: 95 loss: 26.471 acc: 0.765625\n",
      "epoch: 2 batch_num: 96 loss: 19.8955 acc: 0.890625\n",
      "epoch: 2 batch_num: 97 loss: 21.3415 acc: 0.84375\n",
      "epoch: 2 batch_num: 98 loss: 18.9173 acc: 0.90625\n",
      "epoch: 2 batch_num: 99 loss: 22.4822 acc: 0.875\n",
      "epoch: 2 batch_num: 100 loss: 18.9431 acc: 0.890625\n",
      "epoch: 2 batch_num: 101 loss: 24.5009 acc: 0.859375\n",
      "epoch: 2 batch_num: 102 loss: 28.0684 acc: 0.84375\n",
      "epoch: 2 batch_num: 103 loss: 24.8706 acc: 0.828125\n",
      "epoch: 2 batch_num: 104 loss: 19.9056 acc: 0.90625\n",
      "epoch: 2 batch_num: 105 loss: 27.6119 acc: 0.8125\n",
      "epoch: 2 batch_num: 106 loss: 16.8165 acc: 0.890625\n",
      "epoch: 2 batch_num: 107 loss: 24.2643 acc: 0.84375\n",
      "epoch: 2 batch_num: 108 loss: 29.0493 acc: 0.875\n",
      "epoch: 2 batch_num: 109 loss: 30.4891 acc: 0.859375\n",
      "epoch: 2 batch_num: 110 loss: 25.6474 acc: 0.828125\n",
      "epoch: 2 batch_num: 111 loss: 20.8827 acc: 0.890625\n",
      "epoch: 2 batch_num: 112 loss: 24.1579 acc: 0.84375\n",
      "epoch: 2 batch_num: 113 loss: 25.9797 acc: 0.828125\n",
      "epoch: 2 batch_num: 114 loss: 27.046 acc: 0.765625\n",
      "epoch: 2 batch_num: 115 loss: 20.817 acc: 0.8125\n",
      "epoch: 2 batch_num: 116 loss: 19.2124 acc: 0.859375\n",
      "epoch: 2 batch_num: 117 loss: 22.9662 acc: 0.828125\n",
      "epoch: 2 batch_num: 118 loss: 27.929 acc: 0.84375\n",
      "epoch: 2 batch_num: 119 loss: 20.8965 acc: 0.84375\n",
      "epoch: 2 batch_num: 120 loss: 27.2894 acc: 0.8125\n",
      "epoch: 2 batch_num: 121 loss: 21.933 acc: 0.890625\n",
      "epoch: 2 batch_num: 122 loss: 23.5743 acc: 0.859375\n",
      "epoch: 2 batch_num: 123 loss: 17.4726 acc: 0.921875\n",
      "epoch: 2 batch_num: 124 loss: 22.5162 acc: 0.84375\n",
      "epoch: 2 batch_num: 125 loss: 23.8401 acc: 0.890625\n",
      "epoch: 2 batch_num: 126 loss: 24.8359 acc: 0.8125\n",
      "epoch: 2 batch_num: 127 loss: 23.202 acc: 0.84375\n",
      "epoch: 2 batch_num: 128 loss: 24.6832 acc: 0.828125\n",
      "epoch: 2 batch_num: 129 loss: 26.8381 acc: 0.828125\n",
      "epoch: 2 batch_num: 130 loss: 17.7346 acc: 0.921875\n",
      "epoch: 2 batch_num: 131 loss: 22.0917 acc: 0.890625\n",
      "epoch: 2 batch_num: 132 loss: 16.5691 acc: 0.90625\n",
      "epoch: 2 batch_num: 133 loss: 33.9127 acc: 0.734375\n",
      "epoch: 2 batch_num: 134 loss: 26.6858 acc: 0.859375\n",
      "epoch: 2 batch_num: 135 loss: 25.7274 acc: 0.8125\n",
      "epoch: 2 batch_num: 136 loss: 22.2819 acc: 0.859375\n",
      "epoch: 2 batch_num: 137 loss: 24.6877 acc: 0.828125\n",
      "epoch: 2 batch_num: 138 loss: 21.5909 acc: 0.921875\n",
      "epoch: 2 batch_num: 139 loss: 28.87 acc: 0.8125\n",
      "epoch: 2 batch_num: 140 loss: 30.4482 acc: 0.765625\n",
      "epoch: 2 batch_num: 141 loss: 15.1212 acc: 0.953125\n",
      "epoch: 2 batch_num: 142 loss: 22.154 acc: 0.859375\n",
      "epoch: 2 batch_num: 143 loss: 20.7846 acc: 0.84375\n",
      "epoch: 2 batch_num: 144 loss: 16.5285 acc: 0.9375\n",
      "epoch: 2 batch_num: 145 loss: 22.685 acc: 0.8125\n",
      "epoch: 2 batch_num: 146 loss: 20.7182 acc: 0.859375\n",
      "epoch: 2 batch_num: 147 loss: 20.135 acc: 0.859375\n",
      "epoch: 2 batch_num: 148 loss: 18.4372 acc: 0.875\n",
      "epoch: 2 batch_num: 149 loss: 24.4784 acc: 0.828125\n",
      "epoch: 2 batch_num: 150 loss: 18.6691 acc: 0.875\n",
      "epoch: 2 batch_num: 151 loss: 22.0558 acc: 0.859375\n",
      "epoch: 2 batch_num: 152 loss: 23.8789 acc: 0.859375\n",
      "epoch: 2 batch_num: 153 loss: 17.1082 acc: 0.90625\n",
      "epoch: 2 batch_num: 154 loss: 15.0737 acc: 0.921875\n",
      "epoch: 2 batch_num: 155 loss: 25.2239 acc: 0.828125\n",
      "epoch: 2 batch_num: 156 loss: 26.2542 acc: 0.8125\n",
      "epoch: 2 batch_num: 157 loss: 30.5057 acc: 0.8125\n",
      "epoch: 2 batch_num: 158 loss: 23.6872 acc: 0.84375\n",
      "epoch: 2 batch_num: 159 loss: 28.5206 acc: 0.78125\n",
      "epoch: 2 batch_num: 160 loss: 22.4736 acc: 0.859375\n",
      "epoch: 2 batch_num: 161 loss: 25.4081 acc: 0.828125\n",
      "epoch: 2 batch_num: 162 loss: 27.5281 acc: 0.765625\n",
      "epoch: 2 batch_num: 163 loss: 31.8268 acc: 0.78125\n",
      "epoch: 2 batch_num: 164 loss: 22.727 acc: 0.84375\n",
      "epoch: 2 batch_num: 165 loss: 17.7028 acc: 0.921875\n",
      "epoch: 2 batch_num: 166 loss: 20.5546 acc: 0.84375\n",
      "epoch: 2 batch_num: 167 loss: 37.5047 acc: 0.734375\n",
      "epoch: 2 batch_num: 168 loss: 21.5317 acc: 0.890625\n",
      "epoch: 2 batch_num: 169 loss: 24.6787 acc: 0.84375\n",
      "epoch: 2 batch_num: 170 loss: 25.5305 acc: 0.875\n",
      "epoch: 2 batch_num: 171 loss: 24.376 acc: 0.8125\n",
      "epoch: 2 batch_num: 172 loss: 22.2492 acc: 0.84375\n",
      "epoch: 2 batch_num: 173 loss: 16.1435 acc: 0.9375\n",
      "epoch: 2 batch_num: 174 loss: 17.6161 acc: 0.921875\n",
      "epoch: 2 batch_num: 175 loss: 19.8909 acc: 0.890625\n",
      "epoch: 2 batch_num: 176 loss: 21.0346 acc: 0.90625\n",
      "epoch: 2 batch_num: 177 loss: 24.9664 acc: 0.8125\n",
      "epoch: 2 batch_num: 178 loss: 17.7124 acc: 0.9375\n",
      "epoch: 2 batch_num: 179 loss: 19.6827 acc: 0.890625\n",
      "epoch: 2 batch_num: 180 loss: 16.8192 acc: 0.9375\n",
      "epoch: 2 batch_num: 181 loss: 21.0008 acc: 0.84375\n",
      "epoch: 2 batch_num: 182 loss: 22.2869 acc: 0.84375\n",
      "epoch: 2 batch_num: 183 loss: 21.1522 acc: 0.890625\n",
      "epoch: 2 batch_num: 184 loss: 27.6133 acc: 0.859375\n",
      "epoch: 2 batch_num: 185 loss: 28.048 acc: 0.796875\n",
      "epoch: 2 batch_num: 186 loss: 24.7248 acc: 0.84375\n",
      "epoch: 2 batch_num: 187 loss: 19.1125 acc: 0.921875\n",
      "epoch: 2 batch_num: 188 loss: 22.5684 acc: 0.875\n",
      "epoch: 2 batch_num: 189 loss: 24.4447 acc: 0.859375\n",
      "epoch: 2 batch_num: 190 loss: 19.892 acc: 0.890625\n",
      "epoch: 2 batch_num: 191 loss: 18.0356 acc: 0.953125\n",
      "epoch: 2 batch_num: 192 loss: 19.9448 acc: 0.90625\n",
      "epoch: 2 batch_num: 193 loss: 19.1236 acc: 0.90625\n",
      "epoch: 2 batch_num: 194 loss: 25.441 acc: 0.828125\n",
      "epoch: 2 batch_num: 195 loss: 25.102 acc: 0.796875\n",
      "epoch: 2 batch_num: 196 loss: 30.1491 acc: 0.796875\n",
      "epoch: 2 batch_num: 197 loss: 25.0822 acc: 0.8125\n",
      "epoch: 2 batch_num: 198 loss: 24.8383 acc: 0.875\n",
      "epoch: 2 batch_num: 199 loss: 24.4438 acc: 0.890625\n",
      "epoch: 2 batch_num: 200 loss: 24.7748 acc: 0.859375\n",
      "epoch: 2 batch_num: 201 loss: 27.3052 acc: 0.78125\n",
      "epoch: 2 batch_num: 202 loss: 21.3604 acc: 0.859375\n",
      "epoch: 2 batch_num: 203 loss: 23.8584 acc: 0.796875\n",
      "epoch: 2 batch_num: 204 loss: 19.8606 acc: 0.875\n",
      "epoch: 2 batch_num: 205 loss: 28.6949 acc: 0.796875\n",
      "epoch: 2 batch_num: 206 loss: 25.6734 acc: 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch_num: 207 loss: 34.4702 acc: 0.8125\n",
      "epoch: 2 batch_num: 208 loss: 20.2433 acc: 0.84375\n",
      "epoch: 2 batch_num: 209 loss: 24.1955 acc: 0.859375\n",
      "epoch: 2 batch_num: 210 loss: 28.5616 acc: 0.84375\n",
      "epoch: 2 batch_num: 211 loss: 25.7277 acc: 0.8125\n",
      "epoch: 2 batch_num: 212 loss: 26.835 acc: 0.8125\n",
      "epoch: 2 batch_num: 213 loss: 34.0574 acc: 0.765625\n",
      "epoch: 2 batch_num: 214 loss: 26.4137 acc: 0.859375\n",
      "epoch: 2 batch_num: 215 loss: 2.3314 acc: 0.8\n",
      "epoch: 3 batch_num: 1 loss: 22.6765 acc: 0.84375\n",
      "epoch: 3 batch_num: 2 loss: 22.1579 acc: 0.859375\n",
      "epoch: 3 batch_num: 3 loss: 14.4896 acc: 0.953125\n",
      "epoch: 3 batch_num: 4 loss: 18.2508 acc: 0.90625\n",
      "epoch: 3 batch_num: 5 loss: 20.1426 acc: 0.890625\n",
      "epoch: 3 batch_num: 6 loss: 24.1968 acc: 0.796875\n",
      "epoch: 3 batch_num: 7 loss: 15.5047 acc: 0.9375\n",
      "epoch: 3 batch_num: 8 loss: 14.0047 acc: 0.953125\n",
      "epoch: 3 batch_num: 9 loss: 15.8012 acc: 0.90625\n",
      "epoch: 3 batch_num: 10 loss: 15.0613 acc: 0.953125\n",
      "epoch: 3 batch_num: 11 loss: 13.9982 acc: 0.90625\n",
      "epoch: 3 batch_num: 12 loss: 22.02 acc: 0.890625\n",
      "epoch: 3 batch_num: 13 loss: 20.4635 acc: 0.921875\n",
      "epoch: 3 batch_num: 14 loss: 18.2638 acc: 0.921875\n",
      "epoch: 3 batch_num: 15 loss: 14.4802 acc: 0.921875\n",
      "epoch: 3 batch_num: 16 loss: 15.9526 acc: 0.921875\n",
      "epoch: 3 batch_num: 17 loss: 20.2765 acc: 0.890625\n",
      "epoch: 3 batch_num: 18 loss: 21.7046 acc: 0.859375\n",
      "epoch: 3 batch_num: 19 loss: 16.4848 acc: 0.9375\n",
      "epoch: 3 batch_num: 20 loss: 15.9809 acc: 0.90625\n",
      "epoch: 3 batch_num: 21 loss: 20.1508 acc: 0.90625\n",
      "epoch: 3 batch_num: 22 loss: 15.3587 acc: 0.9375\n",
      "epoch: 3 batch_num: 23 loss: 13.1693 acc: 0.9375\n",
      "epoch: 3 batch_num: 24 loss: 11.1132 acc: 0.9375\n",
      "epoch: 3 batch_num: 25 loss: 13.5398 acc: 0.9375\n",
      "epoch: 3 batch_num: 26 loss: 15.4597 acc: 0.9375\n",
      "epoch: 3 batch_num: 27 loss: 17.3365 acc: 0.90625\n",
      "epoch: 3 batch_num: 28 loss: 19.2727 acc: 0.90625\n",
      "epoch: 3 batch_num: 29 loss: 14.7764 acc: 0.90625\n",
      "epoch: 3 batch_num: 30 loss: 18.8691 acc: 0.859375\n",
      "epoch: 3 batch_num: 31 loss: 16.2918 acc: 0.875\n",
      "epoch: 3 batch_num: 32 loss: 13.3357 acc: 0.9375\n",
      "epoch: 3 batch_num: 33 loss: 13.7417 acc: 0.921875\n",
      "epoch: 3 batch_num: 34 loss: 15.6385 acc: 0.890625\n",
      "epoch: 3 batch_num: 35 loss: 13.2622 acc: 0.9375\n",
      "epoch: 3 batch_num: 36 loss: 10.7391 acc: 0.953125\n",
      "epoch: 3 batch_num: 37 loss: 18.03 acc: 0.90625\n",
      "epoch: 3 batch_num: 38 loss: 19.5417 acc: 0.890625\n",
      "epoch: 3 batch_num: 39 loss: 21.9022 acc: 0.875\n",
      "epoch: 3 batch_num: 40 loss: 20.2012 acc: 0.84375\n",
      "epoch: 3 batch_num: 41 loss: 16.096 acc: 0.921875\n",
      "epoch: 3 batch_num: 42 loss: 20.8314 acc: 0.84375\n",
      "epoch: 3 batch_num: 43 loss: 15.4663 acc: 0.9375\n",
      "epoch: 3 batch_num: 44 loss: 15.304 acc: 0.921875\n",
      "epoch: 3 batch_num: 45 loss: 15.841 acc: 0.90625\n",
      "epoch: 3 batch_num: 46 loss: 24.7466 acc: 0.859375\n",
      "epoch: 3 batch_num: 47 loss: 23.9843 acc: 0.828125\n",
      "epoch: 3 batch_num: 48 loss: 28.4262 acc: 0.796875\n",
      "epoch: 3 batch_num: 49 loss: 25.2275 acc: 0.8125\n",
      "epoch: 3 batch_num: 50 loss: 16.0732 acc: 0.921875\n",
      "epoch: 3 batch_num: 51 loss: 15.7399 acc: 0.875\n",
      "epoch: 3 batch_num: 52 loss: 24.1902 acc: 0.890625\n",
      "epoch: 3 batch_num: 53 loss: 14.0176 acc: 0.90625\n",
      "epoch: 3 batch_num: 54 loss: 19.3161 acc: 0.890625\n",
      "epoch: 3 batch_num: 55 loss: 15.8137 acc: 0.9375\n",
      "epoch: 3 batch_num: 56 loss: 15.8852 acc: 0.90625\n",
      "epoch: 3 batch_num: 57 loss: 14.122 acc: 0.953125\n",
      "epoch: 3 batch_num: 58 loss: 16.9177 acc: 0.890625\n",
      "epoch: 3 batch_num: 59 loss: 17.6207 acc: 0.890625\n",
      "epoch: 3 batch_num: 60 loss: 17.6319 acc: 0.890625\n",
      "epoch: 3 batch_num: 61 loss: 21.6996 acc: 0.875\n",
      "epoch: 3 batch_num: 62 loss: 18.657 acc: 0.875\n",
      "epoch: 3 batch_num: 63 loss: 17.2052 acc: 0.90625\n",
      "epoch: 3 batch_num: 64 loss: 19.4733 acc: 0.875\n",
      "epoch: 3 batch_num: 65 loss: 18.0938 acc: 0.890625\n",
      "epoch: 3 batch_num: 66 loss: 16.4724 acc: 0.890625\n",
      "epoch: 3 batch_num: 67 loss: 14.8641 acc: 0.90625\n",
      "epoch: 3 batch_num: 68 loss: 21.8905 acc: 0.828125\n",
      "epoch: 3 batch_num: 69 loss: 12.6081 acc: 0.953125\n",
      "epoch: 3 batch_num: 70 loss: 17.5958 acc: 0.890625\n",
      "epoch: 3 batch_num: 71 loss: 24.7912 acc: 0.875\n",
      "epoch: 3 batch_num: 72 loss: 21.5016 acc: 0.859375\n",
      "epoch: 3 batch_num: 73 loss: 26.8567 acc: 0.8125\n",
      "epoch: 3 batch_num: 74 loss: 22.057 acc: 0.828125\n",
      "epoch: 3 batch_num: 75 loss: 20.2575 acc: 0.859375\n",
      "epoch: 3 batch_num: 76 loss: 13.1552 acc: 0.921875\n",
      "epoch: 3 batch_num: 77 loss: 13.2934 acc: 0.90625\n",
      "epoch: 3 batch_num: 78 loss: 14.1356 acc: 0.96875\n",
      "epoch: 3 batch_num: 79 loss: 20.2808 acc: 0.875\n",
      "epoch: 3 batch_num: 80 loss: 19.2492 acc: 0.859375\n",
      "epoch: 3 batch_num: 81 loss: 12.0084 acc: 0.953125\n",
      "epoch: 3 batch_num: 82 loss: 16.3472 acc: 0.921875\n",
      "epoch: 3 batch_num: 83 loss: 17.1488 acc: 0.859375\n",
      "epoch: 3 batch_num: 84 loss: 20.4116 acc: 0.890625\n",
      "epoch: 3 batch_num: 85 loss: 18.4327 acc: 0.90625\n",
      "epoch: 3 batch_num: 86 loss: 16.2359 acc: 0.921875\n",
      "epoch: 3 batch_num: 87 loss: 17.8526 acc: 0.875\n",
      "epoch: 3 batch_num: 88 loss: 20.0856 acc: 0.875\n",
      "epoch: 3 batch_num: 89 loss: 18.4231 acc: 0.875\n",
      "epoch: 3 batch_num: 90 loss: 24.2679 acc: 0.84375\n",
      "epoch: 3 batch_num: 91 loss: 23.8315 acc: 0.859375\n",
      "epoch: 3 batch_num: 92 loss: 22.4931 acc: 0.875\n",
      "epoch: 3 batch_num: 93 loss: 17.3109 acc: 0.921875\n",
      "epoch: 3 batch_num: 94 loss: 13.0558 acc: 0.953125\n",
      "epoch: 3 batch_num: 95 loss: 21.9788 acc: 0.875\n",
      "epoch: 3 batch_num: 96 loss: 14.4367 acc: 0.90625\n",
      "epoch: 3 batch_num: 97 loss: 19.2304 acc: 0.890625\n",
      "epoch: 3 batch_num: 98 loss: 21.6436 acc: 0.84375\n",
      "epoch: 3 batch_num: 99 loss: 13.2591 acc: 0.921875\n",
      "epoch: 3 batch_num: 100 loss: 17.2729 acc: 0.890625\n",
      "epoch: 3 batch_num: 101 loss: 15.7567 acc: 0.90625\n",
      "epoch: 3 batch_num: 102 loss: 20.4952 acc: 0.890625\n",
      "epoch: 3 batch_num: 103 loss: 19.8369 acc: 0.890625\n",
      "epoch: 3 batch_num: 104 loss: 14.472 acc: 0.9375\n",
      "epoch: 3 batch_num: 105 loss: 20.4734 acc: 0.90625\n",
      "epoch: 3 batch_num: 106 loss: 18.4377 acc: 0.90625\n",
      "epoch: 3 batch_num: 107 loss: 18.7053 acc: 0.890625\n",
      "epoch: 3 batch_num: 108 loss: 19.2833 acc: 0.90625\n",
      "epoch: 3 batch_num: 109 loss: 12.0281 acc: 0.9375\n",
      "epoch: 3 batch_num: 110 loss: 22.176 acc: 0.859375\n",
      "epoch: 3 batch_num: 111 loss: 15.9292 acc: 0.890625\n",
      "epoch: 3 batch_num: 112 loss: 26.7605 acc: 0.84375\n",
      "epoch: 3 batch_num: 113 loss: 21.4126 acc: 0.875\n",
      "epoch: 3 batch_num: 114 loss: 13.0548 acc: 0.953125\n",
      "epoch: 3 batch_num: 115 loss: 16.0737 acc: 0.90625\n",
      "epoch: 3 batch_num: 116 loss: 19.6277 acc: 0.90625\n",
      "epoch: 3 batch_num: 117 loss: 13.5647 acc: 0.9375\n",
      "epoch: 3 batch_num: 118 loss: 17.4632 acc: 0.921875\n",
      "epoch: 3 batch_num: 119 loss: 15.8059 acc: 0.921875\n",
      "epoch: 3 batch_num: 120 loss: 28.3823 acc: 0.953125\n",
      "epoch: 3 batch_num: 121 loss: 24.4854 acc: 0.875\n",
      "epoch: 3 batch_num: 122 loss: 17.2136 acc: 0.921875\n",
      "epoch: 3 batch_num: 123 loss: 15.7114 acc: 0.875\n",
      "epoch: 3 batch_num: 124 loss: 22.7547 acc: 0.890625\n",
      "epoch: 3 batch_num: 125 loss: 28.9062 acc: 0.8125\n",
      "epoch: 3 batch_num: 126 loss: 17.9983 acc: 0.890625\n",
      "epoch: 3 batch_num: 127 loss: 22.0259 acc: 0.890625\n",
      "epoch: 3 batch_num: 128 loss: 14.0358 acc: 0.953125\n",
      "epoch: 3 batch_num: 129 loss: 14.8581 acc: 0.9375\n",
      "epoch: 3 batch_num: 130 loss: 19.9382 acc: 0.859375\n",
      "epoch: 3 batch_num: 131 loss: 15.2837 acc: 0.921875\n",
      "epoch: 3 batch_num: 132 loss: 15.2678 acc: 0.921875\n",
      "epoch: 3 batch_num: 133 loss: 11.6138 acc: 0.953125\n",
      "epoch: 3 batch_num: 134 loss: 26.5849 acc: 0.84375\n",
      "epoch: 3 batch_num: 135 loss: 16.9109 acc: 0.890625\n",
      "epoch: 3 batch_num: 136 loss: 18.1612 acc: 0.921875\n",
      "epoch: 3 batch_num: 137 loss: 17.7232 acc: 0.875\n",
      "epoch: 3 batch_num: 138 loss: 20.0699 acc: 0.875\n",
      "epoch: 3 batch_num: 139 loss: 14.3569 acc: 0.90625\n",
      "epoch: 3 batch_num: 140 loss: 15.5467 acc: 0.9375\n",
      "epoch: 3 batch_num: 141 loss: 14.0875 acc: 0.921875\n",
      "epoch: 3 batch_num: 142 loss: 20.3714 acc: 0.859375\n",
      "epoch: 3 batch_num: 143 loss: 17.8552 acc: 0.921875\n",
      "epoch: 3 batch_num: 144 loss: 18.1276 acc: 0.90625\n",
      "epoch: 3 batch_num: 145 loss: 22.0486 acc: 0.84375\n",
      "epoch: 3 batch_num: 146 loss: 20.5041 acc: 0.890625\n",
      "epoch: 3 batch_num: 147 loss: 13.4143 acc: 0.9375\n",
      "epoch: 3 batch_num: 148 loss: 12.9975 acc: 0.953125\n",
      "epoch: 3 batch_num: 149 loss: 13.0711 acc: 0.9375\n",
      "epoch: 3 batch_num: 150 loss: 14.5249 acc: 0.90625\n",
      "epoch: 3 batch_num: 151 loss: 15.9444 acc: 0.90625\n",
      "epoch: 3 batch_num: 152 loss: 16.4525 acc: 0.890625\n",
      "epoch: 3 batch_num: 153 loss: 20.3071 acc: 0.875\n",
      "epoch: 3 batch_num: 154 loss: 15.9044 acc: 0.90625\n",
      "epoch: 3 batch_num: 155 loss: 15.8337 acc: 0.921875\n",
      "epoch: 3 batch_num: 156 loss: 15.4828 acc: 0.90625\n",
      "epoch: 3 batch_num: 157 loss: 11.9602 acc: 0.9375\n",
      "epoch: 3 batch_num: 158 loss: 21.1247 acc: 0.890625\n",
      "epoch: 3 batch_num: 159 loss: 12.0115 acc: 0.9375\n",
      "epoch: 3 batch_num: 160 loss: 14.3232 acc: 0.921875\n",
      "epoch: 3 batch_num: 161 loss: 14.6609 acc: 0.921875\n",
      "epoch: 3 batch_num: 162 loss: 20.381 acc: 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 batch_num: 163 loss: 15.0984 acc: 0.890625\n",
      "epoch: 3 batch_num: 164 loss: 16.3784 acc: 0.859375\n",
      "epoch: 3 batch_num: 165 loss: 21.2042 acc: 0.859375\n",
      "epoch: 3 batch_num: 166 loss: 17.6731 acc: 0.9375\n",
      "epoch: 3 batch_num: 167 loss: 19.0933 acc: 0.875\n",
      "epoch: 3 batch_num: 168 loss: 9.4246 acc: 1.0\n",
      "epoch: 3 batch_num: 169 loss: 19.5195 acc: 0.875\n",
      "epoch: 3 batch_num: 170 loss: 18.0781 acc: 0.859375\n",
      "epoch: 3 batch_num: 171 loss: 17.5875 acc: 0.921875\n",
      "epoch: 3 batch_num: 172 loss: 15.045 acc: 0.921875\n",
      "epoch: 3 batch_num: 173 loss: 23.5974 acc: 0.84375\n",
      "epoch: 3 batch_num: 174 loss: 21.2827 acc: 0.890625\n",
      "epoch: 3 batch_num: 175 loss: 13.9855 acc: 0.921875\n",
      "epoch: 3 batch_num: 176 loss: 22.1685 acc: 0.84375\n",
      "epoch: 3 batch_num: 177 loss: 9.2617 acc: 0.984375\n",
      "epoch: 3 batch_num: 178 loss: 19.8459 acc: 0.875\n",
      "epoch: 3 batch_num: 179 loss: 19.8476 acc: 0.859375\n",
      "epoch: 3 batch_num: 180 loss: 16.0412 acc: 0.90625\n",
      "epoch: 3 batch_num: 181 loss: 19.4383 acc: 0.859375\n",
      "epoch: 3 batch_num: 182 loss: 13.5976 acc: 0.9375\n",
      "epoch: 3 batch_num: 183 loss: 16.1404 acc: 0.921875\n",
      "epoch: 3 batch_num: 184 loss: 18.5874 acc: 0.890625\n",
      "epoch: 3 batch_num: 185 loss: 13.3598 acc: 0.90625\n",
      "epoch: 3 batch_num: 186 loss: 16.5586 acc: 0.90625\n",
      "epoch: 3 batch_num: 187 loss: 16.2554 acc: 0.890625\n",
      "epoch: 3 batch_num: 188 loss: 14.4857 acc: 0.90625\n",
      "epoch: 3 batch_num: 189 loss: 15.5975 acc: 0.890625\n",
      "epoch: 3 batch_num: 190 loss: 22.3648 acc: 0.828125\n",
      "epoch: 3 batch_num: 191 loss: 18.9082 acc: 0.859375\n",
      "epoch: 3 batch_num: 192 loss: 20.386 acc: 0.90625\n",
      "epoch: 3 batch_num: 193 loss: 18.5694 acc: 0.921875\n",
      "epoch: 3 batch_num: 194 loss: 16.8955 acc: 0.875\n",
      "epoch: 3 batch_num: 195 loss: 16.4068 acc: 0.90625\n",
      "epoch: 3 batch_num: 196 loss: 22.9139 acc: 0.828125\n",
      "epoch: 3 batch_num: 197 loss: 10.441 acc: 0.953125\n",
      "epoch: 3 batch_num: 198 loss: 18.3489 acc: 0.859375\n",
      "epoch: 3 batch_num: 199 loss: 19.0233 acc: 0.875\n",
      "epoch: 3 batch_num: 200 loss: 23.5539 acc: 0.8125\n",
      "epoch: 3 batch_num: 201 loss: 20.0437 acc: 0.875\n",
      "epoch: 3 batch_num: 202 loss: 16.2443 acc: 0.859375\n",
      "epoch: 3 batch_num: 203 loss: 16.3194 acc: 0.90625\n",
      "epoch: 3 batch_num: 204 loss: 19.8099 acc: 0.859375\n",
      "epoch: 3 batch_num: 205 loss: 14.9409 acc: 0.9375\n",
      "epoch: 3 batch_num: 206 loss: 22.5995 acc: 0.90625\n",
      "epoch: 3 batch_num: 207 loss: 14.4673 acc: 0.921875\n",
      "epoch: 3 batch_num: 208 loss: 20.8003 acc: 0.875\n",
      "epoch: 3 batch_num: 209 loss: 25.0162 acc: 0.84375\n",
      "epoch: 3 batch_num: 210 loss: 24.6887 acc: 0.828125\n",
      "epoch: 3 batch_num: 211 loss: 20.563 acc: 0.875\n",
      "epoch: 3 batch_num: 212 loss: 20.1386 acc: 0.890625\n",
      "epoch: 3 batch_num: 213 loss: 17.8162 acc: 0.875\n",
      "epoch: 3 batch_num: 214 loss: 17.845 acc: 0.890625\n",
      "epoch: 3 batch_num: 215 loss: 2.8441 acc: 0.6\n",
      "epoch: 4 batch_num: 1 loss: 12.9384 acc: 0.921875\n",
      "epoch: 4 batch_num: 2 loss: 11.9131 acc: 0.984375\n",
      "epoch: 4 batch_num: 3 loss: 12.0305 acc: 0.9375\n",
      "epoch: 4 batch_num: 4 loss: 14.0904 acc: 0.90625\n",
      "epoch: 4 batch_num: 5 loss: 10.1341 acc: 0.984375\n",
      "epoch: 4 batch_num: 6 loss: 11.7042 acc: 0.953125\n",
      "epoch: 4 batch_num: 7 loss: 13.8321 acc: 0.9375\n",
      "epoch: 4 batch_num: 8 loss: 9.0848 acc: 0.96875\n",
      "epoch: 4 batch_num: 9 loss: 11.1455 acc: 0.953125\n",
      "epoch: 4 batch_num: 10 loss: 18.8976 acc: 0.921875\n",
      "epoch: 4 batch_num: 11 loss: 15.6547 acc: 0.921875\n",
      "epoch: 4 batch_num: 12 loss: 12.4678 acc: 0.953125\n",
      "epoch: 4 batch_num: 13 loss: 12.5389 acc: 0.9375\n",
      "epoch: 4 batch_num: 14 loss: 12.6232 acc: 0.9375\n",
      "epoch: 4 batch_num: 15 loss: 13.8227 acc: 0.9375\n",
      "epoch: 4 batch_num: 16 loss: 13.6511 acc: 0.9375\n",
      "epoch: 4 batch_num: 17 loss: 12.4395 acc: 0.953125\n",
      "epoch: 4 batch_num: 18 loss: 10.9339 acc: 0.984375\n",
      "epoch: 4 batch_num: 19 loss: 10.5782 acc: 0.9375\n",
      "epoch: 4 batch_num: 20 loss: 11.4672 acc: 0.953125\n",
      "epoch: 4 batch_num: 21 loss: 9.3819 acc: 0.953125\n",
      "epoch: 4 batch_num: 22 loss: 8.9066 acc: 0.96875\n",
      "epoch: 4 batch_num: 23 loss: 11.6728 acc: 0.953125\n",
      "epoch: 4 batch_num: 24 loss: 13.809 acc: 0.953125\n",
      "epoch: 4 batch_num: 25 loss: 13.0414 acc: 0.9375\n",
      "epoch: 4 batch_num: 26 loss: 18.1638 acc: 0.921875\n",
      "epoch: 4 batch_num: 27 loss: 30.3347 acc: 0.953125\n",
      "epoch: 4 batch_num: 28 loss: 14.5187 acc: 0.953125\n",
      "epoch: 4 batch_num: 29 loss: 9.25 acc: 0.984375\n",
      "epoch: 4 batch_num: 30 loss: 14.8867 acc: 0.9375\n",
      "epoch: 4 batch_num: 31 loss: 14.9737 acc: 0.921875\n",
      "epoch: 4 batch_num: 32 loss: 17.2649 acc: 0.9375\n",
      "epoch: 4 batch_num: 33 loss: 15.5592 acc: 0.953125\n",
      "epoch: 4 batch_num: 34 loss: 8.9419 acc: 0.984375\n",
      "epoch: 4 batch_num: 35 loss: 8.8719 acc: 0.96875\n",
      "epoch: 4 batch_num: 36 loss: 8.9141 acc: 0.96875\n",
      "epoch: 4 batch_num: 37 loss: 8.9835 acc: 0.9375\n",
      "epoch: 4 batch_num: 38 loss: 16.6928 acc: 0.921875\n",
      "epoch: 4 batch_num: 39 loss: 12.6794 acc: 0.921875\n",
      "epoch: 4 batch_num: 40 loss: 12.3789 acc: 0.953125\n",
      "epoch: 4 batch_num: 41 loss: 13.7888 acc: 0.921875\n",
      "epoch: 4 batch_num: 42 loss: 12.5481 acc: 0.9375\n",
      "epoch: 4 batch_num: 43 loss: 7.9707 acc: 0.96875\n",
      "epoch: 4 batch_num: 44 loss: 14.3926 acc: 0.921875\n",
      "epoch: 4 batch_num: 45 loss: 11.9631 acc: 0.9375\n",
      "epoch: 4 batch_num: 46 loss: 20.2811 acc: 0.90625\n",
      "epoch: 4 batch_num: 47 loss: 12.2365 acc: 0.90625\n",
      "epoch: 4 batch_num: 48 loss: 15.5151 acc: 0.921875\n",
      "epoch: 4 batch_num: 49 loss: 11.0896 acc: 0.953125\n",
      "epoch: 4 batch_num: 50 loss: 16.2199 acc: 0.9375\n",
      "epoch: 4 batch_num: 51 loss: 15.757 acc: 0.9375\n",
      "epoch: 4 batch_num: 52 loss: 18.215 acc: 0.890625\n",
      "epoch: 4 batch_num: 53 loss: 15.0067 acc: 0.921875\n",
      "epoch: 4 batch_num: 54 loss: 7.3237 acc: 0.984375\n",
      "epoch: 4 batch_num: 55 loss: 7.3229 acc: 1.0\n",
      "epoch: 4 batch_num: 56 loss: 12.3014 acc: 0.921875\n",
      "epoch: 4 batch_num: 57 loss: 14.7369 acc: 0.921875\n",
      "epoch: 4 batch_num: 58 loss: 15.411 acc: 0.921875\n",
      "epoch: 4 batch_num: 59 loss: 17.2824 acc: 0.875\n",
      "epoch: 4 batch_num: 60 loss: 9.109 acc: 0.953125\n",
      "epoch: 4 batch_num: 61 loss: 13.1805 acc: 0.921875\n",
      "epoch: 4 batch_num: 62 loss: 10.4507 acc: 0.953125\n",
      "epoch: 4 batch_num: 63 loss: 14.3122 acc: 0.90625\n",
      "epoch: 4 batch_num: 64 loss: 16.0991 acc: 0.90625\n",
      "epoch: 4 batch_num: 65 loss: 15.6188 acc: 0.90625\n",
      "epoch: 4 batch_num: 66 loss: 11.3721 acc: 0.9375\n",
      "epoch: 4 batch_num: 67 loss: 13.39 acc: 0.921875\n",
      "epoch: 4 batch_num: 68 loss: 14.2369 acc: 0.921875\n",
      "epoch: 4 batch_num: 69 loss: 17.4998 acc: 0.875\n",
      "epoch: 4 batch_num: 70 loss: 11.5831 acc: 0.9375\n",
      "epoch: 4 batch_num: 71 loss: 12.4427 acc: 0.921875\n",
      "epoch: 4 batch_num: 72 loss: 11.0349 acc: 0.953125\n",
      "epoch: 4 batch_num: 73 loss: 11.9724 acc: 0.921875\n",
      "epoch: 4 batch_num: 74 loss: 7.0975 acc: 1.0\n",
      "epoch: 4 batch_num: 75 loss: 15.6323 acc: 0.875\n",
      "epoch: 4 batch_num: 76 loss: 9.4601 acc: 0.96875\n",
      "epoch: 4 batch_num: 77 loss: 10.8476 acc: 0.921875\n",
      "epoch: 4 batch_num: 78 loss: 16.9933 acc: 0.875\n",
      "epoch: 4 batch_num: 79 loss: 8.1287 acc: 0.96875\n",
      "epoch: 4 batch_num: 80 loss: 11.3012 acc: 0.9375\n",
      "epoch: 4 batch_num: 81 loss: 10.883 acc: 0.953125\n",
      "epoch: 4 batch_num: 82 loss: 12.4537 acc: 0.9375\n",
      "epoch: 4 batch_num: 83 loss: 15.5407 acc: 0.875\n",
      "epoch: 4 batch_num: 84 loss: 13.8468 acc: 0.921875\n",
      "epoch: 4 batch_num: 85 loss: 13.9026 acc: 0.90625\n",
      "epoch: 4 batch_num: 86 loss: 13.8725 acc: 0.921875\n",
      "epoch: 4 batch_num: 87 loss: 11.5804 acc: 0.9375\n",
      "epoch: 4 batch_num: 88 loss: 15.3424 acc: 0.890625\n",
      "epoch: 4 batch_num: 89 loss: 8.7495 acc: 0.96875\n",
      "epoch: 4 batch_num: 90 loss: 10.7003 acc: 0.953125\n",
      "epoch: 4 batch_num: 91 loss: 11.2219 acc: 0.9375\n",
      "epoch: 4 batch_num: 92 loss: 10.9063 acc: 0.9375\n",
      "epoch: 4 batch_num: 93 loss: 7.4386 acc: 0.984375\n",
      "epoch: 4 batch_num: 94 loss: 18.018 acc: 0.90625\n",
      "epoch: 4 batch_num: 95 loss: 14.2287 acc: 0.921875\n",
      "epoch: 4 batch_num: 96 loss: 19.75 acc: 0.890625\n",
      "epoch: 4 batch_num: 97 loss: 12.8588 acc: 0.921875\n",
      "epoch: 4 batch_num: 98 loss: 10.7219 acc: 0.9375\n",
      "epoch: 4 batch_num: 99 loss: 15.6614 acc: 0.9375\n",
      "epoch: 4 batch_num: 100 loss: 15.7363 acc: 0.875\n",
      "epoch: 4 batch_num: 101 loss: 13.9333 acc: 0.921875\n",
      "epoch: 4 batch_num: 102 loss: 11.1494 acc: 0.953125\n",
      "epoch: 4 batch_num: 103 loss: 11.494 acc: 0.96875\n",
      "epoch: 4 batch_num: 104 loss: 18.3631 acc: 0.90625\n",
      "epoch: 4 batch_num: 105 loss: 13.643 acc: 0.90625\n",
      "epoch: 4 batch_num: 106 loss: 15.2474 acc: 0.90625\n",
      "epoch: 4 batch_num: 107 loss: 11.1925 acc: 0.9375\n",
      "epoch: 4 batch_num: 108 loss: 12.1123 acc: 0.921875\n",
      "epoch: 4 batch_num: 109 loss: 11.2763 acc: 0.96875\n",
      "epoch: 4 batch_num: 110 loss: 13.9851 acc: 0.921875\n",
      "epoch: 4 batch_num: 111 loss: 10.7811 acc: 0.9375\n",
      "epoch: 4 batch_num: 112 loss: 11.182 acc: 0.9375\n",
      "epoch: 4 batch_num: 113 loss: 10.2219 acc: 0.96875\n",
      "epoch: 4 batch_num: 114 loss: 15.7762 acc: 0.921875\n",
      "epoch: 4 batch_num: 115 loss: 17.6228 acc: 0.875\n",
      "epoch: 4 batch_num: 116 loss: 7.6903 acc: 0.96875\n",
      "epoch: 4 batch_num: 117 loss: 11.3184 acc: 0.96875\n",
      "epoch: 4 batch_num: 118 loss: 10.2177 acc: 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 batch_num: 119 loss: 14.2129 acc: 0.9375\n",
      "epoch: 4 batch_num: 120 loss: 12.0775 acc: 0.953125\n",
      "epoch: 4 batch_num: 121 loss: 14.6275 acc: 0.890625\n",
      "epoch: 4 batch_num: 122 loss: 15.0041 acc: 0.953125\n",
      "epoch: 4 batch_num: 123 loss: 14.0199 acc: 0.890625\n",
      "epoch: 4 batch_num: 124 loss: 14.3763 acc: 0.921875\n",
      "epoch: 4 batch_num: 125 loss: 21.7991 acc: 0.859375\n",
      "epoch: 4 batch_num: 126 loss: 17.8419 acc: 0.90625\n",
      "epoch: 4 batch_num: 127 loss: 10.993 acc: 0.96875\n",
      "epoch: 4 batch_num: 128 loss: 7.6408 acc: 1.0\n",
      "epoch: 4 batch_num: 129 loss: 13.7259 acc: 0.921875\n",
      "epoch: 4 batch_num: 130 loss: 12.0638 acc: 0.953125\n",
      "epoch: 4 batch_num: 131 loss: 13.5236 acc: 0.890625\n",
      "epoch: 4 batch_num: 132 loss: 12.4681 acc: 0.9375\n",
      "epoch: 4 batch_num: 133 loss: 12.3894 acc: 0.9375\n",
      "epoch: 4 batch_num: 134 loss: 9.3955 acc: 0.96875\n",
      "epoch: 4 batch_num: 135 loss: 8.6478 acc: 0.96875\n",
      "epoch: 4 batch_num: 136 loss: 11.3799 acc: 0.96875\n",
      "epoch: 4 batch_num: 137 loss: 11.0641 acc: 0.921875\n",
      "epoch: 4 batch_num: 138 loss: 10.8119 acc: 0.921875\n",
      "epoch: 4 batch_num: 139 loss: 9.4447 acc: 0.921875\n",
      "epoch: 4 batch_num: 140 loss: 15.3759 acc: 0.90625\n",
      "epoch: 4 batch_num: 141 loss: 18.8413 acc: 0.875\n",
      "epoch: 4 batch_num: 142 loss: 16.8759 acc: 0.921875\n",
      "epoch: 4 batch_num: 143 loss: 15.2841 acc: 0.9375\n",
      "epoch: 4 batch_num: 144 loss: 11.6062 acc: 0.921875\n",
      "epoch: 4 batch_num: 145 loss: 11.8408 acc: 0.9375\n",
      "epoch: 4 batch_num: 146 loss: 9.3261 acc: 0.953125\n",
      "epoch: 4 batch_num: 147 loss: 9.2582 acc: 0.96875\n",
      "epoch: 4 batch_num: 148 loss: 7.9961 acc: 0.984375\n",
      "epoch: 4 batch_num: 149 loss: 6.49 acc: 0.96875\n",
      "epoch: 4 batch_num: 150 loss: 8.0263 acc: 0.96875\n",
      "epoch: 4 batch_num: 151 loss: 11.391 acc: 0.921875\n",
      "epoch: 4 batch_num: 152 loss: 15.1298 acc: 0.90625\n",
      "epoch: 4 batch_num: 153 loss: 22.2605 acc: 0.875\n",
      "epoch: 4 batch_num: 154 loss: 7.8943 acc: 0.96875\n",
      "epoch: 4 batch_num: 155 loss: 16.805 acc: 0.921875\n",
      "epoch: 4 batch_num: 156 loss: 11.9201 acc: 0.921875\n",
      "epoch: 4 batch_num: 157 loss: 17.629 acc: 0.875\n",
      "epoch: 4 batch_num: 158 loss: 16.7889 acc: 0.890625\n",
      "epoch: 4 batch_num: 159 loss: 14.6731 acc: 0.90625\n",
      "epoch: 4 batch_num: 160 loss: 9.5771 acc: 0.96875\n",
      "epoch: 4 batch_num: 161 loss: 15.3337 acc: 0.921875\n",
      "epoch: 4 batch_num: 162 loss: 15.3538 acc: 0.875\n",
      "epoch: 4 batch_num: 163 loss: 15.1985 acc: 0.84375\n",
      "epoch: 4 batch_num: 164 loss: 17.4289 acc: 0.890625\n",
      "epoch: 4 batch_num: 165 loss: 10.278 acc: 0.96875\n",
      "epoch: 4 batch_num: 166 loss: 11.2522 acc: 0.96875\n",
      "epoch: 4 batch_num: 167 loss: 11.2041 acc: 0.9375\n",
      "epoch: 4 batch_num: 168 loss: 10.5584 acc: 0.9375\n",
      "epoch: 4 batch_num: 169 loss: 10.0765 acc: 0.984375\n",
      "epoch: 4 batch_num: 170 loss: 9.8357 acc: 0.9375\n",
      "epoch: 4 batch_num: 171 loss: 21.464 acc: 0.84375\n",
      "epoch: 4 batch_num: 172 loss: 12.7986 acc: 0.9375\n",
      "epoch: 4 batch_num: 173 loss: 13.6249 acc: 0.921875\n",
      "epoch: 4 batch_num: 174 loss: 10.084 acc: 0.96875\n",
      "epoch: 4 batch_num: 175 loss: 11.7928 acc: 0.9375\n",
      "epoch: 4 batch_num: 176 loss: 10.9446 acc: 0.953125\n",
      "epoch: 4 batch_num: 177 loss: 18.3722 acc: 0.90625\n",
      "epoch: 4 batch_num: 178 loss: 14.2739 acc: 0.921875\n",
      "epoch: 4 batch_num: 179 loss: 11.9568 acc: 0.90625\n",
      "epoch: 4 batch_num: 180 loss: 11.9201 acc: 0.921875\n",
      "epoch: 4 batch_num: 181 loss: 12.498 acc: 0.953125\n",
      "epoch: 4 batch_num: 182 loss: 17.7568 acc: 0.890625\n",
      "epoch: 4 batch_num: 183 loss: 8.2256 acc: 0.984375\n",
      "epoch: 4 batch_num: 184 loss: 7.3282 acc: 0.953125\n",
      "epoch: 4 batch_num: 185 loss: 16.6036 acc: 0.890625\n",
      "epoch: 4 batch_num: 186 loss: 9.7137 acc: 0.984375\n",
      "epoch: 4 batch_num: 187 loss: 7.9175 acc: 0.96875\n",
      "epoch: 4 batch_num: 188 loss: 19.9677 acc: 0.859375\n",
      "epoch: 4 batch_num: 189 loss: 15.1595 acc: 0.890625\n",
      "epoch: 4 batch_num: 190 loss: 11.6924 acc: 0.9375\n",
      "epoch: 4 batch_num: 191 loss: 16.8886 acc: 0.875\n",
      "epoch: 4 batch_num: 192 loss: 14.0312 acc: 0.9375\n",
      "epoch: 4 batch_num: 193 loss: 10.4312 acc: 0.96875\n",
      "epoch: 4 batch_num: 194 loss: 10.1894 acc: 0.953125\n",
      "epoch: 4 batch_num: 195 loss: 9.6462 acc: 0.9375\n",
      "epoch: 4 batch_num: 196 loss: 10.3182 acc: 0.953125\n",
      "epoch: 4 batch_num: 197 loss: 10.701 acc: 0.921875\n",
      "epoch: 4 batch_num: 198 loss: 12.9091 acc: 0.890625\n",
      "epoch: 4 batch_num: 199 loss: 10.8304 acc: 0.953125\n",
      "epoch: 4 batch_num: 200 loss: 16.1275 acc: 0.890625\n",
      "epoch: 4 batch_num: 201 loss: 15.4626 acc: 0.9375\n",
      "epoch: 4 batch_num: 202 loss: 12.5407 acc: 0.9375\n",
      "epoch: 4 batch_num: 203 loss: 11.0258 acc: 0.953125\n",
      "epoch: 4 batch_num: 204 loss: 11.8009 acc: 0.921875\n",
      "epoch: 4 batch_num: 205 loss: 17.1102 acc: 0.84375\n",
      "epoch: 4 batch_num: 206 loss: 8.801 acc: 0.984375\n",
      "epoch: 4 batch_num: 207 loss: 9.0707 acc: 0.953125\n",
      "epoch: 4 batch_num: 208 loss: 13.7582 acc: 0.921875\n",
      "epoch: 4 batch_num: 209 loss: 15.1227 acc: 0.921875\n",
      "epoch: 4 batch_num: 210 loss: 9.5897 acc: 0.96875\n",
      "epoch: 4 batch_num: 211 loss: 13.6818 acc: 0.96875\n",
      "epoch: 4 batch_num: 212 loss: 12.5067 acc: 0.921875\n",
      "epoch: 4 batch_num: 213 loss: 16.7729 acc: 0.859375\n",
      "epoch: 4 batch_num: 214 loss: 14.0843 acc: 0.890625\n",
      "epoch: 4 batch_num: 215 loss: 1.2358 acc: 1.0\n",
      "max_acc: 1.0\n",
      "test acc: 0.7793594306049823\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\", weight=torch.FloatTensor([1, 1]).to(device))  # loss function\n",
    "finnish_model = AnswerableClassifier(vocab_size=2*finnish_vocab_size + 7, num_labels=2, num_hidden=100).to('cuda')\n",
    "optimizer = torch.optim.Adam(finnish_model.parameters(), lr = 0.0005, amsgrad=True)\n",
    "\n",
    "max_acc = train_features_model(model = finnish_model, train_loader=train_features_model_loader,\n",
    "                               criterion= criterion, optimizer=optimizer, model_file_name=\"finnish_model.pth\",\n",
    "                               epochs = 4)\n",
    "print(\"max_acc:\", max_acc)\n",
    "finnish_model.load_state_dict(torch.load(\"finnish_model.pth\"))\n",
    "finnish_model.eval()\n",
    "predict_label = finnish_model(val_features)\n",
    "pred = predict_label.max(-1, keepdim=True)[1]\n",
    "label = val_label\n",
    "test_acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]\n",
    "\n",
    "print(\"test acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix : \n",
      " [[712 131]\n",
      " [241 602]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.74711438, 0.8212824 ]),\n",
       " array([0.84460261, 0.71411625]),\n",
       " array([0.79287305, 0.76395939]),\n",
       " array([843, 843]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"confusion matrix : \\n\", confusion_matrix(label.cpu(),  pred.cpu()))\n",
    "precision_recall_fscore_support(label.cpu(), pred.cpu(), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7841983883736144, 0.7793594306049823, 0.7784162210439443, None)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(label.cpu(), pred.cpu(), average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
