{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import heapq\n",
    "import io\n",
    "from typing import List, Tuple, AnyStr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def getLanguageDataSet(data, language):\n",
    "    return data.filter(lambda x: x['language'] == language)\n",
    "\n",
    "def getEnglishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"english\").remove_columns([\"language\", \"document_url\"])\n",
    "\n",
    "\n",
    "def getJapaneseDataSet(data):\n",
    "    return getLanguageDataSet(data, \"japanese\").remove_columns([\"language\", \"document_url\"])\n",
    "\n",
    "\n",
    "def getFinnishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"finnish\").remove_columns([\"language\", \"document_url\"])\n",
    "\n",
    "\n",
    "# keep only english data\n",
    "english_set = getEnglishDataSet(dataset)\n",
    "japanese_set = getJapaneseDataSet(dataset)\n",
    "finnish_set = getFinnishDataSet(dataset)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "\n",
    "# !unzip wiki-news-300d-1M.vec.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# use get_tokenizer from torchtext to tokenize data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "english_tokenizer = get_tokenizer('basic_english', language=\"en\")\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "japanese_tokenizer = spacy.load(\"ja_core_news_sm\")\n",
    "def new_japanese_tokenizer(sent):\n",
    "    return [token.text for token in japanese_tokenizer(sent)]\n",
    "\n",
    "\n",
    "finnish_tokenizer = spacy.load(\"fi_core_news_sm\")\n",
    "def new_finnish_tokenizer(sent):\n",
    "    return [token.text for token in finnish_tokenizer(sent)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lstm_tokenizer_data(examples, tokenizer):\n",
    "    answer_start = examples[\"annotations\"][\"answer_start\"][0]\n",
    "    if answer_start == -1:\n",
    "        tokens_question = tokenizer(examples[\"question_text\"]) + [\"[SEP]\"]\n",
    "        tokens_doc = tokenizer(examples[\"document_plaintext\"])\n",
    "        examples[\"tokens\"] = tokens_question + tokens_doc\n",
    "        examples[\"labels\"] = [0] * len(examples[\"tokens\"])\n",
    "        examples[\"length\"] = len(examples[\"labels\"])\n",
    "    else:\n",
    "        answer_end = answer_start + len(examples[\"annotations\"][\"answer_text\"][0]) - 1\n",
    "        before_answer = examples[\"document_plaintext\"][:answer_start]\n",
    "        answer = examples[\"annotations\"][\"answer_text\"][0]\n",
    "        after_answer = examples[\"document_plaintext\"][answer_end + 1 :]\n",
    "        tokens_question = tokenizer(examples[\"question_text\"]) + [\"[SEP]\"]\n",
    "        tokens_before_answer = tokenizer(before_answer)\n",
    "        tokens_answer = tokenizer(answer)\n",
    "        tokens_after_answer  = tokenizer(after_answer)\n",
    "        examples[\"tokens\"] = tokens_question + tokens_before_answer + tokens_answer + tokens_after_answer\n",
    "        examples[\"labels\"] = [0] * len(tokens_question) + [0] * len(tokens_before_answer) + [1] + [2]*(len(tokens_answer) -1) + [0]*len(tokens_after_answer)\n",
    "        examples[\"length\"] = len(examples[\"labels\"])\n",
    "    return examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def real_english_tokenizer(examples):\n",
    "    return lstm_tokenizer_data(examples, english_tokenizer)\n",
    "\n",
    "def real_japanese_tokenizer(examples):\n",
    "    return lstm_tokenizer_data(examples, new_japanese_tokenizer)\n",
    "\n",
    "def real_finnish_tokenizer(examples):\n",
    "    return lstm_tokenizer_data(examples, new_finnish_tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "english_tokenized_datasets = english_set.map(real_english_tokenizer, batched=False,\n",
    "                                     remove_columns=english_set[\"train\"].column_names)\n",
    "\n",
    "japanese_tokenized_datasets = japanese_set.map(real_japanese_tokenizer, batched=False,\n",
    "                                     remove_columns=japanese_set[\"train\"].column_names)\n",
    "\n",
    "finnish_tokenized_datasets = finnish_set.map(real_finnish_tokenizer, batched=False,\n",
    "                                     remove_columns=finnish_set[\"train\"].column_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Reduce down to our vocabulary and word embeddings\n",
    "def load_vectors(fname, vocabulary, vocab_size = 30000):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    final_vocab = ['[PAD]', '[UNK]', '[BOS]', '[EOS]','[SEP]']\n",
    "    final_vectors = [np.random.normal(size=(300,)) for _ in range(len(final_vocab))]\n",
    "    for j,line in enumerate(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in vocabulary or len(final_vocab) < vocab_size:\n",
    "          final_vocab.append(tokens[0])\n",
    "          final_vectors.append(np.array(list(map(float, tokens[1:]))))\n",
    "    return final_vocab, np.vstack(final_vectors)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class FasttextTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = {}\n",
    "        for j,l in enumerate(vocabulary):\n",
    "            self.vocab[l.strip()] = j\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Text is assumed to be tokenized\n",
    "        return [self.vocab[t] if t in self.vocab else self.vocab['[UNK]'] for t in text]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "english_vocabulary = (set([t for s in english_tokenized_datasets['train'] for t in s['tokens']]) | set([t for s in english_tokenized_datasets['validation'] for t in s['tokens']]))\n",
    "english_vocabulary, english_pretrained_embeddings = load_vectors('wiki-news-300d-1M.vec', english_vocabulary, vocab_size = 25000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('size of vocabulary: ', len(english_vocabulary))\n",
    "english_tokenizer = FasttextTokenizer(english_vocabulary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data: Tuple, tokenizer) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    input_ids = [tokenizer.encode(i['tokens']) for i in input_data]\n",
    "    seq_lens = [len(i) for i in input_ids]\n",
    "    labels = [i[\"labels\"] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    input_ids = [(i + [0] * (max_length - len(i))) for i in input_ids]\n",
    "    labels = [(i + [0] * (max_length - len(i))) for i in labels] # 0 is the id of the O tag\n",
    "\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    assert (all(len(i) == max_length for i in labels))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def english_collate_batch_bilstm(input_data: Tuple)-> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    return collate_batch_bilstm(input_data, english_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Japanese data processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\n",
    "# !gunzip cc.ja.300.vec.gz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "japanese_vocabulary = (set([t for s in japanese_tokenized_datasets['train'] for t in s['tokens']]) | set([t for s in japanese_tokenized_datasets['validation'] for t in s['tokens']]))\n",
    "japanese_vocabulary, japanese_pretrained_embeddings = load_vectors('cc.ja.300.vec', japanese_vocabulary, vocab_size = 25000)\n",
    "print('size of vocabulary: ', len(japanese_vocabulary))\n",
    "japanese_tokenizer = FasttextTokenizer(japanese_vocabulary)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def japanese_collate_batch_bilstm(input_data: Tuple)-> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    return collate_batch_bilstm(input_data, japanese_tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Finnish data processing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fi.300.vec.gz\n",
    "# !gunzip cc.fi.300.vec.gz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "finnish_vocabulary = (set([t for s in finnish_tokenized_datasets['train'] for t in s['tokens']]) | set([t for s in finnish_tokenized_datasets['validation'] for t in s['tokens']]))\n",
    "finnish_vocabulary, finnish_pretrained_embeddings = load_vectors('cc.fi.300.vec', finnish_vocabulary, vocab_size = 25000)\n",
    "print('size of vocabulary: ', len(finnish_vocabulary))\n",
    "finnish_tokenizer = FasttextTokenizer(finnish_vocabulary)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def finnish_collate_batch_bilstm(input_data: Tuple)-> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    return collate_batch_bilstm(input_data, finnish_tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# beam search model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Encoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializer for EncoderRNN network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n",
    "        :param lstm_dim: The dimensionality of the LSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        \"\"\"\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, and an LSTM layer.\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'lstm': nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, 2, batch_first=True, bidirectional=True),\n",
    "        })\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['lstm'].named_parameters())\n",
    "        for n, p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :return: (lstm output state, lstm hidden state)\n",
    "        \"\"\"\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "                    embeds,\n",
    "                    input_lens.cpu(),\n",
    "                    batch_first=True,\n",
    "                    enforce_sorted=False\n",
    "                )\n",
    "        lstm_out, hidden_states = self.model['lstm'](lstm_in)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        return lstm_out, hidden_states\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Decoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2):\n",
    "        \"\"\"\n",
    "        Initializer for DecoderRNN network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n",
    "        :param lstm_dim: The dimensionality of the LSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: Number of prediction classes\n",
    "        \"\"\"\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a LSTM layer, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'lstm': nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, 2, bidirectional=True, batch_first=True),\n",
    "            'nn': nn.Linear(lstm_dim*2, n_classes),\n",
    "        })\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, inputs, hidden, input_lens):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param hidden: (b) The hidden state of the previous step\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :return: (output predictions, lstm hidden states) the hidden states will be used as input at the next step\n",
    "        \"\"\"\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "                    embeds,\n",
    "                    input_lens.cpu(),\n",
    "                    batch_first=True,\n",
    "                    enforce_sorted=False\n",
    "                )\n",
    "        lstm_out, hidden_states = self.model['lstm'](lstm_in, hidden)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        output = self.model['nn'](lstm_out)\n",
    "        return output, hidden_states\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['lstm'].named_parameters()) + list(self.model['nn'].named_parameters())\n",
    "        for n, p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "# Define the model\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Seq2Seq network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            tokenizer,\n",
    "            weight_list,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic Seq2Seq network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n",
    "        :param lstm_dim: The dimensionality of the LSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        # We'll define the network in a ModuleDict, which consists of an encoder and a decoder\n",
    "        self.model = nn.ModuleDict({\n",
    "            'encoder': EncoderRNN(pretrained_embeddings, lstm_dim, dropout_prob),\n",
    "            'decoder': DecoderRNN(pretrained_embeddings, lstm_dim, dropout_prob, n_classes),\n",
    "        })\n",
    "        self.loss = nn.CrossEntropyLoss(weight=torch.FloatTensor(weight_list).to(device))\n",
    "\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels=None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model.\n",
    "        For the Seq2Seq model this includes 1) encoding the whole input text,\n",
    "        and running *target_length* decoding steps to predict the tag of each token.\n",
    "\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x embedding dim)\n",
    "        encoder_output, encoder_hidden = self.model['encoder'](inputs, input_lens)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.tensor([self.tokenizer.encode(['[BOS]'])]*inputs.shape[0], device=device)\n",
    "        target_length = labels.size(1)\n",
    "\n",
    "        loss = None\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = self.model['decoder'](\n",
    "                decoder_input, decoder_hidden, torch.tensor([1]*inputs.shape[0]))\n",
    "\n",
    "            if loss == None:\n",
    "              loss = self.loss(decoder_output.squeeze(1), labels[:, di])\n",
    "            else:\n",
    "              loss += self.loss(decoder_output.squeeze(1), labels[:, di])\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            decoder_input = labels[:, di].unsqueeze(-1)\n",
    "\n",
    "        return loss / target_length\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dl: DataLoader,\n",
    "    valid_dl: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    n_epochs: int,\n",
    "    device: torch.device,\n",
    "    tokenizer\n",
    "):\n",
    "  \"\"\"\n",
    "  The main training loop which will optimize a given model on a given dataset\n",
    "  :param model: The model being optimized\n",
    "  :param train_dl: The training dataset\n",
    "  :param valid_dl: A validation dataset\n",
    "  :param optimizer: The optimizer used to update the model parameters\n",
    "  :param n_epochs: Number of epochs to train for\n",
    "  :param device: The device to train on\n",
    "  :return: (model, losses) The best model and the losses per iteration\n",
    "  \"\"\"\n",
    "\n",
    "  # Keep track of the loss and best accuracy\n",
    "  losses = []\n",
    "  best_f1 = 0.0\n",
    "\n",
    "  # Iterate through epochs\n",
    "  for ep in range(n_epochs):\n",
    "\n",
    "    loss_epoch = []\n",
    "\n",
    "    #Iterate through each batch in the dataloader\n",
    "    for batch in tqdm(train_dl):\n",
    "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n",
    "      # things like dropout and layer normalization\n",
    "      model.train()\n",
    "\n",
    "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
    "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
    "      # zero them out\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Place each tensor on the GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      labels = batch[2]\n",
    "      input_lens = batch[1]\n",
    "\n",
    "      # Pass the inputs through the model, get the current loss and logits\n",
    "      loss = model(input_ids, labels=labels, input_lens=input_lens)\n",
    "      losses.append(loss.item())\n",
    "      loss_epoch.append(loss.item())\n",
    "\n",
    "      # Calculate all of the gradients and weight updates for the model\n",
    "      loss.backward()\n",
    "\n",
    "      # Optional: clip gradients\n",
    "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      # Finally, update the weights of the model\n",
    "      optimizer.step()\n",
    "\n",
    "    # Perform inline evaluation at the end of the epoch\n",
    "    f1 = evaluate(model, valid_dl, tokenizer)\n",
    "    print(f'Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
    "\n",
    "    # Keep track of the best model based on the accuracy\n",
    "    if f1 > best_f1:\n",
    "      torch.save(model.state_dict(), 'best_model')\n",
    "      best_f1 = f1\n",
    "\n",
    "  return losses\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "def decode(model, inputs, input_lens, tokenizer, labels=None, beam_size=2):\n",
    "    \"\"\"\n",
    "    Decoding/predicting the labels for an input text by running beam search.\n",
    "\n",
    "    :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "    :param input_lens: (b) The length of each input sequence\n",
    "    :param labels: (b) The label of each sample\n",
    "    :param beam_size: the size of the beam\n",
    "    :return: predicted sequence of labels\n",
    "    \"\"\"\n",
    "\n",
    "    assert inputs.shape[0] == 1\n",
    "    # first, encode the input text\n",
    "    encoder_output, encoder_hidden = model.model['encoder'](inputs, input_lens)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # the decoder starts generating after the Begining of Sentence (BOS) token\n",
    "    decoder_input = torch.tensor([tokenizer.encode(['[BOS]',]),], device=device)\n",
    "    target_length = labels.shape[1]\n",
    "\n",
    "    # we will use heapq to keep top best sequences so far sorted in heap_queue\n",
    "    # these will be sorted by the first item in the tuple\n",
    "    heap_queue = []\n",
    "    heap_queue.append((torch.tensor(0), tokenizer.encode(['[BOS]']), decoder_input, decoder_hidden))\n",
    "\n",
    "    # Beam Decoding\n",
    "    for _ in range(target_length):\n",
    "        # print(\"next len\")\n",
    "        new_items = []\n",
    "        # for each item on the beam\n",
    "        for j in range(len(heap_queue)):\n",
    "            # 1. remove from heap\n",
    "            score, tokens, decoder_input, decoder_hidden = heapq.heappop(heap_queue)\n",
    "            # 2. decode one more step\n",
    "            decoder_output, decoder_hidden = model.model['decoder'](\n",
    "                decoder_input, decoder_hidden, torch.tensor([1]))\n",
    "            decoder_output = softmax(decoder_output)\n",
    "            # 3. get top-k predictions\n",
    "            best_idx = torch.argsort(decoder_output[0], descending=True)[0]\n",
    "            # print(decoder_output)\n",
    "            # print(best_idx)\n",
    "            for i in range(beam_size):\n",
    "                decoder_input = torch.tensor([[best_idx[i]]], device=device)\n",
    "\n",
    "                new_items.append((score + decoder_output[0,0, best_idx[i]],\n",
    "                                  tokens + [best_idx[i].item()],\n",
    "                                  decoder_input,\n",
    "                                  decoder_hidden))\n",
    "        # add new sequences to the heap\n",
    "        for item in new_items:\n",
    "          # print(item)\n",
    "          heapq.heappush(heap_queue, item)\n",
    "        # remove sequences with lowest score (items are sorted in descending order)\n",
    "        while len(heap_queue) > beam_size:\n",
    "          heapq.heappop(heap_queue)\n",
    "\n",
    "    final_sequence = heapq.nlargest(1, heap_queue)[0]\n",
    "    assert labels.shape[1] == len(final_sequence[1][1:])\n",
    "    return final_sequence\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model: nn.Module, valid_dl: DataLoader, tokenizer, beam_size:int = 1):\n",
    "  \"\"\"\n",
    "  Evaluates the model on the given dataset\n",
    "  :param model: The model under evaluation\n",
    "  :param valid_dl: A `DataLoader` reading validation data\n",
    "  :return: The accuracy of the model on the dataset\n",
    "  \"\"\"\n",
    "  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n",
    "  # layer normalization and dropout\n",
    "  model.eval()\n",
    "  labels_all = []\n",
    "  logits_all = []\n",
    "  tags_all = []\n",
    "\n",
    "  # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      input_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      best_seq = decode(model, input_ids, input_lens, tokenizer, labels=labels, beam_size=beam_size)\n",
    "      mask = (input_ids != 0)\n",
    "      labels_all.extend([l for seq,samp in zip(list(labels.detach().cpu().numpy()), input_ids) for l,i in zip(seq,samp) if i != 0])\n",
    "      tags_all += best_seq[1][1:]\n",
    "      # print(best_seq[1][1:], labels)\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, tags_all, average='macro')\n",
    "    print(confusion_matrix(labels_all, tags_all))\n",
    "    return F1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_dim = 500\n",
    "dropout_prob = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "n_epochs = 5\n",
    "\n",
    "# Create the model\n",
    "english_model = Seq2Seq(\n",
    "    pretrained_embeddings=torch.FloatTensor(english_pretrained_embeddings),\n",
    "    lstm_dim=lstm_dim,\n",
    "    tokenizer = english_tokenizer,\n",
    "    weight_list = [0.1,100,1],\n",
    "    dropout_prob=dropout_prob,\n",
    "    n_classes=3\n",
    "  ).to(device)\n",
    "\n",
    "english_train_dl = DataLoader(english_tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=english_collate_batch_bilstm, num_workers=8)\n",
    "english_valid_dl = DataLoader(english_tokenized_datasets['validation'], batch_size=1, collate_fn=english_collate_batch_bilstm, num_workers=8)\n",
    "\n",
    "# Create the optimizer\n",
    "english_optimizer = Adam(english_model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "losses = train(english_model, english_train_dl, english_valid_dl, english_optimizer, n_epochs, device, english_tokenizer)\n",
    "english_model.load_state_dict(torch.load('best_model'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_dim = 300\n",
    "dropout_prob = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "n_epochs = 30\n",
    "\n",
    "# Create the model\n",
    "japanese_model = Seq2Seq(\n",
    "    pretrained_embeddings=torch.FloatTensor(japanese_pretrained_embeddings),\n",
    "    lstm_dim=lstm_dim,\n",
    "    tokenizer = japanese_tokenizer,\n",
    "    weight_list = [1,15,3],\n",
    "    dropout_prob=dropout_prob,\n",
    "    n_classes=3\n",
    "  ).to(device)\n",
    "\n",
    "japanese_train_dl = DataLoader(japanese_tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=japanese_collate_batch_bilstm, num_workers=8)\n",
    "japanese_valid_dl = DataLoader(japanese_tokenized_datasets['validation'], batch_size=1, collate_fn=japanese_collate_batch_bilstm, num_workers=8)\n",
    "\n",
    "# Create the optimizer\n",
    "japanese_optimizer = Adam(japanese_model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "losses = train(japanese_model, japanese_train_dl, japanese_valid_dl, japanese_optimizer, n_epochs, device, japanese_tokenizer)\n",
    "japanese_model.load_state_dict(torch.load('best_model'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  [1,8,2]\n",
    "\n",
    "lstm_dim = 300\n",
    "dropout_prob = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "n_epochs = 30\n",
    "\n",
    "# Create the model\n",
    "finnish_model = Seq2Seq(\n",
    "    pretrained_embeddings=torch.FloatTensor(finnish_pretrained_embeddings),\n",
    "    lstm_dim=lstm_dim,\n",
    "    tokenizer = finnish_tokenizer,\n",
    "    weight_list = [1,15,3],\n",
    "    dropout_prob=dropout_prob,\n",
    "    n_classes=3\n",
    "  ).to(device)\n",
    "\n",
    "finnish_train_dl = DataLoader(finnish_tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=finnish_collate_batch_bilstm, num_workers=8)\n",
    "finnish_valid_dl = DataLoader(finnish_tokenized_datasets['validation'], batch_size=1, collate_fn=finnish_collate_batch_bilstm, num_workers=8)\n",
    "\n",
    "# Create the optimizer\n",
    "finnish_optimizer = Adam(finnish_model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "losses = train(finnish_model, finnish_train_dl, finnish_valid_dl, finnish_optimizer, n_epochs, device, finnish_tokenizer)\n",
    "finnish_model.load_state_dict(torch.load('best_model'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
