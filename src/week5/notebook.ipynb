{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from args import *\n",
    "import torch.utils.data as Data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset, load_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collate_batch_bilstm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "squad_v2 = False\n",
    "# distilbert-base-uncased can only be used in English\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "# model_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getLanguageDataSet(data, language):\n",
    "    def printAndL(x):\n",
    "        return x[\"language\"] == language\n",
    "\n",
    "    return data.filter(printAndL)\n",
    "\n",
    "\n",
    "def getEnglishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"english\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# keep only english data\n",
    "english_set = getEnglishDataSet(dataset)\n",
    "english_set = english_set.remove_columns(\"language\")\n",
    "english_set = english_set.remove_columns(\"document_url\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 256  # 输入feature的最大长度，question和context拼接之后\n",
    "doc_stride = 128  # 2个切片之间的重合token数量。\n",
    "pad_on_right = True\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_answer_index(start_index, offset_mapping):\n",
    "    res = -1\n",
    "    for i, t in enumerate(offset_mapping):\n",
    "        if t[0] <= start_index and t[1] >= start_index:\n",
    "            res = i\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_start_and_end_answer_index(start_index, end_index, offset_mapping):\n",
    "    start_res = -1\n",
    "    end_res = -1\n",
    "    for i, t in enumerate(offset_mapping):\n",
    "        if t[0] <= start_index and t[1] >= start_index:\n",
    "            start_res = i\n",
    "        if t[0] <= end_index and t[1] >= end_index:\n",
    "            end_res = i\n",
    "    return (start_res, end_res)\n",
    "\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # 既要对examples进行truncation（截断）和padding（补全）还要还要保留所有信息，所以要用的切片的方法。\n",
    "    # 每一个一个超长文本example会被切片成多个输入，相邻两个输入之间会有交集。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question_text\" if pad_on_right else \"document_plaintext\"],\n",
    "        examples[\"document_plaintext\" if pad_on_right else \"question_text\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 我们使用overflow_to_sample_mapping参数来映射切片片ID到原始ID。\n",
    "    # 比如有2个expamples被切成4片，那么对应是[0, 0, 1, 1]，前两片对应原来的第一个example。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # offset_mapping也对应4片\n",
    "    # offset_mapping参数帮助我们映射到原始输入，由于答案标注在原始输入上，所以有助于我们找到答案的起始和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 重新标注数据\n",
    "    tokenized_examples[\"label\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 对每一片进行处理\n",
    "        # 将无答案的样本标注到CLS上\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "\n",
    "        # 区分question和context\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 拿到原始的example 下标.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"annotations\"][sample_index]\n",
    "        # 如果没有答案，则使用CLS所在的位置为答案.\n",
    "        if len(answers[\"answer_start\"]) == [-1]:\n",
    "            tokenized_examples[\"label\"].append([0] * max_length)\n",
    "        else:\n",
    "            # 答案的character级别Start/end位置.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"answer_text\"][0])\n",
    "\n",
    "            # 找到token级别的index start.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 找到token级别的index end.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "            # 检测答案是否超出文本长度，超出的话也适用CLS index作为标注.\n",
    "            temp = [0] * max_length\n",
    "            if offsets[token_start_index][0] > start_char or offsets[token_end_index][1] < end_char:\n",
    "                tokenized_examples[\"label\"].append(temp)\n",
    "            elif offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char:\n",
    "                s, e = get_start_and_end_answer_index(start_char, end_char, offsets)\n",
    "                temp[s] = 1\n",
    "                temp[s + 1:e + 1] = [2] * (e - s)\n",
    "                tokenized_examples[\"label\"].append(temp)\n",
    "            elif offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] < end_char:\n",
    "                s = get_answer_index(start_char, offsets)\n",
    "                temp[s] = 1\n",
    "                length = len(temp) - s\n",
    "                temp[s:] = [2] * length\n",
    "                tokenized_examples[\"label\"].append(temp)\n",
    "            elif offsets[token_start_index][0] > start_char and offsets[token_end_index][1] >= end_char:\n",
    "                e = get_answer_index(end_char, offsets)\n",
    "                temp[:e + 1] = [2] * e\n",
    "                tokenized_examples[\"label\"].append(temp)\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_loader():\n",
    "    return train_loader\n",
    "\n",
    "def get_test_loader():\n",
    "    return test_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_datasets = english_set.map(prepare_train_features, batched=True,\n",
    "                                     remove_columns=english_set[\"train\"].column_names)\n",
    "\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns(\"token_type_ids\")\n",
    "\n",
    "train_data = tokenized_datasets['train']\n",
    "test_data = tokenized_datasets['validation']\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data,\n",
    "                               batch_size=32,\n",
    "                               shuffle=True)\n",
    "\n",
    "test_loader = Data.DataLoader(dataset=test_data,\n",
    "                              batch_size=32,\n",
    "                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from args import *\n",
    "from transformers import AutoTokenizer\n",
    "from model import *\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from src.week5 import data_processing\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset, load_metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model: nn.Module, valid_dl):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataset\n",
    "    :param model: The model under evaluation\n",
    "    :param valid_dl: A `DataLoader` reading validation data\n",
    "    :return: The accuracy of the model on the dataset\n",
    "    \"\"\"\n",
    "    # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n",
    "    # layer normalization and dropout\n",
    "    model.eval()\n",
    "    labels_all = []\n",
    "    preds_all = []\n",
    "\n",
    "    # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            # batch = tuple(t.to(device) for t in batch)\n",
    "            labels = torch.stack(batch['label'], dim=1).cuda()\n",
    "            hidden_states = None\n",
    "\n",
    "            logits = model(batch)\n",
    "            preds_all.extend(torch.argmax(logits, dim=-1).reshape(-1).detach().cpu().numpy())\n",
    "            labels_all.extend(labels.reshape(-1).detach().cpu().numpy())\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds_all, average='macro')\n",
    "    print(confusion_matrix(labels_all, preds_all))\n",
    "    return F1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def predict_same_with_label(predict, label):\n",
    "    if label.equal(predict):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transformer_sampling(model, sentence, max_len):\n",
    "    # encode context the generation is conditioned on\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    greedy_output = model.generate(input_ids, max_length=max_len, do_sample=True)\n",
    "\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")  # loss function\n",
    "\n",
    "model = QA_model(args.input_dim, args.hidden_dim, args.output_dim).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)\n",
    "\n",
    "max_acc = 0\n",
    "losses = []\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    batch_num = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        label = torch.stack(batch['label'], dim=1).cuda()\n",
    "        label = torch.reshape(label, (-1, 1)).squeeze(-1)\n",
    "        predict_label = model(batch)\n",
    "        loss = criterion(predict_label, label)\n",
    "\n",
    "        pred = predict_label.max(-1, keepdim=True)[1]\n",
    "        acc = predict_same_with_label(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        if (acc > max_acc):\n",
    "            max_acc = acc\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_num += 1\n",
    "        # nni.report_intermediate_result(max_acc)\n",
    "        print(\"epoch:\", epoch + 1, \"batch_num:\", batch_num, \"loss:\", round(loss.item(), 4), \"acc:\", acc)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(\"train_max_acc:\", max_acc)\n",
    "model = QA_model(args.input_dim, args.hidden_dim, args.output_dim).to('cuda')\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "F1 = evaluate(model, test_loader)\n",
    "print(\"F1:\", F1)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
