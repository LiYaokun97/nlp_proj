{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55fa68a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import io\n",
    "\n",
    "from torch.optim import Adam\n",
    "import io\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import torch\n",
    "import random\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchcrf import CRF\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
    "from typing import List, Tuple, AnyStr\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf971c",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLanguageDataSet(data, language):\n",
    "    return data.filter(lambda x: x['language'] == language)\n",
    "\n",
    "def getEnglishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"english\").remove_columns([\"language\", \"document_url\"])\n",
    "\n",
    "\n",
    "def getJapaneseDataSet(data):\n",
    "    return getLanguageDataSet(data, \"japanese\").remove_columns([\"language\", \"document_url\"])\n",
    "\n",
    "\n",
    "def getFinnishDataSet(data):\n",
    "    return getLanguageDataSet(data, \"finnish\").remove_columns([\"language\", \"document_url\"])\n",
    "\n",
    "\n",
    "# keep only english data\n",
    "english_set = getEnglishDataSet(dataset)\n",
    "japanese_set = getJapaneseDataSet(dataset)\n",
    "finnish_set = getFinnishDataSet(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497622e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f57f2e",
   "metadata": {},
   "source": [
    "## use get_tokenizer from torchtext to tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_data2(examples, tokenizer):\n",
    "    answer_start = examples[\"annotations\"][\"answer_start\"][0]\n",
    "    if answer_start == -1:\n",
    "        tokens_question = tokenizer(examples[\"question_text\"])\n",
    "        tokens_doc = tokenizer(examples[\"document_plaintext\"])\n",
    "        examples[\"tokens\"] = tokens_question + tokens_doc\n",
    "        examples[\"labels\"] = [0] * len(examples[\"tokens\"])\n",
    "        examples[\"length\"] = len(examples[\"labels\"])\n",
    "    else:\n",
    "        answer_end = answer_start + len(examples[\"annotations\"][\"answer_text\"][0]) - 1\n",
    "        before_answer = examples[\"document_plaintext\"][:answer_start]\n",
    "        answer = examples[\"annotations\"][\"answer_text\"][0]\n",
    "        after_answer = examples[\"document_plaintext\"][answer_end + 1 :]\n",
    "        tokens_question = tokenizer(examples[\"question_text\"])\n",
    "        #  we should not add EOS before answer, so just skip the last token(actually it is eos)        \n",
    "        tokens_before_answer = tokenizer(before_answer)[:-1]\n",
    "        tokens_answer = tokenizer(answer)[:-1]\n",
    "        tokens_after_answer  = tokenizer(after_answer)\n",
    "        examples[\"tokens\"] = tokens_question + tokens_before_answer + tokens_answer + tokens_after_answer\n",
    "        examples[\"labels\"] = [0] * len(tokens_question) + [0] * len(tokens_before_answer) + [1] + [2]*(len(tokens_answer) -1) + [0]*len(tokens_after_answer)\n",
    "        examples[\"length\"] = len(examples[\"labels\"])\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe785957",
   "metadata": {},
   "source": [
    "## english vocabulary and pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "english_vocab_size = 25000\n",
    "english_dim = 100\n",
    "# Load english model with 25k word-pieces\n",
    "bpemb_en = BPEmb(lang='en', dim=english_dim, vs=english_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b48965",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_pretrained_embeddings = np.concatenate([bpemb_en.emb.vectors, np.zeros(shape=(1,english_dim)),np.zeros(shape=(1,english_dim))], axis=0)\n",
    "english_vocabulary = bpemb_en.emb.index_to_key + ['[BOS]'] + ['[PAD]'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c370b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = bpemb_en\n",
    "def new_english_tokenizer(sent):\n",
    "    return english_tokenizer.encode_ids_with_eos(sent)\n",
    "\n",
    "def real_english_data(examples):\n",
    "    return tokenizer_data2(examples, new_english_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_english_tokenizer(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenized_datasets = english_set.map(real_english_data, batched=False, remove_columns=english_set[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "english_tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70268d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84d8814c",
   "metadata": {},
   "source": [
    "## japanese vocabulary and pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f025131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "japanese_vocab_size = 25000\n",
    "japanese_dim = 100\n",
    "# Load english model with 25k word-pieces\n",
    "bpemb_ja = BPEmb(lang='ja', dim=japanese_dim, vs=japanese_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5894e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "japanese_pretrained_embeddings = np.concatenate([bpemb_ja.emb.vectors, np.zeros(shape=(1,japanese_dim))], axis=0)\n",
    "japanese_vocabulary = bpemb_ja.emb.index_to_key + ['[PAD]'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a318cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_tokenizer = bpemb_ja\n",
    "def new_japanese_tokenizer(sent):\n",
    "    return japanese_tokenizer.encode_ids_with_eos(sent)\n",
    "\n",
    "def real_japanese_data(examples):\n",
    "    return tokenizer_data2(examples, new_japanese_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_tokenized_datasets = japanese_set.map(real_japanese_data, batched=False, remove_columns=japanese_set[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d324f32",
   "metadata": {},
   "source": [
    "## finnish vocabulary and pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ac381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "finnish_vocab_size = 25000\n",
    "# Load english model with 25k word-pieces\n",
    "bpemb_fi = BPEmb(lang='fi', dim=100, vs=finnish_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_pretrained_embeddings = np.concatenate([bpemb_fi.emb.vectors,np.zeros(shape=(1,100))], axis=0)\n",
    "finnish_vocabulary = bpemb_fi.emb.index_to_key + ['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_tokenizer = bpemb_fi\n",
    "\n",
    "def new_finnish_tokenizer(sent):\n",
    "    return finnish_tokenizer.encode_ids_with_eos(sent)\n",
    "\n",
    "def real_finnish_data(examples):\n",
    "    return tokenizer_data2(examples, new_finnish_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_tokenized_datasets = finnish_set.map(real_finnish_data, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995bece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "real_english_tokenizer = TokenizerWrapper(new_english_tokenizer)\n",
    "real_japanese_tokenizer = TokenizerWrapper(new_japanese_tokenizer)\n",
    "real_finnish_tokenizer = TokenizerWrapper(new_finnish_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.tokenizer.encode(['[BOS]'])*inputs.shape[0]\n",
    "[real_english_tokenizer.encode([\"[BOS]\"])]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bos_index(tokenizer, vocab_size):\n",
    "    return vocab_size-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02eccc",
   "metadata": {},
   "source": [
    "# beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f692c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Encoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            pretrained_embeddings: torch.tensor, \n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializer for EncoderRNN network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n",
    "        :param lstm_dim: The dimensionality of the LSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        \"\"\"\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, and an LSTM layer.\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'lstm': nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, 2, batch_first=True, bidirectional=True),\n",
    "        })\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['lstm'].named_parameters())\n",
    "        for n, p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :return: (lstm output state, lstm hidden state) \n",
    "        \"\"\"\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "                    embeds,\n",
    "                    input_lens.cpu(),\n",
    "                    batch_first=True,\n",
    "                    enforce_sorted=False\n",
    "                )\n",
    "        lstm_out, hidden_states = self.model['lstm'](lstm_in)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        return lstm_out, hidden_states\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Decoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, \n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2):\n",
    "        \"\"\"\n",
    "        Initializer for DecoderRNN network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n",
    "        :param lstm_dim: The dimensionality of the LSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: Number of prediction classes\n",
    "        \"\"\"\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a LSTM layer, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'lstm': nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, 2, bidirectional=True, batch_first=True),\n",
    "            'nn': nn.Linear(lstm_dim*2, n_classes),\n",
    "        })\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()      \n",
    "\n",
    "    def forward(self, inputs, hidden, input_lens):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param hidden: (b) The hidden state of the previous step\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :return: (output predictions, lstm hidden states) the hidden states will be used as input at the next step\n",
    "        \"\"\"\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "        print(\"-------------- decoder forward ----------------\")\n",
    "        print(inputs)\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "                    embeds,\n",
    "                    input_lens.cpu(),\n",
    "                    batch_first=True,\n",
    "                    enforce_sorted=False\n",
    "                )\n",
    "     \n",
    "        lstm_out, hidden_states = self.model['lstm'](lstm_in, hidden)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        output = self.model['nn'](lstm_out)\n",
    "        return output, hidden_states\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['lstm'].named_parameters()) + list(self.model['nn'].named_parameters())\n",
    "        for n, p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "# Define the model\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Seq2Seq network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2,\n",
    "            tokenizer = real_english_tokenizer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic Seq2Seq network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained embeddings\n",
    "        :param lstm_dim: The dimensionality of the LSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = pretrained_embeddings.shape[0]\n",
    "        # We'll define the network in a ModuleDict, which consists of an encoder and a decoder\n",
    "        self.model = nn.ModuleDict({\n",
    "            'encoder': EncoderRNN(pretrained_embeddings, lstm_dim, dropout_prob),\n",
    "            'decoder': DecoderRNN(pretrained_embeddings, lstm_dim, dropout_prob, n_classes),\n",
    "        })\n",
    "        self.loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([1]+[8, 2]))\n",
    "\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels=None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model. \n",
    "        For the Seq2Seq model this includes 1) encoding the whole input text, \n",
    "        and running *target_length* decoding steps to predict the tag of each token.\n",
    "\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x embedding dim)\n",
    "        encoder_output, encoder_hidden = self.model['encoder'](inputs, input_lens)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.tensor([[self.vocab_size - 2]]*inputs.shape[0], device=device)\n",
    "        target_length = labels.size(1)\n",
    "\n",
    "        loss = None\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = self.model['decoder'](\n",
    "                decoder_input, decoder_hidden, torch.tensor([1]*inputs.shape[0]))\n",
    "\n",
    "            if loss == None:   \n",
    "              loss = self.loss(decoder_output.squeeze(1), labels[:, di])\n",
    "            else:\n",
    "              loss += self.loss(decoder_output.squeeze(1), labels[:, di])\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            decoder_input = labels[:, di].unsqueeze(-1) \n",
    "\n",
    "        return loss / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module, \n",
    "    train_dl: DataLoader, \n",
    "    valid_dl: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    n_epochs: int, \n",
    "    device: torch.device,\n",
    "    tokenizer\n",
    "):\n",
    "  \"\"\"\n",
    "  The main training loop which will optimize a given model on a given dataset\n",
    "  :param model: The model being optimized\n",
    "  :param train_dl: The training dataset\n",
    "  :param valid_dl: A validation dataset\n",
    "  :param optimizer: The optimizer used to update the model parameters\n",
    "  :param n_epochs: Number of epochs to train for\n",
    "  :param device: The device to train on\n",
    "  :return: (model, losses) The best model and the losses per iteration\n",
    "  \"\"\"\n",
    "\n",
    "  # Keep track of the loss and best accuracy\n",
    "  losses = []\n",
    "  best_f1 = 0.0\n",
    "\n",
    "  # Iterate through epochs\n",
    "  for ep in range(n_epochs):\n",
    "\n",
    "    loss_epoch = []\n",
    "\n",
    "    #Iterate through each batch in the dataloader\n",
    "    for batch in tqdm(train_dl):\n",
    "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on \n",
    "      # things like dropout and layer normalization\n",
    "      model.train()\n",
    "\n",
    "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
    "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
    "      # zero them out\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Place each tensor on the GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      labels = batch[2]\n",
    "      input_lens = batch[1]\n",
    "      \n",
    "      # Pass the inputs through the model, get the current loss and logits\n",
    "      loss = model(input_ids, labels=labels, input_lens=input_lens)\n",
    "      losses.append(loss.item())\n",
    "      loss_epoch.append(loss.item())\n",
    "      \n",
    "      # Calculate all of the gradients and weight updates for the model\n",
    "      loss.backward()\n",
    "\n",
    "      # Optional: clip gradients\n",
    "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      # Finally, update the weights of the model\n",
    "      optimizer.step()\n",
    "        \n",
    "    # Perform inline evaluation at the end of the epoch\n",
    "    f1 = evaluate(model, valid_dl, tokenizer)\n",
    "    print(f'Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
    "\n",
    "    # Keep track of the best model based on the accuracy\n",
    "    if f1 > best_f1:\n",
    "      torch.save(model.state_dict(), 'best_model')\n",
    "      best_f1 = f1\n",
    "  \n",
    "  return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b110837",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "def decode(model, inputs, input_lens, tokenizer, labels=None, beam_size=2, vocab_size=25000):\n",
    "    \"\"\"\n",
    "    Decoding/predicting the labels for an input text by running beam search.\n",
    "\n",
    "    :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "    :param input_lens: (b) The length of each input sequence\n",
    "    :param labels: (b) The label of each sample\n",
    "    :param beam_size: the size of the beam \n",
    "    :return: predicted sequence of labels\n",
    "    \"\"\"\n",
    "\n",
    "    assert inputs.shape[0] == 1\n",
    "    encoder_output, encoder_hidden = model.model['encoder'](inputs, input_lens)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # the decoder starts generating after the Begining of Sentence (BOS) token\n",
    "    decoder_input = torch.tensor([[vocab_size - 2]], device=device)\n",
    "    target_length = labels.shape[1]\n",
    "    # we will use heapq to keep top best sequences so far sorted in heap_queue \n",
    "    # these will be sorted by the first item in the tuple\n",
    "    heap_queue = []\n",
    "    heap_queue.append((torch.tensor(0), [vocab_size - 2], decoder_input, decoder_hidden))\n",
    "\n",
    "    # Beam Decoding\n",
    "    for _ in range(target_length):\n",
    "        # print(\"next len\")\n",
    "        new_items = []\n",
    "        # for each item on the beam\n",
    "        for j in range(len(heap_queue)): \n",
    "            # 1. remove from heap\n",
    "            score, tokens, decoder_input, decoder_hidden = heapq.heappop(heap_queue)\n",
    "            # 2. decode one more step\n",
    "            decoder_output, decoder_hidden = model.model['decoder'](\n",
    "                decoder_input, decoder_hidden, torch.tensor([1]))\n",
    "            decoder_output = softmax(decoder_output)\n",
    "            # 3. get top-k predictions\n",
    "            best_idx = torch.argsort(decoder_output[0], descending=True)[0]\n",
    "            # print(decoder_output)\n",
    "            # print(best_idx)\n",
    "            for i in range(beam_size):\n",
    "                decoder_input = torch.tensor([[best_idx[i]]], device=device)\n",
    "                \n",
    "                new_items.append((score + decoder_output[0,0, best_idx[i]],\n",
    "                                  tokens + [best_idx[i].item()], \n",
    "                                  decoder_input, \n",
    "                                  decoder_hidden))\n",
    "        # add new sequences to the heap\n",
    "        for item in new_items:\n",
    "          # print(item)\n",
    "          heapq.heappush(heap_queue, item)\n",
    "        # remove sequences with lowest score (items are sorted in descending order)\n",
    "        while len(heap_queue) > beam_size:\n",
    "          heapq.heappop(heap_queue)\n",
    "          \n",
    "    final_sequence = heapq.nlargest(1, heap_queue)[0]\n",
    "    assert labels.shape[1] == len(final_sequence[1][1:])\n",
    "    return final_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, valid_dl: DataLoader, tokenizer, beam_size:int = 1):\n",
    "  \"\"\"\n",
    "  Evaluates the model on the given dataset\n",
    "  :param model: The model under evaluation\n",
    "  :param valid_dl: A `DataLoader` reading validation data\n",
    "  :return: The accuracy of the model on the dataset\n",
    "  \"\"\"\n",
    "  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like \n",
    "  # layer normalization and dropout\n",
    "  model.eval()\n",
    "  labels_all = []\n",
    "  logits_all = []\n",
    "  tags_all = []\n",
    "\n",
    "  # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
    "  with torch.no_grad():\n",
    "    count = 0\n",
    "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      input_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      best_seq = decode(model, input_ids, input_lens, tokenizer = tokenizer, labels=labels, beam_size=beam_size)\n",
    "      if count < 5:\n",
    "            print(\"best_seq -----------------------------------------\")\n",
    "            print(best_seq)\n",
    "            \n",
    "      mask = (input_ids != 0)\n",
    "      labels_all.extend([l for seq,samp in zip(list(labels.detach().cpu().numpy()), input_ids) for l,i in zip(seq,samp) if i != 0])\n",
    "      tags_all += best_seq[1][1:]\n",
    "      if count < 5:\n",
    "            print(\"tags_all -----------------------------------------\")\n",
    "            print(tags_all)\n",
    "            print(\"labels_all ----------------------------------------\")\n",
    "            print(labels_all)\n",
    "            count += 1\n",
    "      \n",
    "      # print(best_seq[1][1:], labels)\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, tags_all, average='macro')\n",
    "    print(confusion_matrix(labels_all, tags_all))\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b80c1",
   "metadata": {},
   "source": [
    "### English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data):\n",
    "    input_ids = [i['tokens'] for i in input_data]\n",
    "    seq_lens = [i[\"length\"] for i in input_data]\n",
    "    labels = [i['labels'] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "    \n",
    "    input_ids = [(i + [0] * (max_length - len(i))) for i in input_ids]\n",
    "    labels = [(i + [0] * (max_length - len(i))) for i in labels] # 0 is the id of the O tag\n",
    "\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    assert (all(len(i) == max_length for i in labels))\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_english_tokenizer.encode('BOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[real_english_tokenizer.encode([\"BOS I love you\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf310ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dim = 300\n",
    "dropout_prob = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "n_epochs = 20\n",
    "\n",
    "# Create the model\n",
    "english_model = Seq2Seq(\n",
    "    pretrained_embeddings=torch.FloatTensor(english_pretrained_embeddings), \n",
    "    lstm_dim=lstm_dim, \n",
    "    dropout_prob=dropout_prob, \n",
    "    n_classes=3,\n",
    "    tokenizer=real_english_tokenizer\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e743593",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_dl = DataLoader(english_tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm, num_workers=8)\n",
    "english_valid_dl = DataLoader(english_tokenized_datasets['validation'], batch_size=1, collate_fn=collate_batch_bilstm, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278ae1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "english_optimizer = Adam(english_model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "english_losses = train(english_model, english_train_dl, english_valid_dl, english_optimizer, n_epochs, device, tokenizer=real_english_tokenizer)\n",
    "english_model.load_state_dict(torch.load('best_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b705ab1",
   "metadata": {},
   "source": [
    "### Japanese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a92986",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dim = 300\n",
    "dropout_prob = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "n_epochs = 20\n",
    "\n",
    "# Create the model\n",
    "japanese_model = Seq2Seq(\n",
    "    pretrained_embeddings=torch.FloatTensor(japanese_pretrained_embeddings), \n",
    "    lstm_dim=lstm_dim, \n",
    "    dropout_prob=dropout_prob, \n",
    "    n_classes=3,\n",
    "    tokenizer=real_japanese_tokenizer\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_train_dl = DataLoader(japanese_tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm, num_workers=8)\n",
    "japanese_valid_dl = DataLoader(japanese_tokenized_datasets['validation'], batch_size=1, collate_fn=collate_batch_bilstm, num_workers=8)\n",
    "\n",
    "# Create the optimizer\n",
    "japanese_optimizer = Adam(japanese_model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "japanese_losses = train(japanese_model, japanese_train_dl, japanese_valid_dl, japanese_optimizer, n_epochs, device, tokenizer=real_japanese_tokenizer)\n",
    "japanese_model.load_state_dict(torch.load('best_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ac34f",
   "metadata": {},
   "source": [
    "### Finnish Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4157a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dim = 300\n",
    "dropout_prob = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "n_epochs = 20\n",
    "\n",
    "# Create the model\n",
    "finnish_model = Seq2Seq(\n",
    "    pretrained_embeddings=torch.FloatTensor(finnish_pretrained_embeddings), \n",
    "    lstm_dim=lstm_dim, \n",
    "    dropout_prob=dropout_prob, \n",
    "    n_classes=3,\n",
    "    tokenizer=real_finnish_tokenizer\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592c584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finnish_train_dl = DataLoader(finnish_tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm, num_workers=8)\n",
    "finnish_valid_dl = DataLoader(finnish_tokenized_datasets['validation'], batch_size=1, collate_fn=collate_batch_bilstm, num_workers=8)\n",
    "\n",
    "# Create the optimizer\n",
    "finnish_optimizer = Adam(finnish_model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "finnish_losses = train(finnish_model, finnish_train_dl, finnish_valid_dl, finnish_optimizer, n_epochs, device, tokenizer=real_finnish_tokenizer)\n",
    "finnish_model.load_state_dict(torch.load('best_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48dc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
