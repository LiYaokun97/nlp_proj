# -*- coding: utf-8 -*-
"""week1_siyi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N42TPstdDV6u2loHhJx_I3FCcSnI3n5q
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


torch.manual_seed(123)

!pip install datasets

!pip install gensim==4.2.0

!python -m spacy download en_core_web_sm
!python -m spacy download ja_core_news_sm
!python -m spacy download fi_core_news_sm

import gensim
import torchtext
import spacy
import numpy
import torch.utils.data as Data
from datasets import load_dataset
from torchtext.data.utils import get_tokenizer
from collections import Counter
from torchtext.vocab import Vocab

# epochs
epochs = 200
# batch size
batch_size = 64
# learning rate
lr = 0.001


# model parameters
input_dim = 128
hidden_dim = 256
output_dim = 2

tokenizer_en = spacy.load("en_core_web_sm")
tokenizer_ja = spacy.load("ja_core_news_sm")
tokenizer_fi = spacy.load("fi_core_news_sm")

dataset = load_dataset("copenlu/answerable_tydiqa")
train_set = dataset["train"]
validation_set = dataset["validation"]

def getLanguageDataSet(data, language):
    return data.filter(lambda x: x['language'] == language)



def getJapaneseDataSet(data):
    return getLanguageDataSet(data, "japanese")

def getEnglishDataSet(data):
    return getLanguageDataSet(data, "english")

def getFinnishDataSet(data):
    return getLanguageDataSet(data, "finnish")



def build_vocab(dataSet, tokenizer):
    counter = Counter()
    for data in dataSet:
        counter.update([token.text for token in tokenizer(data['document_plaintext'])])
        # counter.update(tokenizer(data['document_plaintext']))
    return Vocab(counter)

en_train_set = getEnglishDataSet(train_set)
en_validation_set = getEnglishDataSet(validation_set)

test_text = ["Do you want to do a project in collaboration with a company", "but are having doubts about how to go about it?"]

test_doc_1 = []
test_doc_2 = []

tokenizer_1 = get_tokenizer('basic_english', language="en")
tokenizer_2 = tokenizer_en

for element in test_text:
  test_doc_1.append(tokenizer_1(element))
print(test_doc_1)
print(numpy.shape(test_doc_1))

for element in test_text:
  test_doc_2.append([token.text for token in tokenizer_2(element)])
print(test_doc_2)
print(numpy.shape(test_doc_2))

def getWord2VecModel(train_dataSet, test_dataSet, tokenizer):
    sentences = []
    keys = ["document_plaintext", "question_text"]
    for element in train_dataSet:
        for key in keys:
            sentences.append([token.text for token in tokenizer(element[key])])
            # sentences.append(tokenizer(element[key]))
    for element in test_dataSet:
        for key in keys:
            sentences.append([token.text for token in tokenizer(element[key])])
            # sentences.append(tokenizer(element[key]))

    print(numpy.shape(sentences))
    
    w2v_model = gensim.models.Word2Vec(sentences, vector_size=128, min_count=1, window=3, epochs=15)


    w2v_model.wv.save_word2vec_format("vector.txt", binary=False)
    return w2v_model

def data_process(dataSet, w2vModel, tokenizer, tokenPart="document"):
    data = []
    for element in dataSet:
        if tokenPart == "document":
            en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in [token_.text for token_ in tokenizer(element["document_plaintext"])]], dtype=torch.float32)
            # en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in tokenizer(element["document_plaintext"])], dtype=torch.float32)
            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()
            data.append(en_tensor_)
        elif tokenPart == "question":
            en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in [token_.text for token_ in tokenizer(element["question_text"])]], dtype=torch.float32)
            # en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in tokenizer(element["question_text"])], dtype=torch.float32)
            en_tensor_ = torch.mean(en_tensor_, dim=0, keepdim=True).cuda()
            data.append(en_tensor_)
        elif tokenPart == "answer":
            if (element["annotations"]["answer_start"] == [-1]):
                # en_tensor_ = torch.tensor([w2vModel.get_vector(token) for token in tokenizer(element["annotations"]["answer_text"])], dtype=torch.float32)
                # data.append(en_tensor_)
                data.append(torch.tensor([0], dtype=torch.int64).cuda())
            else:
                data.append(torch.tensor([1], dtype=torch.int64).cuda())
    return torch.cat(data, dim=0)

def getEnglishData(data, tokenPart="document"):
    tokenizer = tokenizer_en
    # tokenizer = get_tokenizer('basic_english', language="en")
    print("tokenizer finished")
    englishDataSet = getEnglishDataSet(data)
    print("englishDataSet finished")
    englishVocab = build_vocab(englishDataSet, tokenizer)
    print("englishVocab finished")
    w2vModel = gensim.models.KeyedVectors.load_word2vec_format("vector.txt", binary=False)
    print("w2vModel finished")

    return data_process(englishDataSet, w2vModel, tokenizer, tokenPart)


# todo Japanese的tokenizer
def getJapaneseData(data):
    tokenizer = get_tokenizer('basic_japanese', language="en")
    dataset = getJapaneseDataSet(data)
    vocab = build_vocab(dataset, tokenizer)
    return data_process(dataset, vocab, tokenizer)


# todo Finnish的tokenizer
def getFinnishData(data):
    tokenizer = get_tokenizer('basic_english', language="en")
    dataset = getFinnishDataSet(data)
    vocab = build_vocab(dataset, tokenizer)
    return data_process(dataset, vocab, tokenizer)

Tokenizer = tokenizer_en
# Tokenizer = get_tokenizer('basic_english', language="en")
w2vModel = getWord2VecModel(en_train_set, en_validation_set, Tokenizer)

w2vModel = gensim.models.KeyedVectors.load_word2vec_format("vector.txt", binary=False)

# train_set_en
english_answer_train_set = getEnglishData(train_set, tokenPart="answer")
english_question_train_set = getEnglishData(train_set, tokenPart="question")
english_document_train_set = getEnglishData(train_set, tokenPart="document")

# val_set_en
english_answer_validation_set = getEnglishData(validation_set, tokenPart="answer")
english_question_validation_set = getEnglishData(validation_set, tokenPart="question")
english_document_validation_set = getEnglishData(validation_set, tokenPart="document")

# dataloader
torch_dataset = Data.TensorDataset(english_question_train_set, english_document_train_set, english_answer_train_set)
train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=batch_size, shuffle=True)

class attention(nn.Module):
    def __init__(self, hidden_dim, attn_drop):
        super(attention, self).__init__()
        self.fc = nn.Linear(hidden_dim, hidden_dim, bias=True)
        nn.init.xavier_normal_(self.fc.weight, gain=1.414)

        self.tanh = nn.Tanh()
        self.att = nn.Parameter(torch.empty(size=(1, hidden_dim)), requires_grad=True)
        nn.init.xavier_normal_(self.att.data, gain=1.414)

        self.softmax = nn.Softmax()
        if attn_drop:
            self.attn_drop = nn.Dropout(attn_drop)
        else:
            self.attn_drop = lambda x: x

    def forward(self, embeds):
        beta = []
        attn_curr = self.attn_drop(self.att)
        for embed in embeds:
            sp = self.tanh(self.fc(embed)).mean(dim=0)
            beta.append(attn_curr.matmul(sp.t()))
        beta = torch.cat(beta, dim=-1).view(-1)
        beta = self.softmax(beta)
        #print(ntype+" mp ", beta.data.cpu().numpy())  # semantic attention
        z_mp = 0
        for i in range(len(embeds)):
            z_mp = z_mp+ embeds[i]*beta[i]
        return z_mp


class QA_model(nn.Module):
    def __init__(self,input_dim,hidden_dim,output_dim):
        super(QA_model, self).__init__()
        self.que_in_mlp = nn.Linear(input_dim,hidden_dim)
        self.context_in_mlp = nn.Linear(input_dim,hidden_dim)
        self.output_layer = nn.Linear(hidden_dim,output_dim)
        self.attention_layer = attention(hidden_dim,0.5)

    def forward(self,question,context):
        q_vec = F.leaky_relu(self.que_in_mlp(question))
        c_vec = F.leaky_relu(self.context_in_mlp(context))
        attention_out = self.attention_layer([q_vec,c_vec])
        predict_label = F.sigmoid(self.output_layer(attention_out))

        return predict_label

# model define
model = QA_model(input_dim, hidden_dim, output_dim).to('cuda')

# show model structure
print(model)

# loss function
criterion = nn.CrossEntropyLoss(reduction="sum")

# optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=True)

# model train
max_acc = 0
for epoch in range(epochs):
    model.train()
    batch_num = 0
    for question_vec, document_vec, label in train_loader:
        predict_label = model(question_vec, document_vec)
        loss = criterion(predict_label, label)

        pred = predict_label.max(-1, keepdim=True)[1]
        acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]
        optimizer.zero_grad()
        if (acc > max_acc):
            max_acc = acc
            torch.save(model.state_dict(), 'model.pth')
        loss.backward()
        optimizer.step()
        batch_num += 1
        print("epoch:", epoch + 1, "batch_num:", batch_num, "loss:", round(loss.item(), 4), "acc:", acc)
print("max_acc:", max_acc)

model.load_state_dict(torch.load("model.pth"))

predict_label = model(english_question_validation_set, english_document_validation_set)
pred = predict_label.max(-1, keepdim=True)[1]
label = english_answer_validation_set
test_acc = pred.eq(label.view_as(pred)).sum().item() / predict_label.shape[0]

print("test acc:", test_acc)

